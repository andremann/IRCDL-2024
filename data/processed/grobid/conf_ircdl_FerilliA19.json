{"bibliography":{"title":"Towards a Process Mining Approach to Grammar Induction for Digital Libraries Syntax Checking and Style Analysis","authors":[{"person_name":{"surname":"Ferilli","first_name":"Stefano"},"affiliations":[{"department":"Dipartimento di Informatica","institution":"Università di Bari","laboratory":null}],"email":"stefano.ferilli@uniba.it"},{"person_name":{"surname":"Angelastro","first_name":"Sergio"},"affiliations":[{"department":"Dipartimento di Informatica","institution":"Università di Bari","laboratory":null}],"email":"sergio.angelastro@uniba.it"}],"date":null,"ids":{"DOI":"10.1007/978-3-030-11226-4_23","arXiv":null},"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"keywords":["Grammar induction","Process mining and management","Natural language processing"],"citations":{"b0":{"title":"Trainable grammars for speech recognition","authors":[{"person_name":{"surname":"Baker","first_name":"J"},"affiliations":[],"email":null}],"date":{"year":"1979","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"J. Acoust. Soc. Am","series":null,"scope":{"volume":65,"pages":null}},"b1":{"title":"Incremental learning from positive examples","authors":[{"person_name":{"surname":"Bombini","first_name":"G"},"affiliations":[],"email":null},{"person_name":{"surname":"Di Mauro","first_name":"N"},"affiliations":[],"email":null},{"person_name":{"surname":"Esposito","first_name":"F"},"affiliations":[],"email":null},{"person_name":{"surname":"Ferilli","first_name":"S"},"affiliations":[],"email":null}],"date":{"year":"2009","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b2":{"title":"The Evalita 2014 dependency parsing task","authors":[{"person_name":{"surname":"Bosco","first_name":"C"},"affiliations":[],"email":null},{"person_name":{"surname":"Dell'orletta","first_name":"F"},"affiliations":[],"email":null},{"person_name":{"surname":"Montemagni","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Sanguinetti","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Simi","first_name":"M"},"affiliations":[],"email":null}],"date":{"year":"2014","month":null,"day":null},"ids":null,"target":null,"publisher":"Pisa University Press","journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":1,"to_page":8}}},"b3":{"title":"Overview of the Evalita 2016 part of speech on Twitter for Italian task","authors":[{"person_name":{"surname":"Bosco","first_name":"C"},"affiliations":[],"email":null},{"person_name":{"surname":"Fabio","first_name":"T"},"affiliations":[],"email":null},{"person_name":{"surname":"Andrea","first_name":"B"},"affiliations":[],"email":null},{"person_name":{"surname":"Mazzei","first_name":"A"},"affiliations":[],"email":null}],"date":{"year":"2016","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":1749,"pages":{"from_page":1,"to_page":7}}},"b4":{"title":"CoNLL-X shared task on multilingual dependency parsing","authors":[{"person_name":{"surname":"Buchholz","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Marsi","first_name":"E"},"affiliations":[],"email":null}],"date":{"year":"2006","month":null,"day":null},"ids":null,"target":null,"publisher":"Association for Computational Linguistics","journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":149,"to_page":164}}},"b5":{"title":"Two experiments on learning probabilistic dependency grammars from corpora","authors":[{"person_name":{"surname":"Carroll","first_name":"G"},"affiliations":[],"email":null},{"person_name":{"surname":"Charniak","first_name":"E"},"affiliations":[],"email":null}],"date":{"year":"1992","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b6":{"title":"Unsupervised induction of stochastic context-free grammars using distributional clustering","authors":[{"person_name":{"surname":"Clark","first_name":"A"},"affiliations":[],"email":null}],"date":{"year":"2001","month":null,"day":null},"ids":null,"target":null,"publisher":"Association for Computational Linguistics","journal":null,"series":null,"scope":{"volume":7,"pages":{"from_page":13,"to_page":13}}},"b7":{"title":"Multistrategy theory revision: induction and abduction in INTHELEX","authors":[{"person_name":{"surname":"Esposito","first_name":"F"},"affiliations":[],"email":null},{"person_name":{"surname":"Semeraro","first_name":"G"},"affiliations":[],"email":null},{"person_name":{"surname":"Fanizzi","first_name":"N"},"affiliations":[],"email":null},{"person_name":{"surname":"Ferilli","first_name":"S"},"affiliations":[],"email":null}],"date":{"year":"2000","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"Mach. Learn","series":null,"scope":{"volume":38,"pages":{"from_page":133,"to_page":156}}},"b8":{"title":"WoMan: logic-based workflow learning and management","authors":[{"person_name":{"surname":"Ferilli","first_name":"S"},"affiliations":[],"email":null}],"date":{"year":"2014","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"IEEE Trans. Syst. Man Cybern. Syst","series":null,"scope":{"volume":44,"pages":{"from_page":744,"to_page":756}}},"b9":{"title":"A logic framework for incremental learning of process models","authors":[{"person_name":{"surname":"Ferilli","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Esposito","first_name":"F"},"affiliations":[],"email":null}],"date":{"year":"2013","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"Fundam. Inform","series":null,"scope":{"volume":128,"pages":{"from_page":413,"to_page":443}}},"b10":{"title":"Automatic learning of linguistic resources for stopword removal and stemming from text","authors":[{"person_name":{"surname":"Ferilli","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Esposito","first_name":"F"},"affiliations":[],"email":null},{"person_name":{"surname":"Grieco","first_name":"D"},"affiliations":[],"email":null}],"date":{"year":"2014","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"Proc. Comput. Sci","series":null,"scope":{"volume":38,"pages":{"from_page":116,"to_page":123}}},"b11":{"title":"Language identification as process prediction using WoMan","authors":[{"person_name":{"surname":"Ferilli","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Esposito","first_name":"F"},"affiliations":[],"email":null},{"person_name":{"surname":"Redavid","first_name":"D"},"affiliations":[],"email":null}],"date":{"year":"2016","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":12,"to_page":12}}},"b12":{"title":"Automatic learning of linguistic resources for stopword removal and stemming from text","authors":[{"person_name":{"surname":"Ferilli","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Grieco","first_name":"D"},"affiliations":[],"email":null},{"person_name":{"surname":"Esposito","first_name":"F"},"affiliations":[],"email":null}],"date":{"year":"2014","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":12,"to_page":12}}},"b13":{"title":"Predicting process behavior in WoMan","authors":[{"person_name":{"surname":"Ferilli","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Esposito","first_name":"F"},"affiliations":[],"email":null},{"person_name":{"surname":"Redavid","first_name":"D"},"affiliations":[],"email":null},{"person_name":{"surname":"Angelastro","first_name":"S"},"affiliations":[],"email":null}],"date":{"year":"2016","month":null,"day":null},"ids":{"DOI":"10.1007/978-3-319-49130-1_23","arXiv":null},"target":"https://doi.org/10.1007/978-3-319-49130-123","publisher":"Springer","journal":null,"series":null,"scope":{"volume":10037,"pages":{"from_page":308,"to_page":320}}},"b14":{"title":"Language identification in the limit","authors":[{"person_name":{"surname":"Gold","first_name":"E"},"affiliations":[],"email":null}],"date":{"year":"1967","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"Inf. Contr","series":null,"scope":{"volume":10,"pages":{"from_page":447,"to_page":474}}},"b15":{"title":"Mining: Process mining manifesto","authors":[],"date":{"year":"2012","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"IEEE Task Force on Process","series":null,"scope":{"volume":99,"pages":{"from_page":169,"to_page":194}}},"b16":{"title":"The estimation of stochastic context-free grammars using the inside-outside algorithm","authors":[{"person_name":{"surname":"Lari","first_name":"K"},"affiliations":[],"email":null},{"person_name":{"surname":"Young","first_name":"S"},"affiliations":[],"email":null}],"date":{"year":"1990","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"Comput. Speech Lang","series":null,"scope":{"volume":4,"pages":{"from_page":35,"to_page":56}}},"b17":{"title":"ConNeKTion: a tool for handling conceptual graphs automatically extracted from text","authors":[{"person_name":{"surname":"Leuzzi","first_name":"F"},"affiliations":[],"email":null},{"person_name":{"surname":"Ferilli","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Rotella","first_name":"F"},"affiliations":[],"email":null}],"date":{"year":"2013","month":null,"day":null},"ids":null,"target":null,"publisher":"Springer","journal":null,"series":null,"scope":{"volume":385,"pages":null}},"b18":{"title":"The Penn Treebank: annotating predicate argument structure","authors":[{"person_name":{"surname":"Marcus","first_name":"M"},"affiliations":[],"email":null}],"date":{"year":"1994","month":null,"day":null},"ids":{"DOI":"10.3115/1075812.1075835","arXiv":null},"target":"https://doi.org/10.3115/1075812.1075835","publisher":"Association for Computational Linguistics","journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":114,"to_page":119}}},"b19":{"title":"Using universal linguistic knowledge to guide grammar induction","authors":[{"person_name":{"surname":"Naseem","first_name":"T"},"affiliations":[],"email":null}],"date":{"year":"2010","month":null,"day":null},"ids":null,"target":null,"publisher":"Association for Computational Linguistics","journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":1234,"to_page":1244}}},"b20":{"title":"Learning and exploiting concept networks with conNeKTion","authors":[{"person_name":{"surname":"Rotella","first_name":"F"},"affiliations":[],"email":null},{"person_name":{"surname":"Leuzzi","first_name":"F"},"affiliations":[],"email":null},{"person_name":{"surname":"Ferilli","first_name":"S"},"affiliations":[],"email":null}],"date":{"year":"2015","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"Appl. Intell","series":null,"scope":{"volume":42,"pages":{"from_page":87,"to_page":111}}},"b21":{"title":"Three dependency-and-boundary models for grammar induction","authors":[{"person_name":{"surname":"Spitkovsky","first_name":"V"},"affiliations":[],"email":null},{"person_name":{"surname":"Alshawi","first_name":"H"},"affiliations":[],"email":null},{"person_name":{"surname":"Jurafsky","first_name":"D"},"affiliations":[],"email":null}],"date":{"year":"2012","month":null,"day":null},"ids":null,"target":null,"publisher":"Association for Computational Linguistics","journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":688,"to_page":698}}},"b22":{"title":"Inducing probabilistic grammars by Bayesian model merging","authors":[{"person_name":{"surname":"Stolcke","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Omohundro","first_name":"S"},"affiliations":[],"email":null}],"date":{"year":"1994","month":null,"day":null},"ids":{"DOI":"10.1007/3-540-58473-0_141","arXiv":null},"target":"https://doi.org/10.1007/3-540-58473-0141","publisher":"Springer","journal":null,"series":null,"scope":{"volume":862,"pages":{"from_page":106,"to_page":118}}},"b23":{"title":"Rediscovering workflow models from event-based data","authors":[{"person_name":{"surname":"Weijters","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Van Der Aalst","first_name":"W"},"affiliations":[],"email":null}],"date":{"year":"2001","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":93,"to_page":100}}}},"sections":[{"title":"Introduction","paragraphs":[{"text":"One of the most relevant peculiarities and opportunities provided by Digital Libraries (DLs for short) and Archives, with respect to their physical counterparts, is the possibility of automatically accessing and processing their content by computers for several purposes. Some examples are: indexing aimed at faster and better information retrieval; topic extraction aimed at document organi-Understanding captures some kind of semantic information from the text.","refs":[]},{"text":"While for some tasks (e.g., indexing and retrieval) the lexical level is sufficient, more advanced processing requires higher-level tasks to be carried out.","refs":[]},{"text":"In turn, NLP techniques are often based on the use of linguistic resources: e.g., Language Identification often exploits n-gram distribution, Stopword Removal exploits lists of frequent terms, Normalization exploits lists of suffixes, Part-of-Speech (PoS for short) Tagging exploits suffixes and/or grammatical rules, Parsing uses grammars, Word Sense Disambiguation uses conceptual taxonomies or ontologies. The quality of such resources may dramatically affect the quality, or even determine the feasibility, of the NLP steps. However, developing these resources is a critical, costly, time-consuming and error prone task, because it is typically carried out manually by linguistic experts. To make things worse, each language requires its own set of resources. Most works in the literature are concerned with English [1,6,7,17,22], probably due to its having a structure which is easier than other languages and to its importance as the standard information interchange language worldwide. Little exists for a few other important languages [20], and almost nothing for the vast majority of minor languages. As a result, automatic processing techniques cannot be applied to documents in these languages, leading to the risk that entire cultures might be lost.","refs":[{"start":820,"end":823,"marker":"bibr","target":"#b0"},{"start":823,"end":825,"marker":"bibr","target":"#b5"},{"start":825,"end":827,"marker":"bibr","target":"#b6"},{"start":827,"end":830,"marker":"bibr","target":"#b16"},{"start":830,"end":833,"marker":"bibr","target":"#b21"},{"start":1043,"end":1047,"marker":"bibr","target":"#b19"}]},{"text":"This situation motivated the development of (semi-)automatic techniques to learn the resources and other useful linguistic information from a (representative) set of texts in a given language. Our effort in this direction resulted in BLA-BLA (Broad-spectrum Language Analysis-Based Learning Application), a tool aimed at covering a wide spectrum of NLP tasks, including several techniques that allow to learn in a fully automatic way linguistic resources for language identification [12], stopword removal and term normalization [11,13] and concept extraction [18,21]. The learned resources may be used by NLP systems, and/or be taken as a basis for linguistic studies and/or further manual refinements. Most of the techniques in BLA-BLA are incremental, meaning that whenever more texts become available for the language, it is easy to run again the technique and obtain updated resources. This is a very important feature that is generally unavailable in other approaches in the literature.","refs":[{"start":483,"end":487,"marker":"bibr","target":"#b11"},{"start":529,"end":533,"marker":"bibr","target":"#b10"},{"start":533,"end":536,"marker":"bibr","target":"#b12"},{"start":560,"end":564,"marker":"bibr","target":"#b17"},{"start":564,"end":567,"marker":"bibr","target":"#b20"}]},{"text":"This paper investigates the possibility of extending BLA-BLA with Grammar Induction and Checking features. In particular, we propose the use of an advanced Process Mining approach [16] called WoMan. As the first step in such an investigation, here we aim at assessing whether grammar models learned by WoMan are effective in recognizing the syntactic correctness of sentences in a given language. Experiments show that they are. Possible applications to document collections (libraries or archives) include an assessment of their overall linguistic quality, or an analysis of linguistic style variability therein.","refs":[{"start":180,"end":184,"marker":"bibr","target":"#b15"}]},{"text":"This paper is organized as follows. After discussing some background and related work in the next section, Sect. 3 introduces the WoMan framework for process mining and management and Sect. 4 casts the linguistic problem into a process mining task. Then, Sect. 5 evaluates the proposed approach before concluding the paper.","refs":[]}]},{"title":"Background and Related Work","paragraphs":[{"text":"Grammar Induction is a language acquisition problem. According to Gold's formalization [15], given a target language L from a set L of possible languages, a learner C is shown a sequence [s i ] of positive examples (∀i : s i ∈ L) and after each example s n it must maintain a hypothesis L(C,[s 0 , . . . , s n ]) ∈ L for L. Any s ∈ L will be sooner or later present in the sequence (no guarantees on the order or frequency of examples). The hypothesis is eventually correct if ∃k s.t. ∀j > k : L(C, [s 0 , . . . , s k ]) = L. So, positive-only and incremental learning approaches are inherent this formalization. In general, a Grammar Induction algorithm should be able to discover an underlying grammar from examples, to be used to parse new sentences and to assess their grammaticality.","refs":[{"start":87,"end":91,"marker":"bibr","target":"#b14"}]},{"text":"Approaches proposed in the literature can be divided into supervised and unsupervised. The former process sentences annotated with their constituent tree structure (e.g., the treebanks corpus [19]), which requires significant expert effort. In the latter, only words are annotated (manually or automatically) with their PoS tag. Unsupervised approaches typically exploit phrase structure or dependency grammar representations, and are further classified in structural search, aimed at discovering a suitable grammar structure that compactly describes the data, and parameter search, aimed at finding a set of optimal parameters for a fixed-structure grammar, such that the result best explains the language.","refs":[{"start":192,"end":196,"marker":"bibr","target":"#b18"}]},{"text":"As regards Structural search approaches, [23] proposed a Bayesian model merging framework to find the structure of a probabilistic grammar. Possible uses include the discovery of an Hidden Markov Model (HMM) topology or the set of context-free production for a stochastic context-free grammar. The approach performs incremental merging operations on model substructures attempting to maximize the Bayesian posterior probability of the overall model. An objective evaluation of the model is missing. [7] presents a Context Distribution Clustering (CDC) algorithm that induces clusters of tag sequences based on the context in which they appear. A criterion based on Mutual Information between the left and right context of a sequence, is exploited to filter out non-constituent clusters. The algorithm is incorporated in a Minimum Description Length framework, whereby it chooses the clusters so that the resulting constituents have the shortest description length. However it is computationally expensive since it requires large amounts of memory.","refs":[{"start":41,"end":45,"marker":"bibr","target":"#b22"},{"start":499,"end":502,"marker":"bibr","target":"#b6"}]},{"text":"As regards parameter search approaches, [1] proposed an inside-outside algorithm to induce Probabilistic Context-Free Grammars (PCFGs) 1 , that generalizes the forward-backward algorithm for regular grammars with HMMs. The fixed model consists of productions in Chomsky Normal Form (CNF) (fully binary branching derivations). Given a sequence of words W = w p . . . w q in an example, the aim is to re-estimate production probabilities computing the inside probability β(p, q) of generating W from a non-terminal X, and the outside probability α(p, q) of generating X and the words outside W in the example from the start symbol. [6] extends the approach in [1] by introducing grammar constraints, which guide the search process avoiding the grammatically incompatible generation of non-terminals (e.g., a determiner from an adjective or a verb from a pronoun), and presents a set of experiments in inducing probabilistic dependency grammars.","refs":[{"start":40,"end":43,"marker":"bibr","target":"#b0"},{"start":135,"end":136,"marker":null,"target":"#foot_0"},{"start":630,"end":633,"marker":"bibr","target":"#b5"},{"start":658,"end":661,"marker":"bibr","target":"#b0"}]},{"text":"The evaluation of a learned grammar is based on the comparison of either grammars or trees. Given a gold standard of correct parses, performance can be evaluated as the percentage of correct parses that the algorithm produces.","refs":[]}]},{"title":"Process Mining and the WoMan Framework","paragraphs":[{"text":"This work aims at checking whether Process Mining approaches may be effective for dealing with the syntactic level of natural language. So, let us quickly recall some basic concepts of Process Mining. A process consists of actions performed by agents (humans or artifacts). A workflow is a formal specification of how these actions can be composed (using sequential, parallel, conditional, or iterative schemes) to result in valid processes. A case is a particular execution of activities compliant to a given workflow. It can be described in terms of events associated to the performed activities. Case traces consist of lists of events associated to time points. A task is a generic piece of work, defined to be executed for many cases of the same type. An activity is the actual execution of a task. Process Mining tasks of interest to this work are Process Discovery, aimed at learning a process model from sample cases, and Conformance Checking, aimed at checking whether a (new) case is compliant to a given process model.","refs":[]},{"text":"In particular, inspired by the successful application of approaches based on HMMs to Grammar Induction in the literature, and by previous indications, obtained in other domains, that the Process Mining system WoMan may outperform HMMs in some cases, we propose the adoption of WoMan for our purposes. Also, while Process Mining and Management techniques in the literature have been typically motivated by and exploited in business and industrial domains, Woman proved able to support a wide variety of application domains (including Ambient Intelligence, and even Chess) [14].","refs":[{"start":571,"end":575,"marker":"bibr","target":"#b13"}]},{"text":"The WoMan framework [9] introduced some important novelties in the process mining and management landscape. Experiments proved that it is able to handle efficiently and effectively very complex processes, thanks to its powerful representation formalism and process handling operators. In the following, we briefly and intuitively recall its fundamental notions.","refs":[{"start":20,"end":23,"marker":"bibr","target":"#b8"}]},{"text":"WoMan takes as input trace elements consisting of 6-tuples T, E, W, P, A, O , where T is the event timestamp, E is the type of the event (one of 'begin process', 'end process', 'begin activity', 'end activity', or 'context description'), W is the name of the reference workflow, P is the case identifier, A is the name of the activity (or a list of contextual information for A = context description), and O is the progressive number of occurrence of that activity in that case.","refs":[]},{"text":"WoMan models are expressed using two elements: tasks: the kinds of activities that are allowed in the process; transitions: the allowed connections between activities. plus pre-/post-conditions (that specify what must be true for executing a given task or transition) in the form of First-Order Logic rules based on contextual and control flow information, possibly involving several steps of execution.","refs":[]},{"text":"The core of the model, carrying the information about the flow of activities during process execution, is the set of transitions. A transition t : I ⇒ O, where I and O are multisets of tasks, is enabled if all input tasks in I are active; it occurs when, after stopping (in any order) the concurrent execution of all tasks in I, the concurrent execution of all output tasks in O is started (again, in any order). Any task or transition t is associated to the multiset C t of training cases in which it occurred (indeed, a task or transition may occur several times in the same case, if loops or duplicate tasks are present in the model). It allows us to compute the probability of occurrence of t in a model learned from n training cases as the relative frequency |C t |/n. As shown in [9,10], this representation formalism is more powerful than Petri or Workflow Nets [24], that are the current standard in Process Mining. It can smoothly express complex models involving invisible or duplicate tasks, which are problematic for those formalisms.","refs":[{"start":786,"end":789,"marker":"bibr","target":"#b8"},{"start":789,"end":792,"marker":"bibr","target":"#b9"},{"start":869,"end":873,"marker":"bibr","target":"#b23"}]},{"text":"WoMan's supervision module, WEST (Workflow Enactment Supervisor and Trainer), takes the case events as long as they are available, and returns information about their compliance with the currently available model for the process they refer to. The output for each event can be 'ok', 'error' (e.g., when closing activities that had never begun, or terminating the process while activities are still running), or a set of warnings denoting different kinds of deviations from the model (e.g., unexpected task or transition, preconditions not fulfilled, unexpected resource running a given activity, etc.).","refs":[]},{"text":"The learning module, WIND (Workflow INDucer), allows one to learn or refine a process model according to a case. The refinement may affect the structure and/or the probabilities. Differently from all previous approaches in the literature, it is fully incremental : not only can it refine an existing model according to new cases whenever they become available, it can even start learning from an empty model and a single case, while others need a (large) number of cases to draw significant statistics before learning starts. To learn conditions in form of logic theories, WIND relies on the incremental learning system InTheLex [8]. Indeed, InTheLEx is endowed with a positive only-learning feature [2], which allows it to deal with the positive-only learning approach typical of both Process Mining and Grammar Induction.","refs":[{"start":629,"end":632,"marker":"bibr","target":"#b7"},{"start":700,"end":703,"marker":"bibr","target":"#b1"}]}]},{"title":"Grammar Induction as a Process Discovery Task","paragraphs":[{"text":"Following mainstream literature, we adopt the unsupervised setting for Grammar Induction. So, sentences in the training corpus are annotated with the sequence of PoS tags associated to their constituent tokens (words, values, punctuation). In our approach, a grammar corresponds to a process model; a sentence in natural language (actually, the sequence of PoS tags associated to the words in the sentence) is a case; a task is a PoS tag, and an activity is an occurrence of the tag in a sentence. Under this perspective, process discovery corresponds to grammar induction, and syntactic checking to conformance checking. Just as in Grammar Induction, process discovery typically adopts a positive-only learning approach (i.e., only correct sentences/process execution are included in the training corpus).","refs":[]},{"text":"As regards the set of PoS tags to be used, several options are available in the literature. In the following, we will consider CoNLL-U, a revised version of the CoNLL-X format [5]. It represents a text in natural language as a plain text file, where three types of lines are available: comment lines (starting with #), blank lines (to separate sentences), and word lines (containing token annotations). Each word line reports 10 fields: word index ID; the word form or symbol FORM with its lemma or stem LEMMA; both universal (UPOS ) and language specific (XPOS ) PoS tag; the list of morphological features in FEATS ; head HEAD of the current word (a value of ID) with its type of universal dependency relation DEPREL, and the list DEPS of head-deprel pairs forming the dependency graph; last, any other annotation in MISC. For instance, Table 1 shows the lines for sentence \"Evacuata la Tate Gallery.\", having ID isst tanl-3. So, the sequence of PoS tags that make up a sentence in the CONLL-U file is transformed into a case trace in WoMan, where:","refs":[{"start":176,"end":179,"marker":"bibr","target":"#b4"},{"start":845,"end":846,"marker":"table","target":"#tab_0"}]},{"text":"-the process name expresses the language of the sentence; -the sentence id (#sent id ) is the case identifier; -each word line determines an activity with the UPOS PoS tag (or combination UPOS-XPOS, for a more detailed model) as the activity name, and generates a pair of begin of activity and end of activity events; -the features (FEATS ) can be used as the context of the activity, and reported as a list of FOL predicates in a context description event; -begin of process and end of process events enclose the case.","refs":[]},{"text":"For instance, using UPOS only for the activities, the sentence in Table 1 would generate the following trace in WoMan format:","refs":[{"start":72,"end":73,"marker":"table","target":"#tab_0"}]},{"text":"As regards the learned process model, of tasks is the same as the set of activities encountered in the training sentences. Using the UPOS tagset, that accounts for most natural languages, the model will include at most 17 tasks, which is a fair number for state-of-the-art Process Mining systems [10]. Using the UPOS-XPOS option, even if only a portion of all possible combinations is actually encountered in practice, the number of tasks significantly increases, going beyond the capabilities of many Process Mining systems in the literature but still being within reach for WoMan. Transitions correspond to pairs of adjacent PoS tags allowed in a sentence (e.g., [det] ⇒ [pnoun] means that a definite article may be followed by a proper noun). Note that process models representing grammars will not include any concurrency (sentences are just plain sequences of words), which means that the full power of WoMan in handling concurrency is not used in this domain, but also makes the comparison to HMMs more fair. However, such models will involve loops (including nested and short ones), optional and duplicate tasks, which are among the main sources of complexity in process mining and some of the strengths of WoMan. In particular, duplicate tasks are relevant, because different occurrences of the same PoS tag in a sentence represent distinct components of the discourse and cannot be handled by the same element of the model.","refs":[{"start":296,"end":300,"marker":"bibr","target":"#b9"}]}]},{"title":"Experiments","paragraphs":[{"text":"Based on the proposed mapping between grammars and processes, we ran experiments aimed at checking whether WoMan is able to learn grammar models from sample sentences and whether the learned models can be used effectively for assessing grammatical correctness of new sentences. All experiments were run on a laptop endowed with a 2.8 GHz Intel Core i7-7700HQ Quad-Core (6M Cache) processor and 16 GB RAM, running on Kubuntu Linux 17.10. Our experiments concerned the Italian language, both because less resources are available for it in the NLP literature (but its grammar is well-known and studied in the linguistic area of research), and because its syntax is quite complex compared to English, and thus it can stress more the proposed approach.","refs":[]}]},{"title":"Datasets Description","paragraphs":[{"text":"In our experiments we used two standard, publicly available datasets used for two EvalITA shared tasks: UD Italian-ISDT from EvalITA-2014 [3], obtained by conversion from ISDT (Italian Stanford Dependency Treebank), includes Wikipedia, News and Newspapers articles, Legal texts and Various genres and sentences. Since these are more formal and controlled texts, we expect to learn more reliable grammars from them. PoSTWITA-UD from EvalITA-2016 [4], an Italian tweets collection. Since tweets often use fancy or odd sentences, it is expected to be more tricky than the other one.","refs":[{"start":138,"end":141,"marker":"bibr","target":"#b2"},{"start":445,"end":448,"marker":"bibr","target":"#b3"}]},{"text":"Both datasets are annotated in Universal Dependencies (that can be exploited for the training of NLP systems), and are provided in CoNLL-U format and randomly split in three subsets (training, development and test). Since our does not need to tune any we will ignore the development subset in our experiments. Table 2 reports some statistics about the datasets, for each considered subset thereof. For the linguistic perspective, it reports the number of sentences #sent (i.e., cases in a process perspective) and the overall number of tokens2 #token. For the process perspective, it reports the number of events #event and the number of activities #act (i.e., instances of PoS tags associated to tokens or other symbols) generated by the translation into WoMan traces.","refs":[{"start":316,"end":317,"marker":"table","target":"#tab_1"},{"start":542,"end":543,"marker":null,"target":"#foot_1"}]},{"text":"Both have several thousand training sentences, but PoSTWITA-UD has less than half than UD Italian-ISDT, which is relevant because the former is expected to use a more tricky grammar than the latter, and thus to be more complex to learn. However, the former has a larger test set than the latter. The number of tokens/activities/events (which are somehow interrelated) are in the order of hundred thousands, which means the process discovery problem is not trivial.","refs":[]}]},{"title":"Model Training","paragraphs":[{"text":"As a first step, we ran WoMan's Process Discovery feature to build a model for the Italian grammar from the training set(s). The learned models also included logic theories for pre-and post-conditions of tasks, as learned by InTheLEx. Table 3 shows some statistics about the models. As regards WoMan, for each type of tasks adopted (UPOS or UPOS-XPOS), it reports the number of tasks (#task ) and transitions (#trans) in the learned model, and the average time per sentence (time) needed to learn the model (in seconds). The number of tasks and transitions is consistent among the two datasets for the two task types. The Twitter dataset has slightly less tasks, denoting a simpler lexicon, but slightly more transitions, denoting a more complex grammar. Nevertheless, the average time to process each sentence is the same, and is really low, allowing the use of the system for real applications. As regards Inthelex, again for each type of tasks adopted (UPOS or UPOS-XPOS), Table 3 reports the number of rules (#rules), and the average time per example (time) needed to learn them, for pre-and postconditions of tasks. It also reports, for reference, the number of examples, which is the same as the number of activities in Table 2 (because WoMan generates one example of pre-condition and one example of post-condition for each activity). Again, the complexity of the theories (number of rules) is consistent between the two datasets, and the time needed to learn them is very low. ","refs":[{"start":241,"end":242,"marker":"table","target":"#tab_2"},{"start":982,"end":983,"marker":"table","target":"#tab_2"},{"start":1232,"end":1233,"marker":"table","target":"#tab_1"}]}]},{"title":"Model Evaluation and Possible Uses in DLs","paragraphs":[{"text":"Model evaluation consisted in a grammatical checking of new sentences with respect to a learned grammar. In Process Mining terms, this corresponds to a Conformance Checking task of new event traces with respect to a process model. The learned models were evaluated in two different ways. First, for each dataset and task setting (UPOS or UPOS-XPOS), we ran classical 10-fold cross validation on the training set. This allowed us to understand how good WoMan was to learn the grammar underlying a given collection. Then, we tested the grammar learned on the entire training set of UD Italian-ISDT on the sentences in the test sets. Sentences in the test set of PoSTWITA-UD were tested on the grammar learned from UD Italian-ISDT. We did so because the former is a set of tweets, that are expected to use odd sentence structures, while the latter is a set of more controlled sentences, which are expected to use a correct grammar. So, testing the former on the latter may provide an indication of how bad a grammar tweets use, and of how good the system is in rejecting sentences with wrong syntax.","refs":[]},{"text":"Table 4 reports the experimental results, evaluated according to: Accuracy, computed as the average portion of sentences identified as correct (i.e., the conformance checking never raised 'unexpected task or transition' warnings); Support, defined as percentage of training cases having the same structure as the sentence being tested; and Runtime (in minutes) spent to run the test procedure.","refs":[{"start":6,"end":7,"marker":"table","target":"#tab_3"}]},{"text":"10-fold cross-validation results show that WoMan is extremely effective in learning the grammar underlying a given collection. In a DL, this would allow the librarians to understand whether the grammar style adopted by new texts added to the collection are consistent with the previous content, or to check user comments before publishing them if a discussion section is provided for. These figures also suggest the possibility of using a further feature of WoMan, which is process prediction [14], to automatically classify the linguistic style of new incoming texts from a pre-defined set of syntactic models. Average support for UD Italian-ISDT is 20%, i.e., each test sentence has the same syntactic structure as 1/5 of the training sentences. For PoSTWITA-UD this number is more than doubled, indicating much less variability in writing style, as expected. In a DL, this would allow the librarian to determine how rich is the grammatical structure of the collection.","refs":[{"start":493,"end":497,"marker":"bibr","target":"#b13"}]},{"text":"In the evaluation based on test sets, figures for UD Italian-ISDT basically confirm the results of the cross-validation, reaching an even better performance (100%) for the UPOS setting. As regards PoSTWITA-UD, accuracy drops significantly, as expected due to the very different and informal syntax used in tweets with respect to more formal texts. In particular, it is still high for the UPOS setting, albeit almost 10% less than for the 10-fold cross-validation, but it drops to less than a half the value for UPOS-XPOS, as expected due to the fact that the more tasks available, the more complex the model, the more cases are needed to fully learn it. Support is about 10%, indicating much more style variability in the test set. In a DL, this would allow the librarian to distinguish the literary level of a text, or to distinguish texts by their type of content (e.g., novels vs. articles).","refs":[]},{"text":"Runtime is still low, considering that several hundred sentences are processed in each test phase, ensuring scalability of the framework. Interestingly, again, runtime is higher to process the separate test set than the test sets obtained in the cross-validation. Since most content in Digital Libraries is in the form of text, there is an interest towards the application of Natural Language Processing (NLP) techniques that can extract valuable information from it in order to support various kinds of user activities. Most NLP techniques exploit linguistic resources that are languagespecific, costly and error prone to produce manually, which motivates research for automatic ways to build them. Carrying on previous work on the BLA-BLA tool for learning several kinds of linguistic resources, this paper focuses on Grammar Induction. In particular, it investigates the ability of advanced process mining and management techniques to automatically learn, from a set of texts in a given language, effective models to check grammatical correctness of sentences in that language. In particular, it works on WoMan, a declarative process mining system that proved able to learn effective models in several application domains. Experimental results show that the approach is effective for grammar checking and allows interesting analysis of the collections in a DL. Other possible applications to DLs can be also immediately envisaged, such as classification of the linguistic style in the form of process prediction, and will be further investigated. Future work will aim at extracting an explicit grammar from the learned models. This should be feasible, since WoMan models already learn grammatical rules made up of just single PoS tags, and so more complex rules might be obtained by suitable combinations thereof.","refs":[]}]}],"tables":{"tab_0":{"heading":"Table 1 .","description":"Example of a sentence in CoNLL-U format","rows":[["ID FORM","LEMMA UPOS","XPOS FEATS","HEAD DEPREL DEPS MISC"],["#sent id = isst tanl-3","","","","","","",""],["#text = \"Evatuata la Tate Gallery.\"","","","","",""],["1","Evacuata Evacuare VERB","V","Gender=Fem|","3","acl","-","-"],["","","","","","Number=Sing|","","","",""],["","","","","","Tense=Past|","","","",""],["","","","","","VerbForm=Part","","","",""],["2","la","il","DET","RD","Definite=Def|","3","det","-","-"],["","","","","","Gender=Fem|","","","",""],["","","","","","Number=Sing|","","","",""],["","","","","","PronType=Art","","","",""],["3","Tate","Tate","PROPN SP","-","0","root","-",""],["4","Gallery","Gallery","PROPN SP","-","3","flat:name -","-"],["5",".",".","PUNCT FS","-","3","punct","-","-"]]},"tab_1":{"heading":"Table 2 .","description":"Datasets statistics","rows":[["Dataset","Corpus + Event log","","",""],["","Training set","Test set",""],["","#sent #token #event #act","#sent #token #event #act"],["UD Italian-ISDT 13121 257616 784078 294397 482","9680","29632","11153"],["PoSTWITA-UD","5638","99441 266854 103553 674","12668","31910","12109"]]},"tab_2":{"heading":"Table 3 .","description":"Model statistics","rows":[["Dataset","WoMan","","","Inthelex",""],["","Type","#task #trans time #ex","Pre-Conds","Post-Conds"],["","","","","","#rules time #rules time"],["UD Italian-ISDT UPOS","17","273","0,04 294397 24","0,02 19","0,02"],["","UPOS-XPOS 51","905","0,04","66","0,01 43","0,03"],["PoSTWITA-UD UPOS","17","307","0,04 103553 21","0,01 17","0,02"],["","UPOS-XPOS 45","1082","0,04","56","0,01 49","0,03"]]},"tab_3":{"heading":"Table 4 .","description":"Performance statistic","rows":[["","Measures","ISDT training","PoSTWITA training"],["","","UPOS UPOS+XPOS UPOS UPOS+XPOS"],["Train and","Accuracy","100% 99%","91%","43%"],["test","","","","",""],["","Support","19%","19%","13%","12%"],["","Test runtime (min)","2.57","2.24","3.5","4.79"],["10-fold cross","Accuracy","99%","99%","99%","97%"],["validation","","","","",""],["","Support","20%","20%","42%","43%"],["","Runtime per fold (min) 1.5","2.4","2.99","4.77"],["6 Conclusions and Future Work","",""]]}},"abstract":{"title":"Abstract","paragraphs":[{"text":"Since most content in Digital Libraries and Archives is text, there is an interest in the application of Natural Language Processing (NLP) to extract valuable information from it in order to support various kinds of user activities. Most NLP techniques exploit linguistic resources that are language-specific, costly and error prone to produce manually, which motivates research for automatic ways to build them.","refs":[]},{"text":"This paper extends the BLA-BLA tool for learning linguistic resources, adding a Grammar Induction feature based on the advanced process mining and management system WoMan. Experimental results are encouraging, envisaging interesting applications to Digital Libraries and motivating further research aimed at extracting an explicit grammar from the learned models.","refs":[]}]}}