{"bibliography":{"title":"Re-implementing and Extending Relation Network for R-CBIR","authors":[{"person_name":{"surname":"Messina","first_name":"Nicola"},"affiliations":[{"department":null,"institution":"ISTI-CNR","laboratory":null}],"email":"nicola.messina@isti.cnr.it"},{"person_name":{"surname":"Amato","first_name":"Giuseppe"},"affiliations":[{"department":null,"institution":"ISTI-CNR","laboratory":null}],"email":"giuseppe.amato@isti.cnr.it"},{"person_name":{"surname":"Falchi","first_name":"Fabrizio"},"affiliations":[{"department":null,"institution":"ISTI-CNR","laboratory":null}],"email":"fabrizio.falchi@isti.cnr.it"}],"date":null,"ids":{"DOI":"10.1007/978-3-030-39905-4_9","arXiv":null},"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"keywords":["Deep learning","Image retrieval","Visual features","Relation network"],"citations":{"b0":{"title":"VQA: visual question answering","authors":[{"person_name":{"surname":"Antol","first_name":"S"},"affiliations":[],"email":null}],"date":{"year":"2015","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":2425,"to_page":2433}}},"b1":{"title":"Joint embeddings of scene graphs and images","authors":[{"person_name":{"surname":"Belilovsky","first_name":"E"},"affiliations":[],"email":null},{"person_name":{"surname":"Blaschko","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Kiros","first_name":"J"},"affiliations":[],"email":null},{"person_name":{"surname":"Urtasun","first_name":"R"},"affiliations":[],"email":null},{"person_name":{"surname":"Zemel","first_name":"R"},"affiliations":[],"email":null}],"date":{"year":"2017","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b2":{"title":"Accurate, large minibatch SGD: training imageNet in 1 hour","authors":[{"person_name":{"surname":"Goyal","first_name":"P"},"affiliations":[],"email":null}],"date":{"year":"2017","month":null,"day":null},"ids":null,"target":"http://arxiv.org/abs/1706.02677","publisher":null,"journal":null,"series":null,"scope":null},"b3":{"title":"Learning to reason: endto-end module networks for visual question answering","authors":[{"person_name":{"surname":"Hu","first_name":"R"},"affiliations":[],"email":null},{"person_name":{"surname":"Andreas","first_name":"J"},"affiliations":[],"email":null},{"person_name":{"surname":"Rohrbach","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Darrell","first_name":"T"},"affiliations":[],"email":null},{"person_name":{"surname":"Saenko","first_name":"K"},"affiliations":[],"email":null}],"date":{"year":"2017","month":"10","day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b4":{"title":"CLEVR: a diagnostic dataset for compositional language and elementary visual reasoning","authors":[{"person_name":{"surname":"Johnson","first_name":"J"},"affiliations":[],"email":null},{"person_name":{"surname":"Hariharan","first_name":"B"},"affiliations":[],"email":null},{"person_name":{"surname":"Van Der Maaten","first_name":"L"},"affiliations":[],"email":null},{"person_name":{"surname":"Fei-Fei","first_name":"L"},"affiliations":[],"email":null},{"person_name":{"surname":"Zitnick","first_name":"C"},"affiliations":[],"email":null},{"person_name":{"surname":"Girshick","first_name":"R"},"affiliations":[],"email":null}],"date":{"year":"2017","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b5":{"title":"Inferring and executing programs for visual reasoning","authors":[{"person_name":{"surname":"Johnson","first_name":"J"},"affiliations":[],"email":null}],"date":{"year":"2017","month":"10","day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b6":{"title":"Image retrieval using scene graphs","authors":[{"person_name":{"surname":"Johnson","first_name":"J"},"affiliations":[],"email":null}],"date":{"year":"2015","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":3668,"to_page":3678}}},"b7":{"title":"FigureQA: an annotated figure dataset for visual reasoning","authors":[{"person_name":{"surname":"Kahou","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Atkinson","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Michalski","first_name":"V"},"affiliations":[],"email":null},{"person_name":{"surname":"Kádár","first_name":"Á"},"affiliations":[],"email":null},{"person_name":{"surname":"Trischler","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Bengio","first_name":"Y"},"affiliations":[],"email":null}],"date":{"year":"2017","month":null,"day":null},"ids":null,"target":"http://arxiv.org/abs/1710.07300","publisher":null,"journal":null,"series":null,"scope":null},"b8":{"title":"SGDR: stochastic gradient descent with warm restarts","authors":[{"person_name":{"surname":"Loshchilov","first_name":"I"},"affiliations":[],"email":null},{"person_name":{"surname":"Hutter","first_name":"F"},"affiliations":[],"email":null}],"date":{"year":"2017","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"ICLR","series":null,"scope":null},"b9":{"title":"R-VQA: learning visual relation facts with semantic attention for visual question answering","authors":[{"person_name":{"surname":"Lu","first_name":"P"},"affiliations":[],"email":null},{"person_name":{"surname":"Ji","first_name":"L"},"affiliations":[],"email":null},{"person_name":{"surname":"Zhang","first_name":"W"},"affiliations":[],"email":null},{"person_name":{"surname":"Duan","first_name":"N"},"affiliations":[],"email":null},{"person_name":{"surname":"Zhou","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Wang","first_name":"J"},"affiliations":[],"email":null}],"date":{"year":"2018","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b10":{"title":"A multi-world approach to question answering about real-world scenes based on uncertain input","authors":[{"person_name":{"surname":"Malinowski","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Fritz","first_name":"M"},"affiliations":[],"email":null}],"date":null,"ids":null,"target":null,"publisher":null,"journal":null,"series":"Advances in Neural Information Processing Systems","scope":{"volume":27,"pages":{"from_page":1682,"to_page":1690}}},"b11":{"title":"Transparency by design: closing the gap between performance and interpretability in visual reasoning","authors":[{"person_name":{"surname":"Mascharka","first_name":"D"},"affiliations":[],"email":null},{"person_name":{"surname":"Tran","first_name":"P"},"affiliations":[],"email":null},{"person_name":{"surname":"Soklaski","first_name":"R"},"affiliations":[],"email":null},{"person_name":{"surname":"Majumdar","first_name":"A"},"affiliations":[],"email":null}],"date":{"year":"2018","month":"06","day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b12":{"title":"Learning relationshipaware visual features","authors":[{"person_name":{"surname":"Messina","first_name":"N"},"affiliations":[],"email":null},{"person_name":{"surname":"Amato","first_name":"G"},"affiliations":[],"email":null},{"person_name":{"surname":"Carrara","first_name":"F"},"affiliations":[],"email":null},{"person_name":{"surname":"Falchi","first_name":"F"},"affiliations":[],"email":null},{"person_name":{"surname":"Gennaro","first_name":"C"},"affiliations":[],"email":null}],"date":{"year":"2019","month":null,"day":null},"ids":{"DOI":"10.1007/978-3-030-11018-5_40","arXiv":null},"target":"https://doi.org/10.1007/978-3-030-11018-540","publisher":"Springer","journal":null,"series":null,"scope":{"volume":11132,"pages":{"from_page":486,"to_page":501}}},"b13":{"title":"Learning visual features for relational CBIR","authors":[{"person_name":{"surname":"Messina","first_name":"N"},"affiliations":[],"email":null},{"person_name":{"surname":"Amato","first_name":"G"},"affiliations":[],"email":null},{"person_name":{"surname":"Carrara","first_name":"F"},"affiliations":[],"email":null},{"person_name":{"surname":"Falchi","first_name":"F"},"affiliations":[],"email":null},{"person_name":{"surname":"Gennaro","first_name":"C"},"affiliations":[],"email":null}],"date":{"year":"2019","month":null,"day":null},"ids":{"DOI":"10.1007/s13735-019-00178-7","arXiv":null},"target":"https://doi.org/10.1007/s13735-019-00178-7","publisher":null,"journal":"Int. J. Multimedia Inf. Retr","series":null,"scope":{"volume":null,"pages":{"from_page":1,"to_page":12}}},"b14":{"title":"Discovering objects and their relations from entangled scene representations","authors":[{"person_name":{"surname":"Raposo","first_name":"D"},"affiliations":[],"email":null},{"person_name":{"surname":"Santoro","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Barrett","first_name":"D"},"affiliations":[],"email":null},{"person_name":{"surname":"Pascanu","first_name":"R"},"affiliations":[],"email":null},{"person_name":{"surname":"Lillicrap","first_name":"T"},"affiliations":[],"email":null},{"person_name":{"surname":"Battaglia","first_name":"P"},"affiliations":[],"email":null}],"date":{"year":"2017","month":null,"day":null},"ids":null,"target":"https://openreview.net/forum?id=rkrjrvmKl","publisher":null,"journal":null,"series":"Workshop Track Proceedings","scope":null},"b15":{"title":"Exploring models and data for image question answering","authors":[{"person_name":{"surname":"Ren","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Kiros","first_name":"R"},"affiliations":[],"email":null},{"person_name":{"surname":"Zemel","first_name":"R"},"affiliations":[],"email":null}],"date":null,"ids":null,"target":null,"publisher":null,"journal":null,"series":"Advances in Neural Information Processing Systems","scope":{"volume":28,"pages":{"from_page":2953,"to_page":2961}}},"b16":{"title":"A simple neural network module for relational reasoning","authors":[{"person_name":{"surname":"Santoro","first_name":"A"},"affiliations":[],"email":null}],"date":null,"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":30,"pages":{"from_page":4967,"to_page":4976}}},"b17":{"title":"Don't decay the learning rate, increase the batch size","authors":[{"person_name":{"surname":"Smith","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Kindermans","first_name":"P"},"affiliations":[],"email":null},{"person_name":{"surname":"Ying","first_name":"C"},"affiliations":[],"email":null},{"person_name":{"surname":"Le","first_name":"Q"},"affiliations":[],"email":null}],"date":{"year":"2018","month":null,"day":null},"ids":null,"target":"https://openreview.net/pdf?id=B1Yy1BxCZ","publisher":null,"journal":null,"series":null,"scope":null},"b18":{"title":"Sequence to sequence learning with neural networks","authors":[{"person_name":{"surname":"Sutskever","first_name":"I"},"affiliations":[],"email":null},{"person_name":{"surname":"Vinyals","first_name":"O"},"affiliations":[],"email":null},{"person_name":{"surname":"Le","first_name":"Q"},"affiliations":[],"email":null}],"date":null,"ids":null,"target":null,"publisher":null,"journal":null,"series":"Advances in Neural Information Processing Systems","scope":{"volume":27,"pages":{"from_page":3104,"to_page":3112}}},"b19":{"title":"Stacked attention networks for image question answering","authors":[{"person_name":{"surname":"Yang","first_name":"Z"},"affiliations":[],"email":null},{"person_name":{"surname":"He","first_name":"X"},"affiliations":[],"email":null},{"person_name":{"surname":"Gao","first_name":"J"},"affiliations":[],"email":null},{"person_name":{"surname":"Deng","first_name":"L"},"affiliations":[],"email":null},{"person_name":{"surname":"Smola","first_name":"A"},"affiliations":[],"email":null}],"date":{"year":"2016","month":"06","day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b20":{"title":"Large-scale visual relationship understanding","authors":[{"person_name":{"surname":"Zhang","first_name":"J"},"affiliations":[],"email":null},{"person_name":{"surname":"Kalantidis","first_name":"Y"},"affiliations":[],"email":null},{"person_name":{"surname":"Rohrbach","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Paluri","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Elgammal","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Elhoseiny","first_name":"M"},"affiliations":[],"email":null}],"date":{"year":"2019","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":33,"pages":{"from_page":9185,"to_page":9194}}},"b21":{"title":"Simple baseline for visual question answering","authors":[{"person_name":{"surname":"Zhou","first_name":"B"},"affiliations":[],"email":null},{"person_name":{"surname":"Tian","first_name":"Y"},"affiliations":[],"email":null},{"person_name":{"surname":"Sukhbaatar","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Szlam","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Fergus","first_name":"R"},"affiliations":[],"email":null}],"date":{"year":"2015","month":null,"day":null},"ids":null,"target":"http://arxiv.org/abs/1512.02167","publisher":null,"journal":null,"series":null,"scope":null}},"sections":[{"title":"Introduction","paragraphs":[{"text":"In the growing area of Computer Vision (CV), state-of-the-art Deep Learning methods show impressive results in tasks such as classifying or recognizing objects in images. Several recent studies, however, demonstrated the difficulties of such architectures to intrinsically understand a complex scene to catch spatial, temporal and abstract relationships among objects.","refs":[]},{"text":"One of the most prominent fields of Deep Learning applied to CV within which these ideas are being tested is Relational Visual Question Answering (R-VQA). This task consists in answering to a question asked on a particular input image. While in standard VQA the question usually concerns single objects and their attributes, the R-VQA questions inquire about relationships between multiple objects in the image. R-VQA is considered a challenging task for current state-of-the-art deep learning models since it requires a range of different reasoning capabilities. In fact, in addition to finding and classifying objects inside the image or understanding the meaning of each word of the input question, it is necessary to understand what are the relationships connecting visual objects and it is required to link together learned textual and visual representations.","refs":[]},{"text":"This work is about implementing and training the Relation Network architecture (RN) [17]. Our final goal was to extend the RN to extract visual relationshipaware features for a novel task that we called Relational Content-Based Image Retrieval (R-CBIR). The R-CBIR task consists in finding all the images in a dataset that contains objects in similar relationships with respect to the ones present in a given query image.","refs":[{"start":84,"end":88,"marker":"bibr","target":"#b16"}]},{"text":"Specifically, in [13] and [14] we introduced some extensions to the original RN module, able to extract visual relationship-aware features for efficiently characterizing complex inter-object relationships. We trained our RN variants on the CLEVR R-VQA task and we demonstrated that the extracted visual features were suitable for the novel R-CBIR task.","refs":[{"start":17,"end":21,"marker":"bibr","target":"#b12"},{"start":26,"end":30,"marker":"bibr","target":"#b13"}]},{"text":"The high-level relational understanding could become a fundamental building block in digital libraries, where multi-modal information has to be processed in smart and scalable ways. Furthermore, R-CBIR encourages the development of solutions able to produce efficient yet powerful relationships-aware features, capable of efficiently describing the large number of inter-object relationships present in a digital library. A digital library, in fact, is composed of a large amount of multi-modal objects: it contains both multimedia elements (images, audio, videos) and text. One interesting challenge in digital libraries is finding relationships either between cross-domain data (e.g., a newspaper article with the related video in the newscast) or between the individual objects that are contained in a single multimedia element (e.g., the spatial arrangement of furniture in a picture of a room). This is a must for constructing strong and high-level interconnections between inter-and intra-domain data, to efficiently collect and manage knowledge.","refs":[]},{"text":"The first step was re-implementing the RN architecture and training it on the CLEVR dataset, using the same setup detailed in the original work [17]. This was a necessary step since the original code was not published. RN was originally proposed by Deep Mind, a company owned by Google and our code is the first public working implementation of RN1 on the CLEVR dataset. Thus, it is already largely used.","refs":[{"start":144,"end":148,"marker":"bibr","target":"#b16"},{"start":347,"end":348,"marker":null,"target":"#foot_0"}]},{"text":"We found different issues during the replication process. Hence, in this paper, we give many details about the problems we addressed during the implementation of the original version of RN. In the end, we were able to successfully train this architecture reaching an accuracy of 93,6% on the CLEVR R-VQA task.","refs":[]}]},{"title":"Related Work","paragraphs":[{"text":"R-VQA. R-VQA comes from the task of VQA (Visual Question Answering). Plain VQA consists in giving the correct answer to a question asked on a given picture, so it requires connecting together different entities coming from heterogeneous representations (text and visuals).","refs":[]},{"text":"Some works [20,22] proposed approaches to standard VQA problems on datasets such as VQA [1], DAQUAR [11], COCO-QA [16].","refs":[{"start":11,"end":15,"marker":"bibr","target":"#b19"},{"start":15,"end":18,"marker":"bibr","target":"#b21"},{"start":88,"end":91,"marker":"bibr","target":"#b0"},{"start":100,"end":104,"marker":"bibr","target":"#b10"},{"start":114,"end":118,"marker":"bibr","target":"#b15"}]},{"text":"Recently, there is the tendency to conceptually separate VQA and R-VQA. In R-VQA, in fact, images contain difficult inter-object relationships, and question are formulated in a way that it is impossible for deep architectures to answer correctly without having understood high-level interactions between the objects in the same image. Some datasets, such as CLEVR [5], RVQA [10], FigureQA [8], move the attention towards this new challenging task.","refs":[{"start":364,"end":367,"marker":"bibr","target":"#b4"},{"start":374,"end":378,"marker":"bibr","target":"#b9"},{"start":389,"end":392,"marker":"bibr","target":"#b7"}]},{"text":"In this work, we address the R-VQA task by employing the CLEVR dataset. CLEVR is a synthetic dataset composed of 3D rendered scenes. It contains simple yet photorealistic 3D shapes, and it is suitable for testing out, in a fully controlled environment, the intrinsic relational abilities of deep neural networks.","refs":[]},{"text":"On the CLEVR dataset, [17] and [15] proposed a novel architecture specialized to think in a relational way. They introduced a particular layer called Relation Network (RN), which is specialized in comparing pairs of objects. Objects representations are learned by means of a four-layer CNN, and the question embedding is generated through an LSTM. The overall architecture, composed of CNN, LSTM, and the RN, can be trained fully end-to-end, and it is able to reach superhuman performances. Other solutions [4,6] introduce compositional approaches able to explicitly model the reasoning process by dynamically building a reasoning graph that states which operations must be carried out and in which order to obtain the right answer.","refs":[{"start":22,"end":26,"marker":"bibr","target":"#b16"},{"start":31,"end":35,"marker":"bibr","target":"#b14"},{"start":507,"end":510,"marker":"bibr","target":"#b3"},{"start":510,"end":512,"marker":"bibr","target":"#b5"}]},{"text":"To close the performance gap between interpretable architectures and high performing solutions, [12] proposed a set of visual-reasoning primitives that are able to perform complex reasoning tasks in an explicitly interpretable manner. R-CBIR. On the R-CBIR task there was some experimentation using both CLEVR and real-world datasets. [7] introduced a CRF model able to ground relationships given in the form of a scene graph to test images for image retrieval purposes. However, this model is not able to produce a compact feature. They employed a simple dataset composed of 5000 images and annotated with objects and their relationships.","refs":[{"start":96,"end":100,"marker":"bibr","target":"#b11"},{"start":335,"end":338,"marker":"bibr","target":"#b6"}]},{"text":"More recently, using the Visual Genome dataset, [21] implemented a large scale image retrieval system able to map textual triplets into visual ones (objectsubject-relation inferred from the image) projecting them into a common space learned through a modified version of triplet-loss.","refs":[{"start":48,"end":52,"marker":"bibr","target":"#b20"}]},{"text":"The works by [2,13,14] exploit the graph data associated with every image in order to produce ranking goodness metrics, such as nDCG and Spearman-Rho ranking correlation indexes. Their objective was evaluating the quality of the ranking produced for a given query, keeping into consideration the relational content of every scene. In particular, our previous works [13,14] analyzed two architectures for extracting relational data by exploiting knowledge acquired through R-VQA.","refs":[{"start":13,"end":16,"marker":"bibr","target":"#b1"},{"start":16,"end":19,"marker":"bibr","target":"#b12"},{"start":19,"end":22,"marker":"bibr","target":"#b13"},{"start":365,"end":369,"marker":"bibr","target":"#b12"},{"start":369,"end":372,"marker":"bibr","target":"#b13"}]}]},{"title":"Original Setup","paragraphs":[{"text":"The overall architecture and the initial hyper-parameters we used in our code come from the original paper. Following, we briefly review this original setup.","refs":[]},{"text":"The Relation Network (RN) [17] approached the task of R-VQA and obtained remarkable results on the CLEVR dataset. RN modules combine input objects forming all possible pairs and applies a common transformation to them, producing activations aimed to store information about possible relationships among input objects. For the specific task of R-VQA, authors used a four-layer CNN to learn visual object representations, that are then fed to the RN module and combined with the textual embedding of the question produced by an LSTM, conditioning the relationship information on the textual modality. The core of the RN module is given by the following:","refs":[{"start":26,"end":30,"marker":"bibr","target":"#b16"}]},{"text":"where g θ is a parametric function whose parameters θ can be learned during the training phase. Specifically, it is a multi-layer perceptron (MLP) network. o i and o j are the objects forming the pair under consideration, and q is the question embedding vector obtained from the LSTM module. The answer is then predicted by a downstream network f φ followed by a softmax layer that outputs probabilities for every answer:","refs":[]},{"text":"(2) During our implementation, we followed the guidelines and the hyperparameters configuration by the authors. In particular, we setup the architecture as follows (Fig. 1 depicts the overall architecture):","refs":[{"start":170,"end":171,"marker":"figure","target":"#fig_0"}]},{"text":"-the CNN is composed of 4 convolutional layers each with 24 kernels, ReLU non-linearities and batch normalization; -g θ and f φ are multilayer perceptrons. They are composed of 4 and 2 fullyconnected layers of 256 neurons each. Every layer is followed by the ReLU non-linearity; -a final linear layer with 28 units produces logits for a softmax layer over the answers vocabulary; -dropout with 50% dropping probability is inserted after the penultimate layer of f φ ; -the training is performed using the Adam optimizer, with a learning rate of 1e -4 .","refs":[]},{"text":"We took some decisions that probably brought our code to differ substantially from the original authors implementation. Concerning question processing, we built the dictionaries by sequentially scanning all the questions in the dataset. The zero index was used as padding during the embedding phase. We assumed uni-directional LSTM for question processing. Also, in the first place, we did not consider learning rate schedules nor dataset balancing procedures.","refs":[]}]},{"title":"Preliminary Results","paragraphs":[{"text":"When training using the original configuration, we reached an accuracy plateau at around 53% on the validation set, while the authors claimed an accuracy of 95,5%.","refs":[]},{"text":"We broke down the accuracy for the different question types, to have a better insight of what the network was learning. The validation accuracy curves are reported in Fig. 2. This plot shows that the trained model was completely blind to the concepts of color and material since their accuracy was perfectly compatible with uniform random outcomes. However, even considering the other question types, the model was not performing as expected.","refs":[{"start":172,"end":173,"marker":"figure","target":"#fig_1"}]},{"text":"These results motivated us to concentrate on some implementation-level details that could help the network convergence.","refs":[]},{"text":"In particular, we collected a list of some critical implementation details that may have played a role in the network training failure:","refs":[]},{"text":"punctuation-level tokenization: initially, we did not consider the punctuation as separate elements in the dictionary, so that sentences like \"There is a small cube behind the red object. What is its color?\" generated words like \"object.\" and \"color?\". Instead, one possibility was to break them down into four different entries: \"object\", \".\", \"color\", and \"?\".","refs":[]},{"text":"training regularization: in order to regularize training, we thought of adopting standard regularization procedures, like weight-decay and gradient clipping. -question inversion: even if this trick applies to sentence translation models [19], it has been observed that feeding the LSTM with the reversed sentence often brings to an overall higher accuracy. -SGD optimizer: the SGD optimizer is overall slower but asymptotically often performs slightly better than Adam. -answers balancing: the answers distribution is not uniform in the CLEVR dataset. This could have caused problems during the training since less likely examples were penalized. One possible solution was trying to build batches in which all the answers were equally likely. -CNN pretraining: to help the whole architecture to converge faster, we though of initializing the CNN parameters independently, by employing an easier non-relational task. In particular, we trained the CNN using a multilabel classification task, whose aim was to find out the attributes of all the objects inside the CLEVR scene. We aimed to bring the CNN parameters in a zone of the parameters space suitable for the downstream R-VQA task. -learning rate and batch size schedulers: according to some detailed research on neural network parameters optimization, schedulers have a key role during training. We managed to try different schedulers to see if they could move the network parameters away from local minima.","refs":[{"start":237,"end":241,"marker":"bibr","target":"#b18"}]}]},{"title":"Improvements","paragraphs":[{"text":"Following, we report our findings after experimenting with different variations of the original implementation.","refs":[]},{"text":"Punctuation-Level Tokenization. First of all, we implemented the punctuationlevel tokenization for processing the input questions. However, we immediately measured a strong drop in the validation accuracy, from 53% to 20%. This could be due to the fact that the network was effectively using the word-punctuation tokens (e.g. \"color?\") for easily discerning the question type and better attending to the key question details.","refs":[]},{"text":"Training Regularization. Gradient clipping and weight decay helped to stabilize the training. However, they did not change the accuracy in any significant way. Question Inversion. We tried feeding the questions to the LSTM in reverse order. With these changes, accuracy moved from 53% to around 66%. It turned out that the question inversion was a key implementation detail.","refs":[]},{"text":"To understand how the network outputs were distributed after these changes, we prepared the confusion matrix measured on the validation accuracy (Fig. 3). In CLEVR scenario, there are 28 possible answers and they are clustered in 6 classes: numbers, size, color, material, shape, exists. In the confusion plot, there are 6 diagonal blocks corresponding to these classes. Empty entries outside the squared diagonal blocks show that answers falling outside their class were extremely unlikely. This was an important finding: the network was perfectly able to understand what kind of answer should be given in output (e.g. a binary yes/no answer rather than a color), but it was not able to figure out the correct label within that class. SGD Optimizer. We tried training the network using the SGD optimizer, using the same learning rate employed with Adam (1e -4 ). Unfortunately, the training process using SGD was too slow to collect some useful insights. In particular, during the first 50 epochs the architecture remained completely unable to solve the number and the color classes. Also, the model trained with SGD was not able to understand the 6 different question types, while with Adam this happened already during the first 5-7 epochs.","refs":[{"start":151,"end":152,"marker":"figure","target":"#fig_2"}]},{"text":"CNN Pretraining. Although the CNN pretraining sped up the overall convergence during the first epochs, it did not improve the overall validation accuracy. This made us formulate the hypothesis that the problem could be not in the perception module, but rather in the reasoning one, probably in the core of the relation network. In fact, the multi-label classification task reached a mean average precision of 0.99, meaning that the CNN was perfectly able to attend to all the object attributes. The multi-label classification task was trained using 5000 and 750 training and validation images respectively. Answers Balancing. We wrote a custom batch sampler to ensure a uniform distribution among the answers. In this scenario, we obtained a better accuracy distribution among the different answer classes, w.r.t. the initial validation curves in Fig. 2. In particular, we observed that the model was no more color blind. However, once converged, the overall mean accuracy remained the same as in the initial experiment.","refs":[{"start":852,"end":853,"marker":"figure","target":"#fig_1"}]},{"text":"Schedulers. Initially, we tried standard learning rate schedulers, such as CosineAnnealing [9], Exponential, Step-Exponential, and ReduceOnPlateau. Unfortunately, none of them resulted in an accuracy boost, even trying different hyper-parameters such as the step size (in epochs) and the step multiplier. Accuracy started growing when we adopted the findings by [18], which suggested increasing the batch size instead of decreasing the learning rate. Our policy consisted in doubling the batch size every 45 training epochs, starting from 32 up to a maximum of 640. We experimented on the state description version of the dataset, in which the scene is already encoded in a tensor form suitable for the relation network so that the perception pipeline (the CNN module) is temporarily kept apart. During this experiment, the learning rate remained fixed. With this batch size scheduling policy, we obtained an accuracy of 85%.","refs":[{"start":91,"end":94,"marker":"bibr","target":"#b8"},{"start":362,"end":366,"marker":"bibr","target":"#b17"}]},{"text":"The best result, however, was reached using a warm-up learning rate scheduling policy similar to the one used in [3]. In particular, we doubled the learning rate every 20 epochs, from 1e -6 to 1e -4 . When experimenting on the state description version of CLEVR, we were able to reach an accuracy of 97,9%. We repeated the same experiment on the full CLEVR, training end-to-end from pixels and words to answers, and we finally obtained an accuracy of 93,6%. This value is fully compatible with the accuracy claimed by the authors of 95,5%. Validation curves from this training setup are reported in Fig. 4.","refs":[{"start":113,"end":116,"marker":"bibr","target":"#b2"},{"start":604,"end":605,"marker":"figure","target":"#fig_3"}]},{"text":"The final confusion matrix in Fig. 5 highlights the answers for which there are still problems. Overall, the only remaining issues reside in the number class. In fact, the network has still some difficulties when the objective for a particular question is counting many object instances (the number 9 is almost never output as answer). ","refs":[{"start":35,"end":36,"marker":"figure","target":"#fig_4"}]}]},{"title":"Conclusions","paragraphs":[{"text":"In this work, we re-implemented the Relation Network architecture [17]. After a few experimentations, we were not able to reach the accuracy claimed by the authors for the R-VQA task on the CLEVR dataset. For this reason, we conducted multiple experiments testing different architectural and implementation tweaks to make the network converge to the claimed accuracy values. In the end, we discovered that the learning rate warm-up scheduling policy was the main missing component. We were able to reach an accuracy of 93,6%, perfectly compatible with the one reached by the Deep Mind team.","refs":[{"start":66,"end":70,"marker":"bibr","target":"#b16"}]},{"text":"We used these results to develop some extensions of the original Relation Network, capable of producing relationships-aware visual features for the novel task of R-CBIR. We noticed that slight modifications to the original architecture to achieve our R-CBIR objectives did not affect the network convergence when using the described learning rate scheduling policy. In particular, in [13] the twostage RN (2S-RN) reached almost the same accuracy as the original architecture.","refs":[{"start":384,"end":388,"marker":"bibr","target":"#b12"}]},{"text":"Instead, the introduction of the in-network visual aggregation layer in the Aggregated Visual Features RN (AVF-RN) architecture [14] made the performance drop to around 65%. This was due to the strong visual features compression needed. However, we demonstrated that AVF-RN was still able to produce state-of-the-art relationships-aware visual features suitable for R-CBIR.","refs":[{"start":128,"end":132,"marker":"bibr","target":"#b13"}]}]}],"tables":{},"abstract":{"title":"Abstract","paragraphs":[{"text":"Relational reasoning is an emerging theme in Machine Learning in general and in Computer Vision in particular. Deep Mind has recently proposed a module called Relation Network (RN) that has shown impressive results on visual question answering tasks. Unfortunately, the implementation of the proposed approach was not public. To reproduce their experiments and extend their approach in the context of Information Retrieval, we had to re-implement everything, testing many parameters and conducting many experiments. Our implementation is now public on GitHub and it is already used by a large community of researchers. Furthermore, we recently presented a variant of the relation network module that we called Aggregated Visual Features RN (AVF-RN). This network can produce and aggregate at inference time compact visual relationship-aware features for the Relational-CBIR (R-CBIR) task. R-CBIR consists in retrieving images with given relationships among objects. In this paper, we discuss the details of our Relation Network implementation and more experimental results than the original paper. Relational reasoning is a very promising topic for better understanding and retrieving inter-object relationships, especially in digital libraries.","refs":[]}]}}