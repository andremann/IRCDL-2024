{"bibliography":{"title":"LamBERTa: Law Article Mining Based on Bert Architecture for the Italian Civil Code","authors":[{"person_name":{"surname":"Tagarelli","first_name":"Andrea"},"affiliations":[{"department":"Dept. Computer Engineering, Modeling, Electronics, and Systems Engineering (DIMES)","institution":"University of Calabria","laboratory":null}],"email":"andrea.tagarelli@unical.it"},{"person_name":{"surname":"Simeri","first_name":"Andrea"},"affiliations":[{"department":"Dept. Computer Engineering, Modeling, Electronics, and Systems Engineering (DIMES)","institution":"University of Calabria","laboratory":null}],"email":"andrea.simeri@dimes.unical.it"}],"date":null,"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"keywords":["Deep learning","Legal data","Artificial intelligence and law","Language models"],"citations":{"b0":{"title":"Unsupervised law article mining based on deep pre-trained language representation models with application to the Italian civil code","authors":[{"person_name":{"surname":"Tagarelli","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Simeri","first_name":"A"},"affiliations":[],"email":null}],"date":{"year":"2021","month":null,"day":null},"ids":{"DOI":"10.1007/s10506-021-09301-8","arXiv":null},"target":null,"publisher":null,"journal":"Artif. Intell. Law","series":null,"scope":{"volume":null,"pages":{"from_page":1,"to_page":57}}},"b1":{"title":"Modeling law search as prediction","authors":[{"person_name":{"surname":"Dadgostari","first_name":"F"},"affiliations":[],"email":null},{"person_name":{"surname":"Guim","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Beling","first_name":"P"},"affiliations":[],"email":null},{"person_name":{"surname":"Livermore","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Rockmore","first_name":"D"},"affiliations":[],"email":null}],"date":{"year":"2021","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"Artif. Intell. Law","series":null,"scope":{"volume":29,"pages":{"from_page":3,"to_page":34}}},"b2":{"title":"Learning to predict charges for criminal cases with legal basis","authors":[{"person_name":{"surname":"Luo","first_name":"B"},"affiliations":[],"email":null},{"person_name":{"surname":"Feng","first_name":"Y"},"affiliations":[],"email":null},{"person_name":{"surname":"Xu","first_name":"J"},"affiliations":[],"email":null},{"person_name":{"surname":"Zhang","first_name":"X"},"affiliations":[],"email":null},{"person_name":{"surname":"Zhao","first_name":"D"},"affiliations":[],"email":null}],"date":{"year":"2017","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":2727,"to_page":2736}}},"b3":{"title":"Interpretable charge predictions for criminal cases: Learning to generate court views from fact descriptions","authors":[{"person_name":{"surname":"Ye","first_name":"H"},"affiliations":[],"email":null},{"person_name":{"surname":"Jiang","first_name":"X"},"affiliations":[],"email":null},{"person_name":{"surname":"Luo","first_name":"Z"},"affiliations":[],"email":null},{"person_name":{"surname":"Chao","first_name":"W"},"affiliations":[],"email":null}],"date":{"year":"2018","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":1854,"to_page":1864}}},"b4":{"title":"Classifying sentential modality in legal language: a use case in financial regulations, acts and directives","authors":[{"person_name":{"surname":"O'neill","first_name":"J"},"affiliations":[],"email":null},{"person_name":{"surname":"Buitelaar","first_name":"P"},"affiliations":[],"email":null},{"person_name":{"surname":"Robin","first_name":"C"},"affiliations":[],"email":null},{"person_name":{"surname":"O'brien","first_name":"L"},"affiliations":[],"email":null}],"date":{"year":"2017","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":159,"to_page":168}}},"b5":{"title":"Obligation and Prohibition Extraction Using Hierarchical RNNs","authors":[{"person_name":{"surname":"Chalkidis","first_name":"I"},"affiliations":[],"email":null},{"person_name":{"surname":"Androutsopoulos","first_name":"I"},"affiliations":[],"email":null},{"person_name":{"surname":"Michos","first_name":"A"},"affiliations":[],"email":null}],"date":{"year":"2018","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":254,"to_page":259}}},"b6":{"title":"Legal question answering using ranking SVM and deep convolutional neural network","authors":[{"person_name":{"surname":"Do","first_name":"P"},"affiliations":[],"email":null},{"person_name":{"surname":"Nguyen","first_name":"H"},"affiliations":[],"email":null},{"person_name":{"surname":"Tran","first_name":"C"},"affiliations":[],"email":null},{"person_name":{"surname":"Nguyen","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Nguyen","first_name":"M"},"affiliations":[],"email":null}],"date":{"year":"2017","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b7":{"title":"BERT: pre-training of deep bidirectional transformers for language understanding","authors":[{"person_name":{"surname":"Devlin","first_name":"J"},"affiliations":[],"email":null},{"person_name":{"surname":"Chang","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Lee","first_name":"K"},"affiliations":[],"email":null},{"person_name":{"surname":"Toutanova","first_name":"K"},"affiliations":[],"email":null}],"date":{"year":"2019","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":4171,"to_page":4186}}},"b8":{"title":"Combining similarity and transformer methods for case law entailment","authors":[{"person_name":{"surname":"Rabelo","first_name":"J"},"affiliations":[],"email":null},{"person_name":{"surname":"Kim","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Goebel","first_name":"R"},"affiliations":[],"email":null}],"date":{"year":"2019","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":290,"to_page":296}}},"b9":{"title":"Neural legal judgment prediction in english","authors":[{"person_name":{"surname":"Chalkidis","first_name":"I"},"affiliations":[],"email":null},{"person_name":{"surname":"Androutsopoulos","first_name":"I"},"affiliations":[],"email":null},{"person_name":{"surname":"Aletras","first_name":"N"},"affiliations":[],"email":null}],"date":{"year":"2019","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":4317,"to_page":4323}}},"b10":{"title":"Easing legal news monitoring with learning to rank and BERT","authors":[{"person_name":{"surname":"Sanchez","first_name":"L"},"affiliations":[],"email":null},{"person_name":{"surname":"He","first_name":"J"},"affiliations":[],"email":null},{"person_name":{"surname":"Manotumruksa","first_name":"J"},"affiliations":[],"email":null},{"person_name":{"surname":"Albakour","first_name":"D"},"affiliations":[],"email":null},{"person_name":{"surname":"Martinez","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Lipani","first_name":"A"},"affiliations":[],"email":null}],"date":{"year":"2020","month":null,"day":null},"ids":null,"target":null,"publisher":"Springer","journal":null,"series":"Lecture Notes in Computer Science","scope":{"volume":12036,"pages":{"from_page":336,"to_page":343}}},"b11":{"title":"BERT-PLI: modeling paragraphlevel interactions for legal case retrieval","authors":[{"person_name":{"surname":"Shao","first_name":"Y"},"affiliations":[],"email":null},{"person_name":{"surname":"Mao","first_name":"J"},"affiliations":[],"email":null},{"person_name":{"surname":"Liu","first_name":"Y"},"affiliations":[],"email":null},{"person_name":{"surname":"Ma","first_name":"W"},"affiliations":[],"email":null},{"person_name":{"surname":"Satoh","first_name":"K"},"affiliations":[],"email":null},{"person_name":{"surname":"Zhang","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Ma","first_name":"S"},"affiliations":[],"email":null}],"date":{"year":"2020","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":3501,"to_page":3507}}},"b12":{"title":"LEGAL-BERT: the muppets straight out of law school","authors":[{"person_name":{"surname":"Chalkidis","first_name":"I"},"affiliations":[],"email":null},{"person_name":{"surname":"Fergadiotis","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Malakasiotis","first_name":"P"},"affiliations":[],"email":null},{"person_name":{"surname":"Aletras","first_name":"N"},"affiliations":[],"email":null},{"person_name":{"surname":"Androutsopoulos","first_name":"I"},"affiliations":[],"email":null}],"date":{"year":"2020","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b13":{"title":"JNLP team: Deep learning approaches for legal processing tasks in COLIEE 2021","authors":[{"person_name":{"surname":"Nguyen","first_name":"H"},"affiliations":[],"email":null},{"person_name":{"surname":"Nguyen","first_name":"P"},"affiliations":[],"email":null},{"person_name":{"surname":"Vuong","first_name":"T"},"affiliations":[],"email":null},{"person_name":{"surname":"Bui","first_name":"Q"},"affiliations":[],"email":null},{"person_name":{"surname":"Nguyen","first_name":"C"},"affiliations":[],"email":null},{"person_name":{"surname":"Dang","first_name":"T"},"affiliations":[],"email":null},{"person_name":{"surname":"Tran","first_name":"V"},"affiliations":[],"email":null},{"person_name":{"surname":"Nguyen","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Satoh","first_name":"K"},"affiliations":[],"email":null}],"date":{"year":"2021","month":null,"day":null},"ids":null,"target":"2106.13405","publisher":null,"journal":null,"series":null,"scope":null},"b14":{"title":"BERT-based ensemble methods with data augmentation for legal textual entailment in COLIEE statute law task","authors":[{"person_name":{"surname":"Yoshioka","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Aoki","first_name":"Y"},"affiliations":[],"email":null},{"person_name":{"surname":"Suzuki","first_name":"Y"},"affiliations":[],"email":null}],"date":{"year":"2021","month":null,"day":null},"ids":null,"target":null,"publisher":"ACM","journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":278,"to_page":284}}},"b15":{"title":"Deep contextualized word representations","authors":[{"person_name":{"surname":"Peters","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Neumann","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Iyyer","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Gardner","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Clark","first_name":"C"},"affiliations":[],"email":null},{"person_name":{"surname":"Lee","first_name":"K"},"affiliations":[],"email":null},{"person_name":{"surname":"Zettlemoyer","first_name":"L"},"affiliations":[],"email":null}],"date":{"year":"2018","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":2227,"to_page":2237}}},"b16":{"title":"Improving language understanding by generative pre-training","authors":[{"person_name":{"surname":"Radford","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Sutskever","first_name":"I"},"affiliations":[],"email":null}],"date":{"year":"2018","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b17":{"title":"Extreme Classification","authors":[{"person_name":{"surname":"Bengio","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Dembczynski","first_name":"K"},"affiliations":[],"email":null},{"person_name":{"surname":"Joachims","first_name":"T"},"affiliations":[],"email":null},{"person_name":{"surname":"Kloft","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Varma","first_name":"M"},"affiliations":[],"email":null}],"date":{"year":"2019","month":null,"day":null},"ids":{"DOI":"10.4230/DagRep.8.7.62","arXiv":null},"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b18":{"title":"How \"BERTology\" Changed the State-of-the-Art also for Italian NLP","authors":[{"person_name":{"surname":"Tamburini","first_name":"F"},"affiliations":[],"email":null}],"date":{"year":"2020","month":null,"day":null},"ids":null,"target":"-WS.org","publisher":null,"journal":null,"series":"CEUR Workshop Proceedings, CEUR","scope":{"volume":2769,"pages":null}},"b19":{"title":"AlBERTo: Italian BERT Language Understanding Model for NLP Challenging Tasks Based on Tweets","authors":[{"person_name":{"surname":"Polignano","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Basile","first_name":"P"},"affiliations":[],"email":null},{"person_name":{"surname":"De Gemmis","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Semeraro","first_name":"G"},"affiliations":[],"email":null},{"person_name":{"surname":"Basile","first_name":"V"},"affiliations":[],"email":null}],"date":{"year":"2019","month":null,"day":null},"ids":null,"target":"-WS.org","publisher":null,"journal":null,"series":"CEUR Workshop Proceedings, CEUR","scope":{"volume":2481,"pages":null}},"b20":{"title":"Fixing comma splices in italian with BERT","authors":[{"person_name":{"surname":"Puccinelli","first_name":"D"},"affiliations":[],"email":null},{"person_name":{"surname":"Demartini","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"D'aoust","first_name":"R"},"affiliations":[],"email":null}],"date":{"year":"2019","month":null,"day":null},"ids":null,"target":"-WS.org","publisher":null,"journal":null,"series":"CEUR Workshop Proceedings, CEUR","scope":{"volume":2481,"pages":null}},"b21":{"title":"Recurrent neural network for text classification with multi-task learning","authors":[{"person_name":{"surname":"Liu","first_name":"P"},"affiliations":[],"email":null},{"person_name":{"surname":"Qiu","first_name":"X"},"affiliations":[],"email":null},{"person_name":{"surname":"Huang","first_name":"X"},"affiliations":[],"email":null}],"date":{"year":"2016","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":2873,"to_page":2879}}},"b22":{"title":"Convolutional neural networks for sentence classification","authors":[{"person_name":{"surname":"Kim","first_name":"Y"},"affiliations":[],"email":null}],"date":{"year":"2014","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":1746,"to_page":1751}}},"b23":{"title":"Recurrent convolutional neural networks for text classification","authors":[{"person_name":{"surname":"Lai","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Xu","first_name":"L"},"affiliations":[],"email":null},{"person_name":{"surname":"Liu","first_name":"K"},"affiliations":[],"email":null},{"person_name":{"surname":"Zhao","first_name":"J"},"affiliations":[],"email":null}],"date":{"year":"2015","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":2267,"to_page":2273}}},"b24":{"title":"Text classification research with attention-based recurrent neural networks","authors":[{"person_name":{"surname":"Du","first_name":"C"},"affiliations":[],"email":null},{"person_name":{"surname":"Huang","first_name":"L"},"affiliations":[],"email":null}],"date":{"year":"2018","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"Int. J. Comput. Commun. Control","series":null,"scope":{"volume":13,"pages":{"from_page":50,"to_page":61}}},"b25":{"title":"Neural machine translation by jointly learning to align and translate","authors":[{"person_name":{"surname":"Bahdanau","first_name":"D"},"affiliations":[],"email":null},{"person_name":{"surname":"Cho","first_name":"K"},"affiliations":[],"email":null},{"person_name":{"surname":"Bengio","first_name":"Y"},"affiliations":[],"email":null}],"date":{"year":"2015","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b26":{"title":"Attention is all you need","authors":[{"person_name":{"surname":"Vaswani","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Shazeer","first_name":"N"},"affiliations":[],"email":null},{"person_name":{"surname":"Parmar","first_name":"N"},"affiliations":[],"email":null},{"person_name":{"surname":"Uszkoreit","first_name":"J"},"affiliations":[],"email":null},{"person_name":{"surname":"Jones","first_name":"L"},"affiliations":[],"email":null},{"person_name":{"surname":"Gomez","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Kaiser","first_name":"L"},"affiliations":[],"email":null},{"person_name":{"surname":"Polosukhin","first_name":"I"},"affiliations":[],"email":null}],"date":{"year":"2017","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":5998,"to_page":6008}}},"b27":{"title":"Few-shot charge prediction with discriminative legal attributes","authors":[{"person_name":{"surname":"Hu","first_name":"Z"},"affiliations":[],"email":null},{"person_name":{"surname":"Li","first_name":"X"},"affiliations":[],"email":null},{"person_name":{"surname":"Tu","first_name":"C"},"affiliations":[],"email":null},{"person_name":{"surname":"Liu","first_name":"Z"},"affiliations":[],"email":null},{"person_name":{"surname":"Sun","first_name":"M"},"affiliations":[],"email":null}],"date":{"year":"2018","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":487,"to_page":498}}},"b28":{"title":"Semi-supervised methods for explainable legal prediction","authors":[{"person_name":{"surname":"Branting","first_name":"K"},"affiliations":[],"email":null},{"person_name":{"surname":"Weiss","first_name":"B"},"affiliations":[],"email":null},{"person_name":{"surname":"Brown","first_name":"B"},"affiliations":[],"email":null},{"person_name":{"surname":"Pfeifer","first_name":"C"},"affiliations":[],"email":null},{"person_name":{"surname":"Chakraborty","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Ferro","first_name":"L"},"affiliations":[],"email":null},{"person_name":{"surname":"Pfaff","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Yeh","first_name":"A"},"affiliations":[],"email":null}],"date":{"year":"2019","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":22,"to_page":31}}},"b29":{"title":"Explainable AI under contract and tort law: legal incentives and technical challenges","authors":[{"person_name":{"surname":"Hacker","first_name":"P"},"affiliations":[],"email":null},{"person_name":{"surname":"Krestel","first_name":"R"},"affiliations":[],"email":null},{"person_name":{"surname":"Grundmann","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Naumann","first_name":"F"},"affiliations":[],"email":null}],"date":{"year":"2020","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"Artif. Intell. Law","series":null,"scope":{"volume":28,"pages":{"from_page":415,"to_page":439}}}},"sections":[{"title":"Introduction","paragraphs":[{"text":"Modeling law search and retrieval as prediction problems has recently emerged as a predominant approach in law intelligence [2]. Predictive tasks in legal information systems have often been addressed as text classification problems, ranging from case classification and legal judgment prediction, to legislation norm classification, and statute prediction. Early studies have focused on statistical textual features and machine learning methods, then the progress of deep learning methods for text classification has prompted the development of deep neural network frameworks for several learning tasks, such as charge prediction [3,4], sentence modality classification [5,6], legal question answering [7].","refs":[{"start":124,"end":127,"marker":"bibr","target":"#b1"},{"start":631,"end":634,"marker":"bibr","target":"#b2"},{"start":634,"end":636,"marker":"bibr","target":"#b3"},{"start":671,"end":674,"marker":"bibr","target":"#b4"},{"start":674,"end":676,"marker":"bibr","target":"#b5"},{"start":703,"end":706,"marker":"bibr","target":"#b6"}]},{"text":"More recently, deep pre-trained language models, particularly the Bidirectional Encoder Representations from Transformers (BERT) [8], have emerged showing outstanding effectiveness in several NLP tasks. Thanks to their ability to learn a contextual language understanding model, they overcome the need for feature engineering (upon which classic, sparse vectorial representation models rely). Nonetheless, since these models are originally trained from generic domain corpora, they should not be directly applied to a specific domain corpus, as the distributional representation (embeddings) of their lexical units may significantly shift from the nuances and peculiarities expressed in domain-specific texts; this certainly holds for the legal domain as well, where language understanding is particularly challenging. In this respect, developing BERT models for legal documents has attracted increased attention, mostly concerning classification problems (e.g., [9,10,11,12,13,14,15]).","refs":[{"start":129,"end":132,"marker":"bibr","target":"#b7"},{"start":963,"end":966,"marker":"bibr","target":"#b8"},{"start":966,"end":969,"marker":"bibr","target":"#b9"},{"start":969,"end":972,"marker":"bibr","target":"#b10"},{"start":972,"end":975,"marker":"bibr","target":"#b11"},{"start":975,"end":978,"marker":"bibr","target":"#b12"},{"start":978,"end":981,"marker":"bibr","target":"#b13"},{"start":981,"end":984,"marker":"bibr","target":"#b14"}]}]},{"title":"Motivations for BERT-based approach.","paragraphs":[{"text":"Exploiting deep neural-network, pre-trained language modeling to solve the law article retrieval task has a number of key advantages that include the following. First, like any other deep neural network model, it totally avoids manual feature engineering, and hence the need for feature selection or relevance weighting methods (e.g., TF-IDF). Second, like sophisticated recurrent and convolutional neural networks, it models language semantics and non-linear relationships between terms; however, better than recurrent and convolutional neural networks, it is able to capture subtle and complex lexical patterns including the sequential structure and long-term dependencies, thus obtaining the most comprehensive local and global feature representations of a text sequence. Third, it incorporates the so-called attention mechanism, which allows a learning model to assign higher weight to text features according to their higher informativeness or relevance to the learning task. Fourth, being an effective bidirectional Transformer model, it overcomes the main limitations of early deep contextualized models (e.g., ELMO) [16] or decoder-based Transformer models (e.g., GPT) [17].","refs":[{"start":1124,"end":1128,"marker":"bibr","target":"#b15"},{"start":1177,"end":1181,"marker":"bibr","target":"#b16"}]},{"text":"Research problem and challenges. The problem we tackle in this paper is law article retrieval, i.e., finding articles of interest out of a legal corpus that can be recommended as an appropriate response to a query expressing a legal matter. We assume that any query is expressed in natural language and discusses a legal subject that is in principle covered by the target law code corpus; moreover, a query is assumed to be free of references to any article identifier in the law code.","refs":[]},{"text":"We address the law article retrieval problem based on the supervised machine learning paradigm: given a user-provided instance, i.e., a legal question, the goal is to automatically predict the category associated to the posed question, or more in general, to compute the probability distribution over all the predefined categories. In our context, the prediction is carried out by a machine learning system that must be trained on a target law code, in order to learn a classifier that will be used to perform the predictions against legal queries by exclusively utilizing the textual information contained in the law articles.","refs":[]},{"text":"Like any other machine learning method, using deep pre-trained models like BERT for classification tasks requires the availability of data annotated with the class labels, so to design the independent training and testing phases for the classifier. However, we have to cope with a prediction task that is challenging from different perspectives, which are summarized as follows:","refs":[]},{"text":"• The first challenge refers to the high number of classes, which are in the order of hundreds, or even thousands. • The second challenge corresponds to the so-called few-shot learning problem, i.e., dealing with a small amount of per-class examples to train a machine learning model, which Bengio et al. recognize as one of the \"extreme classification\" scenarios [18]. Indeed, the number of classes are thousands, resp. hundreds, as they correspond to the number of articles in the law code, resp. portion of it, that is used to train the law article retrieval model. Also, all available articles must be used for training the model, therefore it is not straightforward to select a test set from the target corpus. • The third challenge arises from our special interest in Italian law data, whereby we notice a lack of test query benchmarks for Italian legal article retrieval/prediction tasks.","refs":[{"start":364,"end":368,"marker":"bibr","target":"#b17"}]}]},{"title":"Our proposed approach","paragraphs":[{"text":"Our research aims to address all the aforementioned challenges. To this purpose, in [1], we investigate on the modeling, learning and understanding of civil-law-based corpora, and we propose LamBERTa -Law article mining based on BERT architecture, a BERT-based framework for law article retrieval as a prediction problem. LamBERTa is designed to fine-tune an Italian pre-trained BERT on the Italian Civil Code (ICC) as the target law code, for law article retrieval as prediction, i.e., given a natural language query, predict the most relevant ICC article(s). Notably, few works have been developed for Italian BERT-based models [19], such as a retrained BERT for various NLP tasks on Italian tweets [20], and a BERT-based masked-language model for spell correction [21]; however, to the best of our knowledge, no study leveraging BERT for the Italian civil-law has been proposed until [1].","refs":[{"start":84,"end":87,"marker":"bibr","target":"#b0"},{"start":630,"end":634,"marker":"bibr","target":"#b18"},{"start":701,"end":705,"marker":"bibr","target":"#b19"},{"start":767,"end":771,"marker":"bibr","target":"#b20"},{"start":887,"end":890,"marker":"bibr","target":"#b0"}]},{"text":"Data. The ICC is divided into six, logically coherent books, each in charge of providing rules for a particular civil law theme: Book-1, on Persons and the Family, articles 1-455 -contains the discipline of the juridical capacity of persons, of the rights of the personality, of collective organizations, of the family; Book-2, on Successions, articles 456-809 -contains the discipline of succession due to death and the donation contract; Book-3, on Property, articles 810-1172 -contains the discipline of ownership and other real rights; Book-4, on Obligations, articles 1173-2059 -contains the discipline of obligations and their sources, that is mainly of contracts and illicit facts (the so-called civil liability); Book-5, on Labor, articles 2060-2642 -contains the discipline of the company in general, of subordinate and self-employed work, of profit-making companies and of competition; Book-6, on the Protection of Rights, articles 2643-2969 -contains the discipline of the transcription, of the proofs, of the debtor's financial liability and of the causes of pre-emption, of the prescription.","refs":[]},{"text":"Overview of the LamBERTa framework. Figure 1 shows the conceptual architecture of LamBERTa. The starting point is a pre-trained Italian BERT model whose source data consists of a recent Wikipedia dump, various texts from the OPUS corpora collection, and the Italian part of the OSCAR corpus; the final training corpus has a size of 81GB and 13 138 379 147 tokens. 1LamBERTa models are generated by fine-tuning the pre-trained Italian BERT model on a sequence classification task (i.e., BERT with a single linear classification layer on top) given in input the articles of the ICC or a portion of it. This fine-tuning is accomplished by using a typical configuration of BERT for masked language modeling, with 12 attention heads and 12 hidden layers, and initial (i.e., pre-trained) vocabulary of 32 102 tokens. Each model was trained for 10 epochs, using cross-entropy as loss function, Adam optimizer and initial learning rate selected within [1e-5, 5e-5] on batches of 256 examples.","refs":[{"start":43,"end":44,"marker":"figure","target":"#fig_0"},{"start":364,"end":365,"marker":null,"target":"#foot_0"}]},{"text":"The LamBERTa architecture can be configured w.r.t. two model aspects: (i) the learning approach and (ii) the training-instance labeling scheme for a given corpus of ICC articles. As for the former, we consider two learning approaches, here dubbed global and local learning, respectively. A global model is trained on the whole ICC, whereas a local model is trained on a particular book of the ICC, which is seen as a logically coherent subset of the whole civil code. Either type of model is designed to be a classifier at article level, i.e., class labels correspond to the articles in the book(s) covered by the model.","refs":[]},{"text":"LamBERTa models are trained using WordPiece tokenization of the article sentences. To avoid subwording domain-specific (i.e., legal) terms, thus disrupting their semantics, we enrich the BERT vocabulary with a selection of terms from the ICC articles, before tokenization.","refs":[]},{"text":"Given the one-to-one association between classes and articles, and since the entire ICC must be used to embed the whole knowledge therein, a question becomes how to create as many training instances as possible for each article to make LamBERTa learn effectively. To this purpose, we devise various unsupervised schemes of labeling of the ICC articles, to create the training sets of LamBERTa models. These schemes adopt different strategies for selecting and combining portions from each article to derive the training set, while sharing the requirements of generating a minimum number of training units per article, here denoted as 𝑚𝑖𝑛𝑇 𝑈 ; moreover, since each article is usually comprised of few sentences, and 𝑚𝑖𝑛𝑇 𝑈 needs to be relatively large (we chose 32 as default value), each of the schemes implements a round-robin (RR) method that iterates over replicas of the same group of training units per article until at least 𝑚𝑖𝑛𝑇 𝑈 are generated. The most effective scheme turned out to be the unigram with parameterized emphasis on the title, which builds the set of training units for each article as comprised of two subsets: the one containing the article's sentences with round-robin selection, and the other one containing only replicas of the article's title.","refs":[]}]},{"title":"Experimental evaluation.","paragraphs":[{"text":"In [1], LamBERTa models are evaluated through an extensive experimental analysis by considering single-label evaluation as well as multi-label evaluation tasks, based on six different types of queries, which vary by source, length and lexical characteristics, and are summarized as follows: (Q1) Queries that correspond to randomly selected sentences from the articles of a book; (Q2) Same as Q1, but with paraphrasing of the queries; (Q3) Queries defined by comments on the ICC articles, i.e., annotations about the interpretation of the meanings and law implications associated to an article; (Q4) Same as Q3, but the comments are split into sentences; (Q5) Queries defined by case law decisions from the civil section of the Italian Court of Cassation that contains jurisprudential sentences associated with the ICC articles; (Q6) Queries defined by extracting the ICC metadata, i.e., headings of chapters, subchapters, and sections of each ICC book.","refs":[{"start":3,"end":6,"marker":"bibr","target":"#b0"}]},{"text":"The obtained results, which are reported in [1], have shown the effectiveness of LamBERTa w.r.t. all book-specific query sets, and its superiority against widely used deep-learning text classifiers, namely BiLSTM [22], a bidirectional LSTM model as sequence encoder, TextCNN [23], a convolutional-neural-network-based model with multiple filter widths for text encoding and classification, TextRCNN [24], a bidirectional LSTM with a pooling layer on the last sequence output, Seq2Seq-A [25,26], a Seq2Seq model with attention mechanism, and the Transformer model for text classification, which is adapted from the model originally proposed in [27] for machine translation. Also, we considered a few-shot learner conceived for an attribute-aware prediction task [28] that we have originally adapted based on the availability of ICC metadata.","refs":[{"start":44,"end":47,"marker":"bibr","target":"#b0"},{"start":213,"end":217,"marker":"bibr","target":"#b21"},{"start":275,"end":279,"marker":"bibr","target":"#b22"},{"start":399,"end":403,"marker":"bibr","target":"#b23"},{"start":486,"end":490,"marker":"bibr","target":"#b24"},{"start":490,"end":493,"marker":"bibr","target":"#b25"},{"start":643,"end":647,"marker":"bibr","target":"#b26"},{"start":761,"end":765,"marker":"bibr","target":"#b27"}]},{"text":"On both global and local learning scenarios, our LamBERTa models outperform all the above mentioned competing methods, which has confirmed our initial expectation on the superiority of LamBERTa in learning classification models from few per-class labeled examples under a tough multi-class classification scenario.","refs":[]},{"text":"Explainability. Explainability is one crucial aspect that typically arises in deep/machine learning models, and is clearly of high interest also in artificial intelligence and law (e.g., [29,30]). In this regard, in [1] we investigate explainability of our LamBERTa models focusing on (i) understanding of how they form complex relationships between the textual tokens, and (ii) providing insights into the patterns generated by LamBERTa models through a visual exploratory analysis of the learned representation embeddings.","refs":[{"start":187,"end":191,"marker":"bibr","target":"#b28"},{"start":191,"end":194,"marker":"bibr","target":"#b29"},{"start":216,"end":219,"marker":"bibr","target":"#b0"}]}]}],"tables":{},"abstract":{"title":"Abstract","paragraphs":[{"text":"We present LamBERTa, a BERT-based framework for law article retrieval as a prediction problem, focusing on civil-law codes, and specifically trained on the Italian civil code. To the best of our knowledge, LamBERTa is the first advanced, deep learning approach to law article prediction for the Italian legal system. This paper is an extended abstract from our recent research work in [1].","refs":[{"start":385,"end":388,"marker":"bibr","target":"#b0"}]}]}}