{"bibliography":{"title":"A Video Library System Using Scene Detection and Automatic Tagging","authors":[{"person_name":{"surname":"Baraldi","first_name":"Lorenzo"},"affiliations":[{"department":"Dipartimento di Ingegneria \"Enzo Ferrari\"","institution":"Università degli Studi di Modena e Reggio Emilia","laboratory":null}],"email":"lorenzo.baraldi@unimore.it"},{"person_name":{"surname":"Grana","first_name":"Costantino"},"affiliations":[{"department":"Dipartimento di Ingegneria \"Enzo Ferrari\"","institution":"Università degli Studi di Modena e Reggio Emilia","laboratory":null}],"email":"costantino.grana@unimore.it"},{"person_name":{"surname":"Cucchiara","first_name":"Rita"},"affiliations":[{"department":"Dipartimento di Ingegneria \"Enzo Ferrari\"","institution":"Università degli Studi di Modena e Reggio Emilia","laboratory":null}],"email":"rita.cucchiara@unimore.it"}],"date":null,"ids":{"DOI":"10.1007/978-3-319-68130-6","arXiv":null},"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"keywords":["Video browsing","Tagging","Interfaces","Scene detection"],"citations":{"b0":{"title":"Fast shot segmentation combining global and local visual descriptors","authors":[{"person_name":{"surname":"Apostolidis","first_name":"E"},"affiliations":[],"email":null},{"person_name":{"surname":"Mezaris","first_name":"V"},"affiliations":[],"email":null}],"date":{"year":"2014","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":6583,"to_page":6587}}},"b1":{"title":"Affective level design for a role-playing videogame evaluated by a brain-computer interface and machine learning methods","authors":[{"person_name":{"surname":"Balducci","first_name":"F"},"affiliations":[],"email":null},{"person_name":{"surname":"Grana","first_name":"C"},"affiliations":[],"email":null},{"person_name":{"surname":"Cucchiara","first_name":"R"},"affiliations":[],"email":null}],"date":{"year":"2017","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"Vis. Comput","series":null,"scope":{"volume":33,"pages":{"from_page":413,"to_page":427}}},"b2":{"title":"A data-driven approach for tag refinement and localization in web videos","authors":[{"person_name":{"surname":"Ballan","first_name":"L"},"affiliations":[],"email":null},{"person_name":{"surname":"Bertini","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Serra","first_name":"G"},"affiliations":[],"email":null},{"person_name":{"surname":"Del Bimbo","first_name":"A"},"affiliations":[],"email":null}],"date":{"year":"2015","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"Comput. Vis. Image Underst","series":null,"scope":{"volume":140,"pages":{"from_page":58,"to_page":67}}},"b3":{"title":"A deep siamese network for scene detection in broadcast videos","authors":[{"person_name":{"surname":"Baraldi","first_name":"L"},"affiliations":[],"email":null},{"person_name":{"surname":"Grana","first_name":"C"},"affiliations":[],"email":null},{"person_name":{"surname":"Cucchiara","first_name":"R"},"affiliations":[],"email":null}],"date":{"year":"2015","month":null,"day":null},"ids":{"DOI":"10.1145/2733373.2806316","arXiv":null},"target":"http://doi.acm.org/10.1145/2733373.2806316","publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":1199,"to_page":1202}}},"b4":{"title":"Scene-driven retrieval in edited videos using aesthetic and semantic deep features","authors":[{"person_name":{"surname":"Baraldi","first_name":"L"},"affiliations":[],"email":null},{"person_name":{"surname":"Grana","first_name":"C"},"affiliations":[],"email":null},{"person_name":{"surname":"Cucchiara","first_name":"R"},"affiliations":[],"email":null}],"date":{"year":"2016","month":null,"day":null},"ids":{"DOI":"10.1145/2911996.2912012","arXiv":null},"target":"http://doi.acm.org/10.1145/2911996.2912012","publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":23,"to_page":29}}},"b5":{"title":"Recognizing and presenting the storytelling video structure with deep multimodal networks","authors":[{"person_name":{"surname":"Baraldi","first_name":"L"},"affiliations":[],"email":null},{"person_name":{"surname":"Grana","first_name":"C"},"affiliations":[],"email":null},{"person_name":{"surname":"Cucchiara","first_name":"R"},"affiliations":[],"email":null}],"date":{"year":"2017","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"IEEE Trans. Multimed","series":null,"scope":{"volume":19,"pages":{"from_page":955,"to_page":968}}},"b6":{"title":"Historical handwritten text images word spotting through sliding window HOG features","authors":[{"person_name":{"surname":"Bolelli","first_name":"F"},"affiliations":[],"email":null},{"person_name":{"surname":"Borghi","first_name":"G"},"affiliations":[],"email":null},{"person_name":{"surname":"Grana","first_name":"C"},"affiliations":[],"email":null}],"date":{"year":"2017","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b7":{"title":"Scene detection in videos using shot clustering and sequence alignment","authors":[{"person_name":{"surname":"Chasanis","first_name":"V"},"affiliations":[],"email":null},{"person_name":{"surname":"Likas","first_name":"C"},"affiliations":[],"email":null},{"person_name":{"surname":"Galatsanos","first_name":"N"},"affiliations":[],"email":null}],"date":{"year":"2009","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"IEEE Trans. Multimed","series":null,"scope":{"volume":11,"pages":{"from_page":89,"to_page":100}}},"b8":{"title":"Automated high-level movie segmentation for advanced video-retrieval systems","authors":[{"person_name":{"surname":"Hanjalic","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Lagendijk","first_name":"R"},"affiliations":[],"email":null},{"person_name":{"surname":"Biemond","first_name":"J"},"affiliations":[],"email":null}],"date":{"year":"1999","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"IEEE Trans. Circ. Syst. Video Technol","series":null,"scope":{"volume":9,"pages":{"from_page":580,"to_page":588}}},"b9":{"title":"Imagenet classification with deep convolutional neural networks","authors":[{"person_name":{"surname":"Krizhevsky","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Sutskever","first_name":"I"},"affiliations":[],"email":null},{"person_name":{"surname":"Hinton","first_name":"G"},"affiliations":[],"email":null}],"date":{"year":"2012","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":1097,"to_page":1105}}},"b10":{"title":"Content-based multimedia information retrieval: state of the art and challenges","authors":[{"person_name":{"surname":"Lew","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Sebe","first_name":"N"},"affiliations":[],"email":null},{"person_name":{"surname":"Djeraba","first_name":"C"},"affiliations":[],"email":null},{"person_name":{"surname":"Jain","first_name":"R"},"affiliations":[],"email":null}],"date":{"year":"2006","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"ACM Trans. Multimed. Comput. Commun. Appl. (TOMCCAP)","series":null,"scope":{"volume":2,"pages":{"from_page":1,"to_page":19}}},"b11":{"title":"Visual semantic search: retrieving videos via complex textual queries","authors":[{"person_name":{"surname":"Lin","first_name":"D"},"affiliations":[],"email":null},{"person_name":{"surname":"Fidler","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Kong","first_name":"C"},"affiliations":[],"email":null},{"person_name":{"surname":"Urtasun","first_name":"R"},"affiliations":[],"email":null}],"date":{"year":"2014","month":"06","day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":2657,"to_page":2664}}},"b12":{"title":"Learning a contextual/multi-thread model for movie/TV scene segmentation","authors":[{"person_name":{"surname":"Liu","first_name":"C"},"affiliations":[],"email":null},{"person_name":{"surname":"Wang","first_name":"D"},"affiliations":[],"email":null},{"person_name":{"surname":"Zhu","first_name":"J"},"affiliations":[],"email":null},{"person_name":{"surname":"Zhang","first_name":"B"},"affiliations":[],"email":null}],"date":{"year":"2013","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"IEEE Trans. Multimed","series":null,"scope":{"volume":15,"pages":{"from_page":884,"to_page":897}}},"b13":{"title":"Mel frequency cepstral coefficients for music modeling","authors":[{"person_name":{"surname":"Logan","first_name":"B"},"affiliations":[],"email":null}],"date":{"year":"2000","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b14":{"title":"Distributed representations of words and phrases and their compositionality","authors":[{"person_name":{"surname":"Mikolov","first_name":"T"},"affiliations":[],"email":null},{"person_name":{"surname":"Sutskever","first_name":"I"},"affiliations":[],"email":null},{"person_name":{"surname":"Chen","first_name":"K"},"affiliations":[],"email":null},{"person_name":{"surname":"Corrado","first_name":"G"},"affiliations":[],"email":null},{"person_name":{"surname":"Dean","first_name":"J"},"affiliations":[],"email":null}],"date":{"year":"2013","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":3111,"to_page":3119}}},"b15":{"title":"Detection and representation of scenes in videos","authors":[{"person_name":{"surname":"Rasheed","first_name":"Z"},"affiliations":[],"email":null},{"person_name":{"surname":"Shah","first_name":"M"},"affiliations":[],"email":null}],"date":{"year":"2005","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"IEEE Trans. Multimed","series":null,"scope":{"volume":7,"pages":{"from_page":1097,"to_page":1105}}},"b16":{"title":"CNN features off-theshelf: an astounding baseline for recognition","authors":[{"person_name":{"surname":"Sharif Razavian","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Azizpour","first_name":"H"},"affiliations":[],"email":null},{"person_name":{"surname":"Sullivan","first_name":"J"},"affiliations":[],"email":null},{"person_name":{"surname":"Carlsson","first_name":"S"},"affiliations":[],"email":null}],"date":{"year":"2014","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":806,"to_page":813}}},"b17":{"title":"Temporal video segmentation to scenes using high-level audiovisual features","authors":[{"person_name":{"surname":"Sidiropoulos","first_name":"P"},"affiliations":[],"email":null},{"person_name":{"surname":"Mezaris","first_name":"V"},"affiliations":[],"email":null},{"person_name":{"surname":"Kompatsiaris","first_name":"I"},"affiliations":[],"email":null},{"person_name":{"surname":"Meinedo","first_name":"H"},"affiliations":[],"email":null},{"person_name":{"surname":"Bugalho","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Trancoso","first_name":"I"},"affiliations":[],"email":null}],"date":{"year":"2011","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"IEEE Trans. Circ. Syst. Video Technol","series":null,"scope":{"volume":21,"pages":{"from_page":1163,"to_page":1177}}},"b18":{"title":"Very deep convolutional networks for large-scale image recognition","authors":[{"person_name":{"surname":"Simonyan","first_name":"K"},"affiliations":[],"email":null},{"person_name":{"surname":"Zisserman","first_name":"A"},"affiliations":[],"email":null}],"date":{"year":"2014","month":null,"day":null},"ids":{"DOI":null,"arXiv":"arXiv:1409.1556"},"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b19":{"title":"Video google: a text retrieval approach to object matching in videos","authors":[{"person_name":{"surname":"Sivic","first_name":"J"},"affiliations":[],"email":null},{"person_name":{"surname":"Zisserman","first_name":"A"},"affiliations":[],"email":null}],"date":{"year":"2003","month":null,"day":null},"ids":null,"target":null,"publisher":"IEEE","journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":1470,"to_page":1477}}},"b20":{"title":"Adding semantics to detectors for video retrieval","authors":[{"person_name":{"surname":"Snoek","first_name":"C"},"affiliations":[],"email":null},{"person_name":{"surname":"Huurnink","first_name":"B"},"affiliations":[],"email":null},{"person_name":{"surname":"Hollink","first_name":"L"},"affiliations":[],"email":null},{"person_name":{"surname":"De Rijke","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Schreiber","first_name":"G"},"affiliations":[],"email":null},{"person_name":{"surname":"Worring","first_name":"M"},"affiliations":[],"email":null}],"date":{"year":"2007","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"IEEE Trans. Multimed","series":null,"scope":{"volume":9,"pages":{"from_page":975,"to_page":986}}},"b21":{"title":"Video browsing using clustering and scene transitions on compressed sequences","authors":[{"person_name":{"surname":"Yeung","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Yeo","first_name":"B"},"affiliations":[],"email":null},{"person_name":{"surname":"Wolf","first_name":"W"},"affiliations":[],"email":null},{"person_name":{"surname":"Liu","first_name":"B"},"affiliations":[],"email":null}],"date":{"year":"1995","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":399,"to_page":413}}}},"sections":[{"title":"Introduction","paragraphs":[{"text":"Video is currently the largest source of internet traffic: after having been used mainly for fun and entertainment in the last decades, it is now employed into novel contexts, like social networks, online advertisement, and education.","refs":[]},{"text":"Unfortunately, the vast majority of video content available on the web is not provided with annotations, and is therefore cumbersome to retrieve. Furthermore, video browsing platforms like Youtube, Facebook and Dailymotion treat the video as an indivisible entity, so that the user receives no help in finding the portion of the video that really interests him. The user must either watch the entire video or move from one portion of the video to another through seek operations. In the case of educational video clips, which are usually longer than the average user-generated video, this becomes even more evident, and fining a short segment on a specific topic of interest often becomes intractable.","refs":[]},{"text":"In this paper, we propose a system which tries to address this limitation by exploiting computer vision and machine learning techniques. In particular, we rely on scene detection, a pattern recognition technique which enables the decomposition of a video into semantically coherent parts. In the case of scene detection, therefore, the objective is that of automatically segmenting an input video into meaningful and story-telling sequences, using perceptual and semantic features and exploiting editing rules or clustering algorithms. It is straightforward to see that scene detection can enhance video access and browsing, as it transforms a long video into a sequence of homogeneous parts. Also, it enables a fine-grained search inside the video itself, with which short sequences could be more easily retrieved with textual queries. Finally, it is worth to mention that sequences from the original video can also be exploited to create presentations or video lectures, thus enhancing the re-use of video collections.","refs":[]},{"text":"In the case of broadcast edited videos, scenes represent one of the three levels at which the video can be decomposed. Edited videos are indeed made by sequences of shots, which in turn are frames taken by the same camera. Shots can then be grouped, according to their semantic meaning, into scenes. From this perspective, scene detection can be seen as the task of grouping temporally adjacent shots, with the objective of maximizing the semantic coherence of the resulting segments. The computer vision pipeline which will be described in the rest of this paper relies on this assumption, by creating an embedding space in which shots can be projected according to their relative semantic similarity, and exploiting a temporal clustering technique which is in charge of defining the final temporal segmentation of the video. We should also notice that using shots instead of the single frames as the basic unit of computation enables a reduction in computational complexity, and at the same time assures a quasi-optimal decomposition of the video, since shots usually have a uniform semantic content. This granularity level is also beneficial for visualization, since representative keyframes can be extracted from shots.","refs":[]},{"text":"Using a scene detection algorithm that we have recently proposed in literature [6], and thanks to the application of Speech-to-Text techniques, it has been possible to automatically annotate a set of 500 educational broadcast videos taken from the large Rai Scuola archive1 . Also, we developed a browsing and retrieval interface on top of a commercial ECMS, namely eXo Platform, from which the results of the automatic annotation can be browsed and manually refined. The interface has been developed as part of the Città Educante project, cofunded by the Italian Ministry of Education, with the aim of providing new technologies for education.","refs":[{"start":79,"end":82,"marker":"bibr","target":"#b5"},{"start":272,"end":273,"marker":null,"target":"#foot_0"}]},{"text":"The rest of this paper is organized as follows: in Sect. 2 we give an overview of relevant works related to the topic of this paper; Sect. 3 describes the main algorithmic components of the system, by giving details on the scene detection approach and on the retrieval strategy we employ, while showing several examples and screenshots from the actual user interface. Finally, Sect. 4 draws the conclusion of the work.","refs":[]}]},{"title":"Related Work","paragraphs":[{"text":"In this section, we give a brief overview of the works related with the two main features of our system, namely video decomposition and video retrieval.","refs":[]},{"text":"Video Decomposition. Since more than 15 years, automatic video decomposition has been categorized into three categories [9]: rule-based methods, that consider the way a video is structured in professional movie production, graph-based methods, where shots are arranged in a graph representation, and clusteringbased methods.","refs":[{"start":120,"end":123,"marker":"bibr","target":"#b8"}]},{"text":"Rule-based approaches consider the way a scene is structured in professional movie production. Of course, the drawback of this kind of methods is that they tend to fail in videos where film-editing rules are not followed, or when two adjacent scenes are similar and follow the same rules. Liu et al. [13], for example, propose a visual based probabilistic framework that imitates the authoring process. In [8], shots are represented by means of key-frames, clustered using spectral clustering and low level color features, and then labeled according to the clusters they belong to. Scene boundaries are then detected from the alignment score of the symbolic sequences, using the Needleman-Wunsch algorithm.","refs":[{"start":300,"end":304,"marker":"bibr","target":"#b12"},{"start":406,"end":409,"marker":"bibr","target":"#b7"}]},{"text":"In graph-based methods, instead, shots are arranged in a graph representation and then clustered by partitioning the graph. The Shot Transition Graph (STG) [22] is one of the most used models in this category: here each node represents a shot and the edges between the shots are weighted by shot similarity. In [16], color and motion features are used to represent shot similarity, and the STG is then split into subgraphs by applying the normalized cuts for graph partitioning. Sidiropoulos et al. [18] introduced a STG approximation that exploits features from the visual and the auditory channel.","refs":[{"start":156,"end":160,"marker":"bibr","target":"#b21"},{"start":311,"end":315,"marker":"bibr","target":"#b15"},{"start":499,"end":503,"marker":"bibr","target":"#b17"}]},{"text":"Clustering-based solutions assume that similarity of shots can be used to group them into meaningful clusters, thus directly providing the final story boundaries. With this approach, a deep learning based strategy has recently been proposed [4]. In this model, a Siamese Network is used together with features extracted from a Convolutional Neural Network and time features to learn distances between shots. Spectral clustering is then applied to detect coherent sequences.","refs":[{"start":241,"end":244,"marker":"bibr","target":"#b3"}]},{"text":"Video Retrieval. Lot of work has also been proposed for video retrieval: with the explosive growth of online videos, this has become a hot topic in computer vision. In their seminal work, Sivic et al. proposed Video Google [20], a system that retrieves videos from a database via bag-of-words matching. Lew et al. [11] reviewed earlier efforts in video retrieval, which mostly relied on feature-based relevance feedback or similar methods.","refs":[{"start":223,"end":227,"marker":"bibr","target":"#b19"},{"start":314,"end":318,"marker":"bibr","target":"#b10"}]},{"text":"More recently, concept-based methods have emerged as a popular approach to video retrieval. Snoek et al. [21] proposed a method based on a set of concept detectors, with the aim to bridge the semantic gap between visual features and high level concepts. In [3], authors proposed a video retrieval approach based on tag propagation: given an input video with user-defined tags, Flickr, Google Images and Bing are mined to collect images with similar tags: these are used to label each temporal segment of the video, so that the method increases the number of tags originally proposed by the users, and localizes them temporally. In [12] the problem of retrieving videos using complex natural language queries is tackled, by first parsing the sentential descriptions into a semantic graph, which is then matched to visual concepts using a generalized bipartite matching algorithm. This also allows to retrieve the relevant video segment given a text query. Our method, in contrast to [3], does not need any kind of initial manual annotation, and, thanks to the availability of the video structure, is able to return specific stories related to the user query. This provides the retrieved result with a context that allows to better understand the video content.","refs":[{"start":105,"end":109,"marker":"bibr","target":"#b20"},{"start":257,"end":260,"marker":"bibr","target":"#b2"},{"start":631,"end":635,"marker":"bibr","target":"#b11"},{"start":982,"end":985,"marker":"bibr","target":"#b2"}]}]},{"title":"The System","paragraphs":[{"text":"The proposed system is comprises three main components: a scene detection algorithm, which is in charge of performing a temporal segmentation of the input video into coherent parts, an automatic tagging algorithm, and a retrieval module, with which users can search for scenes inside a video collection.","refs":[]}]},{"title":"Scene Detection","paragraphs":[{"text":"The decomposition of a video into semantically coherent parts is an intrinsic multi-modal task, which cannot be solved by applying heuristic rules, or a-priori defined models due to the variety of boundaries which can be found in professionally edited video. The definition of a hand-crafted rules would indeed be very time consuming, and would probably lead to poor results in terms of localization accuracy. We therefore choose to rely on machine learning, and to build a deep learning architecture for temporal video segmentation which can learn the optimal way of segmenting the video by learning from examples annotated by different users. On a different note, to tackle the multi-modal nature of the problem, we employ a combination of multi-modal features which range from the frames and the audio track of the video, to the transcript of the speaker.","refs":[]},{"text":"The video is firstly decomposed into a set of chunks taken by the same camera (i.e. shots), using an open source shot detector [1]. Given that the content of a shot is usually uniform from a semantic point of view, we can constrain scene boundaries to be a subset of shot boundaries, therefore reducing the problem of scene detection to that of clustering adjacent shots. This preliminary decomposition also reduces the computational efforts needed to process the entire video, given that few key-frames can be used as the representative of the whole shot. Similarly, features coming from other modalities can be encoded at the shot level by following the same homogeneity assumption. For each shot of the video, we extract different features, in order to take into account all the modalities present in the video.","refs":[{"start":127,"end":130,"marker":"bibr","target":"#b0"}]},{"text":"Visual Appearance Features. We encode the visual appearance of a shot, and information about the timestamp and the length of a given shot. Visual appearance is extracted with a pre-trained Convolutional Neural Network which is shortened by cutting out the last fully connected layers. This extracts high level features from the input image, which can be a rich source of information to identify changes in visual content between one portion of the video and another. Given that a single key-frame might be too poor to describe a shot, we uniformly sample three key-frame from the input shot, and take the pixelwise maximum of the network responses.","refs":[]}]},{"title":"Visual Concept Features.","paragraphs":[{"text":"Using a part-of-speech tagger, we parse the transcript obtained with standard speech-to-text techniques, and retain unigrams which are annotated as noun, proper noun and foreign word. Those are then mapped to the Imagenet corpus by means of a skip-gram model [15] trained on the dump of the Italian Wikipedia. By means of this mapping we build a classifier to detect the presence of a visual concept in a shot. Images from the external corpus are represented using feature activations from pre-trained deep convolutional neural networks (CNN), which can extract rich semantic information from an input image [10]. In our case, we employ the VGG-16 model [19], which is well known for providing state-of-the-art results on image classifications, and for its good generalization properties [17]. Then, a linear probabilistic SVM is trained for each concept, using randomly sampled negative training data; the probability output of each classifier is then used as an indicator of the presence of a concept in a shot.","refs":[{"start":259,"end":263,"marker":"bibr","target":"#b14"},{"start":608,"end":612,"marker":"bibr","target":"#b9"},{"start":654,"end":658,"marker":"bibr","target":"#b18"},{"start":788,"end":792,"marker":"bibr","target":"#b16"}]},{"text":"Keeping a shot-based representation, we build a feature vector which encodes the influence of each concept group on the considered shot. Formally, the visual concept feature of shot s, v(s), is a K-dimensional vector, defined as","refs":[]},{"text":"where T is the multiset of all terms inside a video, δ t,i indicates whether term t belongs to the i-th concept group, u t and u s are the timestamps of term t and shot s. M is the mapping function to the external corpus, and f M (t) (s) is the probability given by the SVM classifier trained on concept M (t) and tested on shot s.","refs":[]}]},{"title":"Textual Concept Features.","paragraphs":[{"text":"Textual concepts are as important as visual concepts to detect story changes, and detected concept groups provide an ideal mean to describe topic changes in text. Therefore, a textual concept feature vector, t(s), is built as the textual counterpart of v(s)","refs":[]},{"text":"We thus get a representation of how much each concept group is present in a shot and in its neighborhood.","refs":[]},{"text":"Audio Features. Audio is another meaningful cue for detecting scene boundaries, since audio effects and soundtracks are often used to underline the development of a scene or a change in content. We extract MFCCs descriptors [14] over a 10 ms window. The MFCC descriptors are aggregated by Fisher vectors using a Gaussian Mixture Model with 256 components.","refs":[{"start":224,"end":228,"marker":"bibr","target":"#b13"}]},{"text":"Multi-modal Fusion. The overall feature vector for a shot is the concatenation of all the previously defined features. A Triplet Deep Network is then trained on ground-truth decompositions by minimizing a contrastive loss function: at each training iteration, the network processes a triplet of examples, namely an anchor example, a positive and a negative example. The anchor example is randomly selected from the available shots in the database, the positive one is constrained to be part of the same scene of the anchor, and the negative sample is selected from a different scene. Each of the sample is embedded by the same function into a common, multimodal, embedding space. The contrastive loss function then forces the distance between the anchor and the positive shot to be smaller than the distance between the anchor and the negative. This, during training, promotes the creation of an embedding function suitable for the task.","refs":[]},{"text":"At test time, the network has learned to distinguish similar and dissimilar shots, and can be therefore employed to perform scene detection. In particular, our clustering algorithm relies on the minimization of variances inside each scene. For further details, the reader is encouraged to read the paper in which the technique was proposed [6].","refs":[{"start":340,"end":343,"marker":"bibr","target":"#b5"}]}]},{"title":"User Interface","paragraphs":[{"text":"While the temporal segmentation step is carried out off-line, and its results are saved into a database for browsing, we also build an appropriate user interface for visualization. In particular, we extend a popular Enterprise Content Management System (ECMS), namely eXo Platform, which is largely used to build enterprise intranets and dynamic portals. We exploit the extension capabilities of eXo Platform and develop an add-on which can visualize videos decomposed into scenes. Every time a video is uploaded on the platform, a remote web service is called to perform the automatic decomposition of the video into scene, and to extract key-frames for visualization. Each video can then be visualized in a time line fashion, were each is scene is presented by means of the key-frames it contains.","refs":[]},{"text":"Figures 1 and2 show two sample screenshot from the proposed interface. As it can be noticed, we built two different views, one showing the list of available videos, each presented with one of its keyframes, and one for the browsing of the video itself, in which the actual temporal segmentation is shown. It can also be noticed that the visualization of each scene is enriched by a set of tags. These  are obtained by parsing the transcript of the video, and extracting nouns and proper nouns with a NLP tagger trained for the Italian language.","refs":[{"start":8,"end":9,"marker":"figure","target":"#fig_0"},{"start":13,"end":14,"marker":"figure","target":"#fig_1"}]},{"text":"Of course, even though generally precise, the automatic decomposition in scene might not always be correct, or appreciated by the final user, also considering the subjective nature of the task. Therefore, the output of the algorithm can not always be satisfactory for the user. For this reason, the interface allows the user to refine the automatic annotation, merging adjoining scenes together. Data collected from this manual annotation feature could be exploited in further works, both to extend the training set and to use it an a relevance feedback loop.","refs":[]}]},{"title":"Retrieval","paragraphs":[{"text":"The ability to index parts of a video is an essential feature of a video browsing platform, as it enables a fine-grained search which is also important for video re-use. In developing this feature, we wanted video clips to be indexed at the  scene level, so that users can search inside video clips and not only among video clips. Secondly, we constrained the indexing system to be fully automatic, and therefore chose to rely on the transcription solely, rather than exploiting usergenerated annotations. This allows our retrieval strategy to be enough precise, and content oriented, as most of the extracted keywords will focus on the topics addressed during the video, rather than on what is actually shown in the video.","refs":[]},{"text":"From an implementation point of view, we extended the built-in search capabilities of eXo, by developing a component which can search inside the video collection, given a textual query. This is done by building a Search Connector component, which is called by eXo itself every time a user performs a textual search. The Connector, in turn, searches for the given query inside the video database, and matches the query with the available tags. Of course, more sophisticated techniques could be used, even though they are outside the scope of this paper.","refs":[]},{"text":"For each retrieved scene, a thumbnail is also selected among the key-frames of the video by using a semantic and aesthetic criterion [5], so that the user can be confident about the result of his research by simply looking at the provided  thumbnails. Moreover, since results are presented in terms of scenes, the user is redirected directly to the video portion of interest for his query, without the need to search within the video of interest. Figures 3 and4 show, respectively, the search interface and the results page.","refs":[{"start":133,"end":136,"marker":"bibr","target":"#b4"},{"start":455,"end":456,"marker":"figure","target":"#fig_2"},{"start":460,"end":461,"marker":"figure","target":"#fig_3"}]}]},{"title":"Integration with Third-Party Add-Ons","paragraphs":[{"text":"One key feature of the system is the possibility to integrate our temporal segmentation, retrieval and visualization tools into third part components. This makes the developed system integrable with other software, which is a desirable property in most of the use cases, ranging from portals to project demonstrators. Indeed, all the extensions which have been presented in the previous sections are designed to be naturally integrated with the underlying data structure of eXo, namely the Java JCR, and with third-party applications. In particular, full-length videos or portions of them can be retrieved from the database by any application inside the platform. Also, at an higher level, videos and scene managed by our application can be copied and pasted into the eXo Clipboard. This is a cross-application clipboard, which can be read an written across different applications and add-ons on the platform, and which provides an effective way to transfer content from one application to another.","refs":[]},{"text":"Finally, for the purpose of demonstration, we developed a simple portlet which can showcase the benefit of using data provided by our algorithm outside the specific user interface we built for visualization. The demo portlet allows the insertion of video portions within a simple canvas, by exploiting the eXo Clipboard. A dialog shows the contents of the clipboard with the selected scenes by the user while navigating in the database: these can then be pasted on the canvas and displayed (Figs. 5 and6).","refs":[{"start":497,"end":498,"marker":"figure","target":"#fig_4"},{"start":502,"end":503,"marker":"figure","target":"#fig_5"}]},{"text":"Beside this simple example, the same extension capabilities showcased by our application can be used for integrating with other add-ons. In particular, this will also be beneficial in the context of the Città Educante project, in which the eXo Platform will be used as the enabling tool of the final demonstrator [2,7].","refs":[{"start":313,"end":316,"marker":"bibr","target":"#b1"},{"start":316,"end":318,"marker":"bibr","target":"#b6"}]}]},{"title":"Conclusion","paragraphs":[{"text":"This paper has presented a video browsing and retrieval system for edited videos, which has been developed in the context of a national project, Città Educante. The main distinguishing feature of the system, with respect to other video browsing approaches, is that videos are automatically parsed and decomposed into meaningful segments (called scenes), by means of a novel scene detection algorithm which exploits state of the art multi-modal descriptors and machine learning techniques. In particular, it relies on a Triplet Deep Network which learns a multi-modal semantic embedding space in which shots from the input video can be projected, and on a temporal clustering algorithm which provides the final segmentation into scenes. The web-based interface enables the interactive visualization of a video, its automatic and semi-automatic annotation, as well as a keyword-based search inside a video collection. Finally, it is worth mentioning that using the proposed algorithm it has been possible to automatically annotate a set of 500 educational broadcast videos taken from the large Rai Scuola archive, which can be browsed and retrieved inside the internal portal of the Città Educante project. As a future work, it will also be possible to exploit the corrections and annotations provided by the users, as a source of additional training data, and to build a human-in-the-loop system which can possibly provide better temporal segmentation results.","refs":[]}]}],"tables":{},"abstract":{"title":"Abstract","paragraphs":[{"text":"We present a novel video browsing and retrieval system for edited videos, based on scene detection and automatic tagging. In the proposed system, database videos are automatically decomposed into meaningful and storytelling parts (i.e. scenes) and tagged in an automatic way by leveraging their transcript. We rely on computer vision and machine learning techniques to learn the optimal scene boundaries: a Triplet Deep Neural Network is trained to distinguish video sequences belonging to the same scene and sequences from different scenes, by exploiting multimodal features from images, audio and captions. The system also features a user interface build as a set of extensions to the eXo Platform Enterprise Content Management System (ECMS) (https:// www.exoplatform.com/). This set of extensions enable the interactive visualization of a video, its automatic and semi-automatic annotation, as well as a keyword-based search inside the video collection. The platform also allows a natural integration with third-party add-ons, so that automatic annotations can be exploited outside the proposed platform.","refs":[]}]}}