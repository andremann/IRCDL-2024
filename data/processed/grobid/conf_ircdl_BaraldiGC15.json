{"bibliography":{"title":"Analysis and Re-Use of Videos in Educational Digital Libraries with Automatic Scene Detection","authors":[{"person_name":{"surname":"Baraldi","first_name":"Lorenzo"},"affiliations":[{"department":"Dipartimento di Ingegneria \"Enzo Ferrari\"","institution":"Università Degli Studi di Modena e Reggio Emilia","laboratory":null}],"email":"lorenzo.baraldi@unimore.it"},{"person_name":{"surname":"Grana","first_name":"Costantino"},"affiliations":[{"department":"Dipartimento di Ingegneria \"Enzo Ferrari\"","institution":"Università Degli Studi di Modena e Reggio Emilia","laboratory":null}],"email":"costantino.grana@unimore.it"},{"person_name":{"surname":"Cucchiara","first_name":"Rita"},"affiliations":[{"department":"Dipartimento di Ingegneria \"Enzo Ferrari\"","institution":"Università Degli Studi di Modena e Reggio Emilia","laboratory":null}],"email":"rita.cucchiara@unimore.it"}],"date":null,"ids":{"DOI":"10.1007/978-3-319-41938-1","arXiv":null},"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"keywords":["Performance evaluation","Spectral clustering","Scene detection"],"citations":{"b0":{"title":"Fast shot segmentation combining global and local visual descriptors","authors":[{"person_name":{"surname":"Apostolidis","first_name":"E"},"affiliations":[],"email":null},{"person_name":{"surname":"Mezaris","first_name":"V"},"affiliations":[],"email":null}],"date":{"year":"2014","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":6583,"to_page":6587}}},"b1":{"title":"Gesture recognition in ego-centric videos using dense trajectories and hand segmentation","authors":[{"person_name":{"surname":"Baraldi","first_name":"L"},"affiliations":[],"email":null},{"person_name":{"surname":"Paci","first_name":"F"},"affiliations":[],"email":null},{"person_name":{"surname":"Serra","first_name":"G"},"affiliations":[],"email":null},{"person_name":{"surname":"Benini","first_name":"L"},"affiliations":[],"email":null},{"person_name":{"surname":"Cucchiara","first_name":"R"},"affiliations":[],"email":null}],"date":{"year":"2014","month":"06","day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b2":{"title":"Touch screen versus keyboard: a comparison of task performance of young children","authors":[{"person_name":{"surname":"Battenberg","first_name":"J"},"affiliations":[],"email":null},{"person_name":{"surname":"Merbler","first_name":"J"},"affiliations":[],"email":null}],"date":{"year":"1989","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"J. Spec. Educ. Technol","series":null,"scope":{"volume":10,"pages":{"from_page":24,"to_page":28}}},"b3":{"title":"Dynamic pictorially enriched ontologies for video digital libraries","authors":[{"person_name":{"surname":"Bertini","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Del Bimbo","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Serra","first_name":"G"},"affiliations":[],"email":null},{"person_name":{"surname":"Torniai","first_name":"C"},"affiliations":[],"email":null},{"person_name":{"surname":"Cucchiara","first_name":"R"},"affiliations":[],"email":null},{"person_name":{"surname":"Grana","first_name":"C"},"affiliations":[],"email":null},{"person_name":{"surname":"Vezzani","first_name":"R"},"affiliations":[],"email":null}],"date":{"year":"2009","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"IEEE MultiMedia Mag","series":null,"scope":{"volume":16,"pages":{"from_page":41,"to_page":51}}},"b4":{"title":"Scene detection in videos using shot clustering and sequence alignment","authors":[{"person_name":{"surname":"Chasanis","first_name":"V"},"affiliations":[],"email":null},{"person_name":{"surname":"Likas","first_name":"C"},"affiliations":[],"email":null},{"person_name":{"surname":"Galatsanos","first_name":"N"},"affiliations":[],"email":null}],"date":{"year":"2009","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"IEEE Trans. Multimedia","series":null,"scope":{"volume":11,"pages":{"from_page":89,"to_page":100}}},"b5":{"title":"Automated high-level movie segmentation for advanced video-retrieval systems","authors":[{"person_name":{"surname":"Hanjalic","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Lagendijk","first_name":"R"},"affiliations":[],"email":null},{"person_name":{"surname":"Biemond","first_name":"J"},"affiliations":[],"email":null}],"date":{"year":"1999","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"IEEE Trans. Circ. Syst. Vid. Technol","series":null,"scope":{"volume":9,"pages":{"from_page":580,"to_page":588}}},"b6":{"title":"Learning a contextual multi-thread model for movie/tv scene segmentation","authors":[{"person_name":{"surname":"Liu","first_name":"C"},"affiliations":[],"email":null},{"person_name":{"surname":"Wang","first_name":"D"},"affiliations":[],"email":null},{"person_name":{"surname":"Zhu","first_name":"J"},"affiliations":[],"email":null},{"person_name":{"surname":"Zhang","first_name":"B"},"affiliations":[],"email":null}],"date":{"year":"2013","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"IEEE Trans. Multimedia","series":null,"scope":{"volume":15,"pages":{"from_page":884,"to_page":897}}},"b7":{"title":"Beyond student-centered and teacher-centered pedagogy: teaching and learning as guided participation","authors":[{"person_name":{"surname":"Mascolo","first_name":"M"},"affiliations":[],"email":null}],"date":{"year":"2009","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"Pedagogy Hum. Sci","series":null,"scope":{"volume":1,"pages":{"from_page":3,"to_page":27}}},"b8":{"title":"Detection and representation of scenes in videos","authors":[{"person_name":{"surname":"Rasheed","first_name":"Z"},"affiliations":[],"email":null},{"person_name":{"surname":"Shah","first_name":"M"},"affiliations":[],"email":null}],"date":{"year":"2005","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"IEEE Trans. Multimedia","series":null,"scope":{"volume":7,"pages":{"from_page":1097,"to_page":1105}}},"b9":{"title":"Hand segmentation for gesture recognition ego-vision","authors":[{"person_name":{"surname":"Serra","first_name":"Camurri"},"affiliations":[],"email":null},{"person_name":{"surname":"Baraldi","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Benedetti","first_name":"L"},"affiliations":[],"email":null},{"person_name":{"surname":"Cucchiara","first_name":"M"},"affiliations":[],"email":null}],"date":{"year":"2013","month":"10","day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b10":{"title":"Temporal video segmentation to scenes using high-level audiovisual features","authors":[{"person_name":{"surname":"Sidiropoulos","first_name":"P"},"affiliations":[],"email":null},{"person_name":{"surname":"Mezaris","first_name":"V"},"affiliations":[],"email":null},{"person_name":{"surname":"Kompatsiaris","first_name":"I"},"affiliations":[],"email":null},{"person_name":{"surname":"Meinedo","first_name":"H"},"affiliations":[],"email":null},{"person_name":{"surname":"Bugalho","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Trancoso","first_name":"I"},"affiliations":[],"email":null}],"date":{"year":"2011","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"IEEE Trans. Circ. Syst. Vid. Technol","series":null,"scope":{"volume":21,"pages":{"from_page":1163,"to_page":1177}}},"b11":{"title":"Systematic evaluation of logical story unit segmentation","authors":[{"person_name":{"surname":"Vendrig","first_name":"J"},"affiliations":[],"email":null},{"person_name":{"surname":"Worring","first_name":"M"},"affiliations":[],"email":null}],"date":{"year":"2002","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"IEEE Trans. Multimedia","series":null,"scope":{"volume":4,"pages":{"from_page":492,"to_page":499}}},"b12":{"title":"Video browsing using clustering and scene transitions on compressed sequences","authors":[{"person_name":{"surname":"Yeung","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Yeo","first_name":"B"},"affiliations":[],"email":null},{"person_name":{"surname":"Wolf","first_name":"W"},"affiliations":[],"email":null},{"person_name":{"surname":"Liu","first_name":"B"},"affiliations":[],"email":null}],"date":{"year":"1995","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":399,"to_page":413}}}},"sections":[{"title":"Introduction","paragraphs":[{"text":"In recent years, the research efforts in video access and video re-use have expanded their interest boundaries beyond traditional fields like news, web entertainment, and sport broadcasting to explore new areas, given the pervasive availability of huge amounts of digital footage. One of such emerging field is surely education that is a key-topic of many international research programs, like the European programs on Smart Communities and the 2020 European Digital Agenda, and that can benefit considerably in accessing the available digital material.","refs":[]},{"text":"Indeed, many modern approaches to education try to engage the students with technological novelties, such as touch screens [3], hand and body pose recognition [2,10] or multimedia contents. In particular, Massive Open Online Courses (MOOC) already make use of video as the basic media for educating and transmitting knowledge. Moreover, recent educational projects rethink the concepts of the classical transmission model of the education, towards a sociocultural-constructivist model where the massive use of video and multimedia content becomes the principal actor in the process of construction of new knowledge centered on the student in strict collaboration between broadcasting bodies, content owners, teachers and the whole society [8]. For this aim, new instruments should be provided to each level of school for accessing and re-using media contents in different topics, allowing a personalized creation of knowledge, a sharing of multi-cultural practices and the assessment of new social experiences.","refs":[{"start":123,"end":126,"marker":"bibr","target":"#b2"},{"start":159,"end":162,"marker":"bibr","target":"#b1"},{"start":162,"end":165,"marker":"bibr","target":"#b9"},{"start":739,"end":742,"marker":"bibr","target":"#b7"}]},{"text":"In the \"Citt Educante\" research project, in which we are involved, we are developing new solutions for the re-use of educational video production. The goal is to provide efficient tools for students to access the video content, creating their personalized educational experience on specific topics (e.g. geography or art) and across-topics, to share experiences by enriching the footage with user-generated content and data coming from web and social media. In this scenario, even if a huge amount of video from national broadcasting agencies is available and pedagogy researchers are trying to leverage this new possibility in education, the IT tools are still not adequate to allow video content re-use, tagging, annotation and personalization [4].","refs":[{"start":746,"end":749,"marker":"bibr","target":"#b3"}]},{"text":"Nowadays, people can access video through web or specific apps, but it is difficult to find which section is really the one they want (e.g. a two minute scene withing a two hour program). Even if we know what is the part of interest, extracting and integrating it in our own presentation and re-using it in a suitable manner is still challenging. One basic necessary tool should allow an \"access by scene\" that improves the level of abstraction from single frame or shot to the scene, i.e. a conceptually meaningful and homogeneous element, composed by more than one shot. Unfortunately, most of the reusable content, owned by broadcast agencies, has not pre-defined sub-units and is not annotated. Therefore, we need accurate scene detection to identify coherent sequences (i.e. scenes) in videos, without asking manual segmentation to editors or publishers. The problem has been approached in the past in the literature with some promising, but not conclusive, results.","refs":[]},{"text":"We present a novel proposal for scene segmentation, based on spectral clustering, which shows competitive results when compared to state-of-the-art methods. As well, also the broad concept of accuracy should be better defined for scene detection, especially when the goal is not only an algorithm comparison but a concrete result, which should be useful in many applications where a successive human interaction is expected, e.g. for browsing, tagging, selecting etc. In this case, for instance, the precise position of the cut is not important while skipping a scene and integrating it in another longer, preventing people (in our case students) to find a useful part of the video without seeing all the material, is more important. Thus we compare classical precision/recall measures with a better suited definition of coverage/overflow, which solves frequently observed cases in which the numeric interpretation would be quite different from the expected results by users.","refs":[]},{"text":"The rest of this paper is organized as follows: Sect. 2 presents a summary of the existing approaches to scene detection and temporal clutering. In Sect. 3 we describe our algorithm; in Sect. 4 we discuss performance evaluation and in Sect. 5 experimentally evaluate them and show a sample use case.","refs":[]}]},{"title":"Related Work","paragraphs":[{"text":"Video decomposition techniques aim to partition a video into sequences, like shots or scenes. Shots are elementary structural segments that are defined as sequences of images taken without interruption by a single camera. Scenes, on the contrary, are often defined as series of temporally contiguous shots characterized by overlapping links that connect shots with similar content [6]. Therefore, the fundamental goal of scene detection algorithms is to identify semantically coherent shots that are temporally close to each other. Most of the existing works can be roughly categorized into three categories: rule-based methods, that consider the way a scene is structured in professional movie production, graph-based methods, where shots are arranged in a graph representation, and clusteringbased methods. They can rely on visual, audio, and textual features.","refs":[{"start":381,"end":384,"marker":"bibr","target":"#b5"}]},{"text":"Rule-based approaches consider the way a scene is structured in professional movie production. Of course, the drawback of this kind of methods is that they tend to fail in videos where film-editing rules are not followed, or when two adjacent scenes are similar and follow the same rules. Liu et al. [7], for example, propose a visual based probabilistic framework that imitates the authoring process and detects scenes by incorporating contextual dynamics and learning a scene model. In [5], shots are represented by means of key-frames, thus, the first step of this method is to extract several key-frames from each shot: frames from a shot are clustered using the spectral clustering algorithm, color histograms as features, and the euclidean distance to compute the similarity matrix. The number of clusters is selected by applying a threshold T h on the eigenvalues of the Normalized Laplacian. The distance between a pair of shots is defined as the maximum similarity between key-frames belonging to the two shots, computed using histogram intersection. Shots are clustered using again spectral clustering and the aforesaid distance measure, and then labeled according to the clusters they belong to. Scene boundaries are then detected from the alignment score of the symbolic sequences.","refs":[{"start":300,"end":303,"marker":"bibr","target":"#b6"},{"start":488,"end":491,"marker":"bibr","target":"#b4"}]},{"text":"In graph-based methods, instead, shots are arranged in a graph representation and then clustered by partitioning the graph. The Shot Transition Graph (STG), proposed in [13], is one of the most used models in this category: here each node represents a shot and the edges between the shots are weighted by shot similarity. In [9], color and motion features are used to represent shot similarity, and the STG is then split into subgraphs by applying the normalized cuts for graph partitioning. More recently, Sidiropoulos et al. [11] introduced a new STG approximation that exploits features automatically extracted from the visual and the auditory channel. This method extends the Shot Transition Graph using multimodal low-level and high-level features. To this aim, multiple STGs are constructed, one for each kind of feature, and then a probabilistic merging process is used to combine their results. The used features include visual features, such as HSV histograms, outputs of visual concept detectors trained using the Bag of Words approach, and audio features, like background conditions classification results, speaker histogram, and model vectors constructed from the responses of a number of audio event detectors.","refs":[{"start":169,"end":173,"marker":"bibr","target":"#b12"},{"start":325,"end":328,"marker":"bibr","target":"#b8"},{"start":527,"end":531,"marker":"bibr","target":"#b10"}]},{"text":"We propose a simpler solution based on the spectral clustering approach, where we modify the standard spectral clustering algorithm in order to produce temporally consistent clusters.","refs":[]}]},{"title":"A Spectral Clustering Approach","paragraphs":[{"text":"Our scene detection method generates scenes by grouping adjacent shots. Shots are described by means of color histograms, hence relying on visual features only: given a video, we compute a three-dimensional histogram of each frame, by quantizing each RGB channel in eight bins, for a total of 512 bins. Then, we sum histograms from frames belonging to the same shot, thus obtaining a single L 1 -normalized histogram for each shot.","refs":[]},{"text":"In contrast to other approaches that used spectral clustering for scene detection, we build a similarity matrix that jointly describes appearance similarity and temporal proximity. Its generic element κ ij , defines the similarity between shots x i and x j as","refs":[]},{"text":"where ψ(x i ) is the normalized histogram of shot x i , d 2 1 is the Bhattacharyya distance and d 2 2 (x i , x j ) is the normalized temporal distance between shot x i and shot x j , while the parameter α tunes the relative importance of color similarity and temporal distance. To describe temporal distance between frames, d 2 2 (x i , x j ) is defined as","refs":[]},{"text":"where m i is the index of the central frame of shot x i , and l is the total number of frames in the video. The spectral clustering algorithm is then applied to the similarity matrix, using the Normalized Laplacian and the maximum eigen-gap criterion to select k:","refs":[]},{"text":"where λ i is the i-th eigenvalue of the Normalized Laplacian.","refs":[]},{"text":"As shown in Fig. 1, the effect of applying increasing values of α to the similarity matrix is to raise the similarities of adjacent shots, therefore boosting the temporal consistency of the resulting groups. Of course, this does not guarantee a completely temporal consistent clustering (i.e. some clusters may still contain non-adjacent shots); at the same time, too high values of α would lead to a segmentation that ignores color dissimilarity. The final scene boundaries are created between adjacent shots that do not belong to the same cluster.","refs":[{"start":17,"end":18,"marker":"figure","target":null}]}]},{"title":"Evaluating Scene Segmentation","paragraphs":[{"text":"The first possibility to evaluate the results of a scene detection algorithm is to count correctly and wrongly detected boundaries, without considering the temporal distance between a ground truth cut and the nearest detected cut.","refs":[]},{"text":"The most used measures in this context are precision and recall, together with the F-Score measure, that summarizes both. Precision is the ratio of the number of correctly identified scenes boundaries to the total number of scenes detected by the algorithm. Recall is the ratio of the number of correctly identified boundaries to the total number of scenes in the ground truth.","refs":[]},{"text":"Of course this kind of evaluation does not discern the seriousness of an error: if a boundary is detected one shot before or after its ground truth position, an error is counted in recall as if the boundary was not detected at all, and in precision as if the boundary was put far away. This issue appears to be felt also by other authors, with the result that sometimes a tolerance factor is used. For example, [9] uses a best match method with a sliding window of 30 s, so that a detected boundary is considered correct if it matches a ground truth boundary in the sliding window.","refs":[{"start":411,"end":414,"marker":"bibr","target":"#b8"}]},{"text":"To deal with these problems, Vendrig et al. [12] proposed the Coverage and Overflow measures. Coverage C measures the quantity of shots belonging to the same scene correctly grouped together, while Overflow O evaluates to what extent shots not belonging to the same scene are erroneously grouped together. Formally, given the set of automatically detected scenes s = [s 1 , s 2 , ..., s m ], and the ground truth s = [s 1 , s2 , ..., sn ], where each element of s and s is a set of shot indexes, the coverage C t of scene st is proportional to the longest overlap between s i and st :","refs":[{"start":44,"end":48,"marker":"bibr","target":"#b11"}]},{"text":"where #(s i ) is the number of shots in scene s i . The overflow of a scene st , O t , is the amount of overlap of every s i corresponding to st with the two surrounding scenes st-1 and st+1 :","refs":[]},{"text":"The computed per-scene measures can then be aggregated into values for an entire video as follows:","refs":[]},{"text":"Finally, an F-Score measure can be defined to combine Coverage and Overflow in a single measure, by taking the harmonic mean of C and 1 -O.","refs":[]}]},{"title":"Evaluation","paragraphs":[{"text":"We evaluate the aforesaid measures and algorithms on a collection of ten challenging broadcasting videos from the Rai Scuola video archive1 , mainly documentaries and talk shows. Shots have been obtained running the state of the art shot detector of [1] and manually grouped into scenes by a set of human experts to define the ground truth. Our dataset and the corresponding annotations are available for download at http://imagelab.ing.unimore.it/ files/RaiSceneDetection.zip.","refs":[{"start":138,"end":139,"marker":null,"target":"#foot_0"},{"start":250,"end":253,"marker":"bibr","target":"#b0"}]},{"text":"We reimplemented the approach in [5] and used the executable of [11] provided by the authors2 . The threshold T h of [5] was selected to maximize the performance on our dataset, and α was set to 0.05 in all our experiments.","refs":[{"start":33,"end":36,"marker":"bibr","target":"#b4"},{"start":64,"end":68,"marker":"bibr","target":"#b10"},{"start":92,"end":93,"marker":null,"target":"#foot_1"},{"start":117,"end":120,"marker":"bibr","target":"#b4"}]},{"text":"Figure 3 shows the results of the compared methods on a frame sequence from our dataset.   Tables 1 and2 compare the two different approaches using Boundary level and Shot level performance measures. As show in Table 1, detected boundaries rarely correspond to ground truth boundaries exactly, therefore leading to poor results in terms of precision and recall, even when considering a recent and state-of-the-art approach like [11].","refs":[{"start":7,"end":8,"marker":"figure","target":"#fig_1"},{"start":98,"end":99,"marker":"table","target":"#tab_0"},{"start":103,"end":104,"marker":"table","target":"#tab_1"},{"start":217,"end":218,"marker":"table","target":"#tab_0"},{"start":428,"end":432,"marker":"bibr","target":"#b10"}]},{"text":"As expected, the two measures behave differently and there is not a complete agreement among them: [5] performs worse than the other two methods according to both measures, while [11] performs equal to the spectral clustering approach with boundary level measures, but slightly worse than the spectral clustering approach according to shot level measures. Detected scenes, finally, can used as an input for video browsing or re-using software. As an example, we built a web-based browsing interface for broadcasting videos (see Fig. 4) where users can visualize a summary of the content by means of the extracted scenes. Scenes are represented with key-frames in a timeline fashion, and when a particular scene is selected, all its shots are unfolded. To ease the browsing even more, most frequent words, obtained from the transcript of the audio, are reported under each scene. Users can jump from one part of the video to another by clicking on the corresponding scene or shot.","refs":[{"start":99,"end":102,"marker":"bibr","target":"#b4"},{"start":179,"end":183,"marker":"bibr","target":"#b10"},{"start":533,"end":534,"marker":"figure","target":"#fig_2"}]}]},{"title":"Conclusions","paragraphs":[{"text":"We investigated the problem of evaluating scene detection algorithms with tests conducted on two different performance measures and on three different and recent approaches to scene segmentation. Results show that the problem of scene detection is still far from being solved, and that simple approaches like the suggested spectral clustering technique can sometimes achieve equivalent or better results than more complex methods.","refs":[]}]}],"tables":{"tab_0":{"heading":"Table 1 .","description":"Performance comparison on the RAI dataset using the boundary level measures (Precision, Recall, F-Score)","rows":[["Video","Spectral Clustering","","Chasanis et al. [5]","","Sidiropoulos et al. [11]"],["","F-Score Precision Recall F-Score Precision Recall F-Score Precision Recall"],["V 1","0.12","0.09","0.17","0.25","0.20","0.33","0.29","0.25","0.33"],["V 2","0.36","0.27","0.55","0.00","0.00","0.00","0.30","0.33","0.27"],["V 3","0.37","0.29","0.53","0.13","0.13","0.13","0.31","0.36","0.27"],["V 4","0.30","0.23","0.43","0.10","0.10","0.10","0.22","0.50","0.14"],["V 5","0.44","0.31","0.75","0.00","0.00","0.00","0.36","0.31","0.42"],["V 6","0.18","0.10","0.75","0.00","0.00","0.00","0.36","0.29","0.50"],["V 7","0.18","0.33","0.13","0.00","0.00","0.00","0.13","0.13","0.13"],["V 8","0.10","0.06","0.27","0.13","0.10","0.18","0.21","0.25","0.18"],["V 9","0.25","0.16","0.62","0.00","0.00","0.00","0.21","0.33","0.15"],["V 10","0.23","0.15","0.60","0.26","0.38","0.20","0.19","0.33","0.13"],["Average 0.25","0.20","0.48","0.09","0.09","0.09","0.26","0.31","0.25"]]},"tab_1":{"heading":"Table 2 .","description":"Performance comparison on the RAI dataset using Shot level measures (Coverage, Overflow and F-Score)","rows":[["Video","Spectral Clustering Chasanis et al. [5] Sidiropoulos et al. [11]"],["","F-Score C","O","F-Score C","O","F-Score C","O"],["V1","0.64","0.81 0.48 0.70","0.64 0.24 0.72","0.84 0.37"],["V2","0.68","0.61 0.22 0.36","0.80 0.77 0.59","0.85 0.55"],["V3","0.65","0.68 0.38 0.58","0.73 0.52 0.58","0.90 0.57"],["V4","0.74","0.69 0.22 0.50","0.65 0.60 0.33","0.94 0.80"],["V5","0.77","0.68 0.11 0.25","0.93 0.86 0.66","0.76 0.41"],["V6","0.51","0.37 0.17 0.18","0.89 0.90 0.71","0.77 0.34"],["V7","0.30","0.97 0.82 0.37","0.70 0.75 0.51","0.78 0.62"],["V8","0.59","0.53 0.33 0.62","0.57 0.32 0.45","0.88 0.70"],["V9","0.67","0.55 0.15 0.27","0.87 0.84 0.43","0.92 0.72"],["V10","0.57","0.42 0.12 0.54","0.91 0.62 0.44","0.94 0.71"],["Average 0.61","0.63 0.30 0.44","0.77 0.64 0.54","0.86 0.58"]]}},"abstract":{"title":"Abstract","paragraphs":[{"text":"The advent of modern approaches to education, like Massive Open Online Courses (MOOC), made video the basic media for educating and transmitting knowledge. However, IT tools are still not adequate to allow video content re-use, tagging, annotation and personalization. In this paper we analyze the problem of identifying coherent sequences, called scenes, in order to provide the users with a more manageable editing unit. A simple spectral clustering technique is proposed and compared with state-of-the-art results. We also discuss correct ways to evaluate the performance of automatic scene detection algorithms.","refs":[]}]}}