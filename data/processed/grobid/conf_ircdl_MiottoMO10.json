{"bibliography":{"title":"Content-Based Cover Song Identification in Music Digital Libraries","authors":[{"person_name":{"surname":"Miotto","first_name":"Riccardo"},"affiliations":[{"department":"Department of Information Engineering","institution":"University of Padova Padova","laboratory":null}],"email":"riccardo.miotto@dei.unipd.it"},{"person_name":{"surname":"Montecchio","first_name":"Nicola"},"affiliations":[{"department":"Department of Information Engineering","institution":"University of Padova Padova","laboratory":null}],"email":"nicola.montecchio@dei.unipd.it"},{"person_name":{"surname":"Orio","first_name":"Nicola"},"affiliations":[{"department":"Department of Information Engineering","institution":"University of Padova Padova","laboratory":null}],"email":"nicola.orio@dei.unipd.it"}],"date":null,"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"keywords":[],"citations":{"b0":{"title":"A review of audio fingerprinting","authors":[{"person_name":{"surname":"Cano","first_name":"P"},"affiliations":[],"email":null},{"person_name":{"surname":"Batlle","first_name":"E"},"affiliations":[],"email":null},{"person_name":{"surname":"Kalker","first_name":"T"},"affiliations":[],"email":null},{"person_name":{"surname":"Haitsma","first_name":"J"},"affiliations":[],"email":null}],"date":{"year":"2005","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"Journal of VLSI Signal Processing","series":null,"scope":{"volume":41,"pages":{"from_page":271,"to_page":284}}},"b1":{"title":"Content-based music audio recommendation","authors":[{"person_name":{"surname":"Cano","first_name":"P"},"affiliations":[],"email":null},{"person_name":{"surname":"Koppenberger","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Wack","first_name":"N"},"affiliations":[],"email":null}],"date":{"year":"2005","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":211,"to_page":212}}},"b2":{"title":"An industrial-strength audio search algorithm","authors":[{"person_name":{"surname":"Wang","first_name":"A"},"affiliations":[],"email":null}],"date":{"year":"2003","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b3":{"title":"Music information retrieval","authors":[{"person_name":{"surname":"Downie","first_name":"J"},"affiliations":[],"email":null}],"date":{"year":"2003","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"Annual Review of Information Science and Technology","series":null,"scope":{"volume":37,"pages":{"from_page":295,"to_page":340}}},"b4":{"title":"Efficient index-based audio matching","authors":[{"person_name":{"surname":"Kurth","first_name":"F"},"affiliations":[],"email":null},{"person_name":{"surname":"Muller","first_name":"M"},"affiliations":[],"email":null}],"date":{"year":"2008","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"IEEE Transactions on Audio, Speech, and Language Processing","series":null,"scope":{"volume":16,"pages":{"from_page":382,"to_page":395}}},"b5":{"title":"Chroma binary similarity and local alignment applied to cover song identification","authors":[{"person_name":{"surname":"Serra","first_name":"J"},"affiliations":[],"email":null},{"person_name":{"surname":"Gomez","first_name":"E"},"affiliations":[],"email":null},{"person_name":{"surname":"Herrera","first_name":"P"},"affiliations":[],"email":null},{"person_name":{"surname":"Serra","first_name":"X"},"affiliations":[],"email":null}],"date":{"year":"2008","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"IEEE Transactions on Audio, Speech, and Language Processing","series":null,"scope":{"volume":16,"pages":{"from_page":1138,"to_page":1151}}},"b6":{"title":"A music identification system based on chroma indexing and statistical modeling","authors":[{"person_name":{"surname":"Miotto","first_name":"R"},"affiliations":[],"email":null},{"person_name":{"surname":"Orio","first_name":"N"},"affiliations":[],"email":null}],"date":{"year":"2008","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":301,"to_page":306}}},"b7":{"title":"Similarity search in high dimensions via hashing","authors":[{"person_name":{"surname":"Gionis","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Indyk","first_name":"P"},"affiliations":[],"email":null},{"person_name":{"surname":"Motwani","first_name":"R"},"affiliations":[],"email":null}],"date":{"year":"1999","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"The VLDB Journal","series":null,"scope":{"volume":null,"pages":{"from_page":518,"to_page":529}}},"b8":{"title":"Locality-sensitive hashing for finding nearest neighbors [lecture notes","authors":[{"person_name":{"surname":"Slaney","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Casey","first_name":"M"},"affiliations":[],"email":null}],"date":{"year":"2008","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"IEEE Signal Processing Magazine","series":null,"scope":{"volume":25,"pages":{"from_page":128,"to_page":131}}},"b9":{"title":"Integration of chroma and rhythm histogram features in a music identification system","authors":[{"person_name":{"surname":"Miotto","first_name":"R"},"affiliations":[],"email":null},{"person_name":{"surname":"Montecchio","first_name":"N"},"affiliations":[],"email":null}],"date":{"year":"2009","month":null,"day":null},"ids":null,"target":null,"publisher":"WEMIS","journal":null,"series":null,"scope":null},"b10":{"title":"Evaluation of feature extractors and psycho-acoustic transformations for music genre classification","authors":[{"person_name":{"surname":"Lidy","first_name":"T"},"affiliations":[],"email":null},{"person_name":{"surname":"Rauber","first_name":"A"},"affiliations":[],"email":null}],"date":{"year":"2005","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":34,"to_page":41}}},"b11":{"title":"Identifying 'cover songs' with chroma features and dynamic programming beat tracking","authors":[{"person_name":{"surname":"Ellis","first_name":"D"},"affiliations":[],"email":null},{"person_name":{"surname":"Poliner","first_name":"G"},"affiliations":[],"email":null}],"date":{"year":"2007","month":null,"day":null},"ids":null,"target":null,"publisher":"ICASSP","journal":null,"series":null,"scope":{"volume":4,"pages":{"from_page":1429,"to_page":1432}}},"b12":{"title":"Combination of multiple searches","authors":[{"person_name":{"surname":"Fox","first_name":"E"},"affiliations":[],"email":null},{"person_name":{"surname":"Shaw","first_name":"J"},"affiliations":[],"email":null}],"date":{"year":"1994","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":243,"to_page":249}}}},"sections":[{"title":"Introduction","paragraphs":[{"text":"As digital libraries continue to gain an ever more pervasive role in the everyday experience of users, so do the information needs of users grow in complexity up to the point that classic text and metadata-based information retrieval techniques are not suitable to be applied to multimedia content such as music. Contentbased music identification has become an important research topic because it can provide tools to efficiently retrieve and organize music documents according to some measure of similarity.","refs":[]},{"text":"A prominent social phenomenon of the last years regarded an increasing number of users joining social communities to upload their personal recordings and performances. The large availability of such non-commercial recordings puts a major interest towards the cover identification problem: generally, the term cover song defines a new rendition of a previously recorded song in genres such as rock and pop; cover songs can be either live or studio recordings with a potentially completely different arrangement.","refs":[]},{"text":"A typical approach to music identification is audio fingerprinting [1], that consists in a content-based signature of a music recording to describe digital music even in presence of noise, distortion, and compression [2]; several commercial systems have appeared that make use of such techniques to identify an audio recording, e.g. [3], however audio fingerprinting is most useful for retrieving the exact recording given as query, while it is not explicitly designed for a cover identification task.","refs":[{"start":67,"end":70,"marker":"bibr","target":"#b0"},{"start":217,"end":220,"marker":"bibr","target":"#b1"},{"start":333,"end":336,"marker":"bibr","target":"#b2"}]},{"text":"On the contrary, cover identification approaches must be able to identify a song from the recording of a performance, yet independently from the particular performance. For example, identification of live performances may not benefit from the fingerprint of other performances, because most of the acoustic parameters may be different. Collecting all the possible live and cover versions of a music work is clearly unfeasible.","refs":[]},{"text":"As stated in [4], one of the most interesting challenges in Music Information Retrieval is to exploit the complex interaction of the many different music information facets: pitch (melodic), temporal (rhythmic), harmonic, timbral, editorial, textual, and bibliographic. The system presented here attempts at exploiting in particular the harmonic and rhythmic facets.","refs":[{"start":13,"end":16,"marker":"bibr","target":"#b3"}]},{"text":"Cover music identification methodologies described in literature generally exploit Chroma features to describe the harmonic content of music recordings. In particular Chroma have been widely exploited in [5], [6] and [7]. Since Chroma features are high dimensional, they considerably affect computational time for search operations; efficiency becomes then a key issue if an identification system is proposed to a large community of users, as in the case of a Web-based music search engine or collaborative digital library.","refs":[{"start":204,"end":207,"marker":"bibr","target":"#b4"},{"start":209,"end":212,"marker":"bibr","target":"#b5"},{"start":217,"end":220,"marker":"bibr","target":"#b6"}]},{"text":"In [7], we proposed an efficient methodology to identify classical music recordings by applying the Locality Sensitive Hashing (LSH) paradigm [8], a general approach to handle high dimensional spaces by using ad-hoc hashing functions to create collisions between vectors that are close in the space. LSH has been applied to efficient search of different media [9].","refs":[{"start":3,"end":6,"marker":"bibr","target":"#b6"},{"start":142,"end":145,"marker":"bibr","target":"#b7"},{"start":360,"end":363,"marker":"bibr","target":"#b8"}]},{"text":"This paper focuses on pop music identification, through the integration of feature descriptors that are relative to two characterizing aspects of a song: harmonic and rhythmic content. The main idea is that usually cover songs preserve not only the harmonic-melodic characteristics of the original work but also its rhythmic profile. We show that combining information evidence from the two facets effectively improves identification rate.","refs":[]},{"text":"This work builds on previously published material [7,10].","refs":[{"start":50,"end":53,"marker":"bibr","target":"#b6"},{"start":53,"end":56,"marker":"bibr","target":"#b9"}]}]},{"title":"System Model","paragraphs":[{"text":"Evidence given by Chroma and rhythmic descriptors is combined into a single ranking of possible matching songs. The rhythmic descriptors used are Rhythm Histogram (RH) features [11], which were originally proposed in a genre classification task. While Chroma features have been thoroughly investigated previously and are provided with an efficient implementation, RH features have only recently been adopted by us and their performances in terms of speed are not comparable yet; both aspects are described below. An overview of the system is depicted in Figure 1.","refs":[{"start":177,"end":181,"marker":"bibr","target":"#b10"},{"start":561,"end":562,"marker":"figure","target":null}]}]},{"title":"Chroma Features","paragraphs":[{"text":"A music descriptor widely applied to cover identification is Chroma features. Chroma features are related to the intensity associated with each of the 12 semitones within an octave, with all octaves folded together. The concept behind Fig. 1. Overview of the system model chroma is that the perceived quality of a chord depends only partially on the octaves in which the individual notes are played. Instead, what seems to be relevant are the pitch classes of the notes (the names of the notes on the chromatic scale) that form a chord. This robustness to changes in octaves is also exploited by artists who play a cover song: while the main melody is usually very similar to the original one, the accompaniment can have large variations without affecting the recognizability of the song.","refs":[{"start":240,"end":241,"marker":"figure","target":null}]},{"text":"As described in [7], a Chroma vector c is a 12-dimensional vector of pitch classes, computed by processing a windowed signal with a Fourier transform. According to the approach proposed in [12], chroma features have been computed using the instantaneous frequency within each FFT bin to identify strong tonal components and to achieve higher resolution. In Figure 2(a) a Chroma vector corresponding to an A7 chord is depicted; Figure 2(b) shows the evolution of Chroma vectors over time for an excerpt of the song \"Heroes\" by D. Bowie.","refs":[{"start":16,"end":19,"marker":"bibr","target":"#b6"},{"start":189,"end":193,"marker":"bibr","target":"#b11"},{"start":364,"end":365,"marker":"figure","target":"#fig_1"},{"start":434,"end":438,"marker":"figure","target":"#fig_1"}]},{"text":"For each vector c, quantization q is achieved by considering the ranks of the chroma pitch classes, instead of their absolute values, to obtain a general representation robust to variations due to different performing styles. In particular, rank-based quantization is carried out by computing the rank of the value of the energy in the various pitch classes.","refs":[]},{"text":"Rank-based quantization aims at a final compact representation, which can be obtained by considering that a vector q can be thought as a twelve digit number represented in base k. A simple hashing function h can be computed by obtaining the decimal representation of this number, according to equation","refs":[]},{"text":"where additional hashing techniques can be applied to store the values h in one array, which can be accessed in constant time. A typical technique is to compute the reminder of h divided by a carefully chosen prime number. The described approach is applied both to the songs in the collection and to the queries. With the main goal of efficiency, retrieval is carried out using the bag of words paradigm. Similarity between the query and the recordings in the collection is measured by simply counting the number of hashes they have in common. This measure of similarity does not take into account the distribution of hash values. In particular, the occurrence of a chroma hash inside a song and the frequency of a chroma hash across the collection of documents have not been considered. The choice of this particular similarity measure has been motivated by a number of tests using short queries of about 10 seconds, where this simple measure outperformed more complex ones [7].","refs":[{"start":975,"end":978,"marker":"bibr","target":"#b6"}]},{"text":"Since queries of a cover identification task can be complete recordings, the frequency of chroma hashes and their relative position along the song may become a relevant piece of information. In order to handle this issue, long music queries have been divided into overlapping short sub-queries of a fixed duration and for each query an independent retrieval task is carried out. A similar processing is applied to documents, that are divided into overlapping frames with a length comparable to the one of the sub-queries. At the end, the result scores of each single retrieval are combined. In particular, preliminary evaluation showed that, in this context, geometric mean outperformed all the other main fusion techniques reported in literature [13].","refs":[{"start":747,"end":751,"marker":"bibr","target":"#b12"}]},{"text":"A problem that may affect retrieval effectiveness is that chroma-based representation is sensible to transpositions. The problem is not dealt with in this paper, as the focus mainly resides in the integration with rhythmic features; it is however part of future work and possible solutions are described in Section 4.","refs":[]}]},{"title":"Rhythm Histogram Features","paragraphs":[{"text":"Rhythm Histogram features [11] are a descriptor for the general rhythmic characteristics of an audio document. In a RH the magnitudes of each modulation frequency bin for all the critical bands of the human auditory range are summed up to form a histogram of \"rhythmic energy\" per modulation frequency.","refs":[{"start":26,"end":30,"marker":"bibr","target":"#b10"}]},{"text":"In their original form, a single \"global\" RH represents a whole piece; in our approach, as is the case for Chroma features, a sequence of RHs is computed for each song by segmenting the audio into overlapping windows of 15 seconds, in order to be able to individually match parts of songs which might be characterized by different rhythmic structures (e.g. verse and chorus). Figures 2(c  The first step in the computation of the similarity between the songs a and b is the construction of the similarity matrix M , in which each entry m ij is given by the cosine similarity of the i-th RH of a and the j-th RH of b. For each segment of a, the best matching segment of b (that is the one with the highest cosine similarity) is retained, and the mean of these values over all segments of a is computed; a simmetric procedure is then applied to song b and finally the average1 of these two scores is returned as the similarity of a and b. Experimental results showed that this strategy performs slightly better than the simpler comparison of the global RHs.","refs":[{"start":384,"end":387,"marker":"figure","target":"#fig_1"},{"start":873,"end":874,"marker":null,"target":"#foot_0"}]},{"text":"It is clear that this approach is computationally intensive, since the cosine similarity of the RHs must be computed for each song in the collection and for each segment pair. Possible optimizations, similar to the ones used for Chroma features, are under investigation. In Section 3.3 a straightforward strategy for reducing the computational load is proposed, based on the consideration that query songs can be compared to just a small subset of the songs in the collection while retaining the same precision in the results.","refs":[]}]},{"title":"Feature Combination","paragraphs":[{"text":"The similarity score s for a pair of songs, that governs the ranking returned by the system, is computed combining the two scores c and r given by the Chroma features and the Rhythm Histogram features respectively. Two strategies have been used:","refs":[]},{"text":"-weighted product","refs":[]},{"text":"As pointed out in Section 3.3, their performance is similar.","refs":[]}]},{"title":"Experimental Results","paragraphs":[{"text":"Experimental results are presented to show how performances can be improved by combining the scores for the two feature descriptors used. The performances of the system are evaluated using Mean Reciprocal Rank (MRR) as a measure of precision.","refs":[]}]},{"title":"Test Collection","paragraphs":[{"text":"The proposed approach has been evaluated with a test collection of 500 recordings of pop and rock songs, taken from personal collections of the authors. The idea was to have a collection as close as possible to a real scenario. In fact, the indexed documents were all the original version of the music works -i.e., the studio album versions -for which it is expected that metadata are correctly stored and that can be reasonably used in a real system as the reference collection.","refs":[]},{"text":"The query set included 60 recordings of different versions of a subset of the collection, which were live version by the same artist who recorded the original song and studio or live covers by other artists. We decided to have in the collection one single correct match for each query in order to balance the contribution of each different song. Queries had different durations, generally including the core of the music works -verses and choruses -plus possible introductions and endings, in particular in the live versions. All the audio files were stereophonic recordings (converted to monophonic in the audio feature extraction step) with a sampling rate of 44.1 kHz and stored in MP3 format at different bitrates (at most 192 kbps). In fact, in order to simulate a real context, we preferred a compressed format rather than an uncompressed one such as PCM.","refs":[]}]},{"title":"Individual Features Results","paragraphs":[{"text":"The performance of Chroma features individually is already satisfying, with a MRR of 78.4%. Rhythmic Histogram features on the other hand are less reliable, resulting in a MRR of 34.0%. If the RH features scores are computed directly on the global RH (instead of subdividing the song and computing the best match for each segment) MRR is is 28.5%.","refs":[]}]},{"title":"Combined Features Results","paragraphs":[{"text":"Figure 3 shows typical dispositions of the feature score pairs for some queries; each point in the feature space is associated to a comparison between a query song and the songs in the collection, the red circles being associated to the relevant matches. In particular Figure 3(a) is an example of the best possible situation, in which both Chroma features and Rhythm Histogram features individually rank the correct match for the query song in the first position. Figure 3(b) depicts the most common situation, in which Chroma features correctly identify the match but RH features are misleading; the dual situation is reported in Figure 3(c), which is rare but represents a significant evidence for the usefulness of RH features. Finally Figure 3(d  Perhaps the most interesting disposition of score pairs in the feature space is the one depicted in Figure 4: neither feature can identify the matching song by itself, but a combination of the two is indeed able to rank it in the first position.  The two approaches to feature combination reported in Equations 2 and 3 have been tested for several values of the parameter α, which weights the influence of RH features in the score, and the resulting MRRs are depicted in Figure 5. For the optimal value of α, the MRR increases from 78.4% (using only Chroma features) to 82.0% and 81.6%, using a linear combination and a weighted product of the features scores respectively. Similar performances are achieved using a single global RH for computing the similarity of songs, with a MRR of 81.5% for the case of the best linear combination of features. Even though the MRR maximum value is located in local peaks of the graphic, which are probably due to the rather small size of the test collection, setting α in a rather large neighbourhood of its optimal value still yields a significant improvement in MRR.","refs":[{"start":7,"end":8,"marker":"figure","target":"#fig_2"},{"start":276,"end":277,"marker":"figure","target":"#fig_2"},{"start":472,"end":473,"marker":"figure","target":"#fig_2"},{"start":639,"end":640,"marker":"figure","target":"#fig_2"},{"start":747,"end":750,"marker":"figure","target":"#fig_2"},{"start":859,"end":860,"marker":"figure","target":"#fig_3"},{"start":1230,"end":1231,"marker":"figure","target":"#fig_5"}]},{"text":"As anticipated in Section 2.2, it is clear that performing the comparison of a query song against the whole set of songs in the collection is unfeasible for a large collection, especially when comparing all the segments against each other. Fortunately Chroma features are able to rank the relevant match in the first positions, and this can be done efficiently thanks to the hashing mechanisms discussed above; an effective solution is to exploit this robustness by reranking only the top t position with the aid of Rhythm Histogram features: with t ranging from 15 to 50 the optimal MRR (82.0%) is unchanged for the collection used. Although the collection is very small, previous experiments with Chroma features on larger collections [7] have shown how the relevant matches for query songs are almost never ranked in very low positions, thus Rhythm Histogram features can be effectively exploited computing them on just a very small fraction of the songs in the collection.  ","refs":[{"start":737,"end":740,"marker":"bibr","target":"#b6"}]}]},{"title":"Conclusion","paragraphs":[{"text":"The paper presented a methodology for the pop music cover identification problem in a music digital library, mainly focusing on the improvements given by the introduction of a rhythmic profile descriptor in addition to the description of harmonic content. Many directions for future work are yet to be explored, and the most promising ones are briefly reviewed below.","refs":[]},{"text":"The modeling of harmonic content still lacks an effective solution for handling the possible transpositions in pitch of cover songs; in fact, if the cover song used as query and the original work stored in the collection are played in different tonalities, they will have totally different sets of chroma. This problem can be simply addressed by considering that a transposition of n semitones will result in a rotation of Chroma vectors of n steps. Therefore, the tonality issue can be faced by simply computing all the twelve transpositions for each query and identifying all the transposed versions, though this step is likely to decrease the performance both in terms of efficacy and efficiency. Alternatively, a methodology to estimate the key of a song may be exploited in order to transpose the chroma sets to a reference tonality [6]. Including key estimation algorithms in the proposed system will be an important part of future work.","refs":[{"start":838,"end":841,"marker":"bibr","target":"#b5"}]},{"text":"While harmonic content description has been deeply studied, the investigation of rhythmic content descriptors is still in a very early stage. In particular, computational performance is the main concern, and at this aim an hash-based retrieval approach is under development.","refs":[]},{"text":"Finally, it is clear how evaluation of the system should be performed on a larger test collection. However, this step poses additional issues, not only related to the size of the data that has to be managed, but also to problems regarding music","refs":[]}]}],"tables":{},"abstract":{"title":"Abstract","paragraphs":[{"text":"In this paper we report the status of our research on the problem of content-based cover song identification in music digital libraries. An approach which exploits both harmonic and rhythmic facets of music is presented and evaluated against a test collection. Directions for future work are proposed, and particular attention is given to the scalability challenge.","refs":[]}]}}