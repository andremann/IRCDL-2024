{"bibliography":{"title":"Keep, Change or Delete? Setting up a Low Resource OCR Post-correction Framework for a Digitized Old Finnish Newspaper Collection","authors":[{"person_name":{"surname":"Kettunen","first_name":"Kimmo"},"affiliations":[{"department":"Center for Preservation and Digitisation","institution":"National Library of Finland","laboratory":null}],"email":"kimmo.kettunen@helsinki.fi"}],"date":null,"ids":{"DOI":"10.1007/978-3-319-41938-1_11","arXiv":null},"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"keywords":["Ocr post-correction","Evaluation","Historical newspaper collections"],"citations":{"b0":{"title":"A Nordic digital newspaper library","authors":[{"person_name":{"surname":"Bremer-Laamanen","first_name":"M.-L"},"affiliations":[],"email":null}],"date":{"year":"2001","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"Int. Preserv. News","series":null,"scope":{"volume":26,"pages":{"from_page":18,"to_page":20}}},"b1":{"title":"Connecting to the pastnewspaper digitization in the Nordic countries","authors":[{"person_name":{"surname":"Bremer-Laamanen","first_name":"M.-L"},"affiliations":[],"email":null}],"date":{"year":"2005","month":"08","day":"18"},"ids":null,"target":"http://archive.ifla.org/IV/ifla71/papers/019e-Bremer-Laamanen.pdf","publisher":null,"journal":null,"series":null,"scope":null},"b2":{"title":"In the spotlight for crowdsourcing","authors":[{"person_name":{"surname":"Bremer-Laamanen","first_name":"M.-L"},"affiliations":[],"email":null}],"date":{"year":"2014","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"Scand. Librarian Q","series":null,"scope":{"volume":1,"pages":{"from_page":18,"to_page":21}}},"b3":{"title":"Analyzing and improving the quality of a historical news collection using language technology and statistical machine learning methods","authors":[{"person_name":{"surname":"Kettunen","first_name":"K"},"affiliations":[],"email":null},{"person_name":{"surname":"Honkela","first_name":"T"},"affiliations":[],"email":null},{"person_name":{"surname":"Lindén","first_name":"K"},"affiliations":[],"email":null},{"person_name":{"surname":"Kauppinen","first_name":"P"},"affiliations":[],"email":null},{"person_name":{"surname":"Pääkkönen","first_name":"T"},"affiliations":[],"email":null},{"person_name":{"surname":"Kervinen","first_name":"J"},"affiliations":[],"email":null}],"date":{"year":"2014","month":null,"day":null},"ids":null,"target":"http://www.ifla.org/files/assets/newspapers/Geneva_2014/s6-honkela-en.pdf","publisher":null,"journal":null,"series":null,"scope":null},"b4":{"title":"How good can it get? Analysing and improving OCR accuracy in large scale historic newspaper digitisation programs","authors":[{"person_name":{"surname":"Holley","first_name":"R"},"affiliations":[],"email":null}],"date":{"year":"2009","month":null,"day":null},"ids":null,"target":"http://www.dlib.org/dlib/march09/holley/03holley.html","publisher":null,"journal":"D-Lib Mag","series":null,"scope":{"volume":3,"pages":null}},"b5":{"title":"Reducing OCR errors in Gothic-script documents","authors":[{"person_name":{"surname":"Furrer","first_name":"L"},"affiliations":[],"email":null},{"person_name":{"surname":"Volk","first_name":"M"},"affiliations":[],"email":null}],"date":{"year":"2011","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":97,"to_page":103}}},"b6":{"title":"The current state-of-art in newspaper digitization. a market perspective","authors":[{"person_name":{"surname":"Klijn","first_name":"E"},"affiliations":[],"email":null}],"date":{"year":"2008","month":null,"day":null},"ids":null,"target":"http://www.dlib.org/dlib/january08/klijn/01klijn.html","publisher":null,"journal":"D-Lib Mag","series":null,"scope":{"volume":14,"pages":{"from_page":5,"to_page":5}}},"b7":{"title":"Unsupervised post-correction of OCR errors","authors":[{"person_name":{"surname":"Niklas","first_name":"K"},"affiliations":[],"email":null}],"date":{"year":"2010","month":null,"day":null},"ids":null,"target":"www.l3s.de/*tahmasebi/Diplomarbeit_Niklas.pdf","publisher":null,"journal":null,"series":null,"scope":null},"b8":{"title":"Evaluation of model-based retrieval effectiveness with OCR text","authors":[{"person_name":{"surname":"Taghva","first_name":"K"},"affiliations":[],"email":null},{"person_name":{"surname":"Borsack","first_name":"J"},"affiliations":[],"email":null},{"person_name":{"surname":"Condit","first_name":"A"},"affiliations":[],"email":null}],"date":{"year":"1996","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"ACM Trans. Inf. Syst","series":null,"scope":{"volume":14,"pages":{"from_page":64,"to_page":93}}},"b9":{"title":"Measuring mass text digitization quality and usefulness. Lessons learned from assessing the OCR accuracy of the british library's 19th century online newspaper Archive","authors":[{"person_name":{"surname":"Tanner","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Muñoz","first_name":"T"},"affiliations":[],"email":null},{"person_name":{"surname":"Ros","first_name":"P"},"affiliations":[],"email":null}],"date":{"year":"2009","month":null,"day":null},"ids":null,"target":"http://www.dlib.org/dlib/july09/munoz/07munoz.html","publisher":null,"journal":"D-Lib Magazine","series":null,"scope":{"volume":15,"pages":null}},"b10":{"title":"Optical character recognition errors and their effects on natural language processing","authors":[{"person_name":{"surname":"Lopresti","first_name":"D"},"affiliations":[],"email":null}],"date":{"year":"2009","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"Int. J. Doc. Anal. Recogn","series":null,"scope":{"volume":12,"pages":{"from_page":141,"to_page":151}}},"b11":{"title":"Digitalkoot: making old archives accessible using crowdsourcing","authors":[{"person_name":{"surname":"Chrons","first_name":"O"},"affiliations":[],"email":null},{"person_name":{"surname":"Sundell","first_name":"S"},"affiliations":[],"email":null}],"date":{"year":"2011","month":null,"day":null},"ids":null,"target":"http://www.aaai.org/ocs/index.php/WS/AAAIW11/paper/view/3813/4246","publisher":null,"journal":null,"series":null,"scope":null},"b12":{"title":"How to do lexical quality estimation of a large OCRed historical Finnish newspaper collection with scarce resources","authors":[{"person_name":{"surname":"Kettunen","first_name":"K"},"affiliations":[],"email":null},{"person_name":{"surname":"Pääkkönen","first_name":"T"},"affiliations":[],"email":null}],"date":{"year":"2016","month":null,"day":null},"ids":null,"target":"http://www.lrec-conf.org/proceedings/lrec2016/pdf/17_Paper.pdf","publisher":null,"journal":null,"series":null,"scope":null},"b13":{"title":"Techniques for automatically correcting words in text","authors":[{"person_name":{"surname":"Kukich","first_name":"K"},"affiliations":[],"email":null}],"date":{"year":"1992","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"ACM Comput. Surv","series":null,"scope":{"volume":24,"pages":{"from_page":377,"to_page":439}}},"b14":{"title":"Foundations of Statistical Language Processing","authors":[{"person_name":{"surname":"Manning","first_name":"C"},"affiliations":[],"email":null},{"person_name":{"surname":"Schütze","first_name":"H"},"affiliations":[],"email":null}],"date":{"year":"1999","month":null,"day":null},"ids":null,"target":null,"publisher":"The MIT Press","journal":null,"series":null,"scope":null},"b15":{"title":"How to write a spelling corrector","authors":[{"person_name":{"surname":"Norvig","first_name":"P"},"affiliations":[],"email":null}],"date":{"year":"2008","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null}},"sections":[{"title":"Introduction","paragraphs":[{"text":"Newspapers of the 19 th and early 20 th century were many times printed in the Gothic (Fraktur, blackletter) typeface in Europe. It is well known that the typeface is difficult to recognize for OCR software [5,6]. Other aspects that affect the quality of the OCR recognition are the following, among others [6,7]: quality of the original source and microfilm, scanning resolution and file format, layout of the page, OCR engine training, etc.","refs":[{"start":207,"end":210,"marker":"bibr","target":"#b4"},{"start":210,"end":212,"marker":"bibr","target":"#b5"},{"start":307,"end":310,"marker":"bibr","target":"#b5"},{"start":310,"end":312,"marker":"bibr","target":"#b6"}]},{"text":"As a result of these difficulties scanned and OCRed document collections have a varying number of errors in their content. The number of errors depends heavily on the period and printing form of the original data. Older newspapers and magazines are more difficult for OCR; newspapers from the 20 th century are easier (cf. for example data of [8] that consists of a 200 year period of The Times of London from 1785 to 1985). There is no clear measure of the number of errors that makes the material useful or less useful for some purpose, and the use purposes of the digitized material vary hugely. A linguist who is interested in the forms of the words needs as error free data as possible; a historian who interprets the texts on a more general level may be satisfied with text data that has more errors.","refs":[{"start":343,"end":346,"marker":"bibr","target":"#b7"}]},{"text":"OCR errors in digitized text collections may have several harmful effects, one of the most important being possibly worse searchability of the documents in the collections. Ranking of the documents in search result list is usually clearly harmed. With high-enough word level accuracy of the OCRed collections searchability is not harmed significantly according to Taghva et al. [9]. Tanner et al. [10] suggest that word accuracy rates less than 80 % are harmful for search, but when the word accuracy is over 80 %, fuzzy search capabilities of search engines should manage the problems caused by word errors.","refs":[{"start":378,"end":381,"marker":"bibr","target":"#b8"},{"start":397,"end":401,"marker":"bibr","target":"#b9"}]},{"text":"Other effects of poor OCR quality will show in the more detailed processing of the documents, such as sentence boundary detection, tokenization and part-of-speechtagging, which are important in higher-level natural language processing tasks [11]. Part of the problems may be local, but part will cumulate in the whole pipeline of NLP processing causing errors. Thus the quality of the OCRed texts is the cornerstone for any kind of further usage of the material.","refs":[{"start":241,"end":245,"marker":"bibr","target":"#b10"}]}]},{"title":"Framework of Post-correction","paragraphs":[{"text":"Correction of the OCR output can be done interactively during the OCR process and interactively or non-interactively after the OCR process has finished. Then it can be based on crowdsourcing or automatic correction. Crowdsourcing with the Digi data has been tried with small amount of data [12], but as the amount of data is ca. 2.385 G words, this approach is clearly not feasible. It is obvious that partial or total re-OCRing and automatic post-correction are the only realistic ways of improving the quality of the data. We shall concentrate on the automatic post-correction in our discussion.","refs":[{"start":290,"end":294,"marker":"bibr","target":"#b11"}]},{"text":"In [4] we evaluated the quality of the Finnish Digi data with seven smallish samples which have about 212 000 words altogether. The results of the evaluation show that the quality varies from about 60 % word accuracy at worst to about 90 % accuracy at best. The evaluation samples, however, are small, and it is hard to estimate what the overall quality of the data is. We expect it to be somewhere in the range of 50-80 % accuracy, but there may be a lot of variation. As the spelling error examples of [4] show, there are lots of really hard misspellings in the data, up to Levenshtein distance of 8 and even further from that. A detailed analysis with a modern Finnish morphological analyser Omorfi1 [13], has shown, that about 69 % of the tokens in the collection can be recognized. If v/w variation and estimation of out-of-vocabulary words is taken into account, the estimated recognition rate is about 74-75 %. The detailed analysis showed that about 625 M of the words in the collection are unrecognized, and most of them are OCR errors, and probably at least half of them hard errors.","refs":[{"start":3,"end":6,"marker":"bibr","target":"#b3"},{"start":504,"end":507,"marker":"bibr","target":"#b3"},{"start":701,"end":702,"marker":null,"target":"#foot_0"},{"start":703,"end":707,"marker":"bibr","target":"#b12"}]}]},{"title":"Post-correction","paragraphs":[{"text":"Our discussion of OCR post-correction in this paper concerns non-word error detection and isolated word error correction as defined in Kukich [14]. Non-word detection detects words that do not occur in the used dictionary or wordlist. Isolated word correction tries to correct single words out of their context. There are several techniques for doing this, and Kukich [14], for example, lists six different approaches. In our result examples we will show results of one particular technique, minimum edit distance aka Levenshtein distance. In this phase we do not aim to do real-world spelling-correction, i.e. context sensitive word correction, as this would clearly be out of the scope of our means and resources.","refs":[{"start":142,"end":146,"marker":"bibr","target":"#b13"},{"start":368,"end":372,"marker":"bibr","target":"#b13"}]},{"text":"OCR result evaluation and post-correction evaluation are based on character level match between the characters of the output of the OCR results and the original \"error free\" data. The originals used as the comparisonmany times known as ground truthare usually hand-edited material or good quality parallel digital versions of the material. Due to lack of availability of high quality comparison material, evaluations of the digitation process and its post-correction are mainly based on quite small samples, which is inevitable.","refs":[]}]},{"title":"Evaluation Data","paragraphs":[{"text":"As evaluation data we use six of the seven parallel samples used in [4]. One of the samples is too large to be used with the OCR Frontiers toolkit and was omitted. Number of words in these six corpuses is about 63 000. Besides that we have two different compiled wordlists. The wordlists are 3850_L (word count 3855), and 3850L_8000 M (word count 11 971). 3850_L has been compiled in Department of Modern Languages at the University of Helsinki. 3850L_8000 M is a mixture of the data of Crohns and Sundell [12] with 8116 words from the crowd-sourced data combined with the 3850_L wordlist. Both of these lists have word pairs where one is the misspelled word and the other the correct version. The accuracy of the lists has not been intellectually checked, they are used on as is basis.","refs":[{"start":68,"end":71,"marker":"bibr","target":"#b3"},{"start":506,"end":510,"marker":"bibr","target":"#b11"}]},{"text":"Some comments on the nature of the evaluation data are in order. Newspaper data is realistic in its error counts, and the six different corpuses have different number of errors, as shown in Table 1. Word pair lists are more artificial in their distributions. 3850_L word list has an error percentage of about 17 % (3195 correct word pairs and 660 erroneous ones), which seems low compared to our real data. 3850L_8000 M contains 3393 correct word pairs (72 % errors).","refs":[{"start":196,"end":197,"marker":"table","target":"#tab_0"}]}]},{"title":"Evaluation Measures","paragraphs":[{"text":"There are various possible measures to use in OCR post-correction evaluation. In our quality assessment of the Digi data [4] we used word accuracy, word error rate, precision, recall and Fmean for measuring the number of errors the evaluation samples have. We used four different readymade software for the analysis. Two of them were dedicated OCR evaluation software, two MT quality evaluation software. One of them, OCR Frontiers Toolkit 1.02 , which measures word accuracy, is also used in this paper because the software is able to evaluate the parallel newspaper data comparing the original data to output of the spelling correction with one word per line. Word level accuracy is not a very good measure while it is only sensitive to the number of errors in comparison and does not show details of correction [15: 269]. With this material, however, it is the most suitable available measure, as the data is not in one-to-one correspondence on word level.","refs":[{"start":121,"end":124,"marker":"bibr","target":"#b3"},{"start":443,"end":444,"marker":null,"target":"#foot_1"},{"start":814,"end":823,"marker":"bibr","target":null}]},{"text":"For the wordlist data we have compiled later we use recall, precision and F-score [15: 268-269]. Given that we have tuples of error, original and correction, <$1, $2, $3> , we can define true positives, false positives, false negatives and true negatives as follows using Gnu-AWK's notation of is equal to (==), is not equal to (! =) and conjunction (&&):","refs":[{"start":82,"end":95,"marker":"bibr","target":null}]},{"text":"• (($1 ! = $2) && ($2 == $3)) TP, true positive: a wrongly spelled word is corrected • (($1 == $2) && ($2 ! = $3)) FP, false positive: a correct word is changed to a misspelling • (($1 ! = $2) && ($2 ! = $3)) FN, false negative: a wrongly spelled word is wrong after correction • (($1 == $2) && ($2 == $3)) TN, true negative: a correct word is correct after correction Recall, R, is TP /(TP + FN), Precision, P, is TP /(TP + FP) and F-score, F, is 2*R*P /(R + P).","refs":[]}]},{"title":"Correction Algorithm","paragraphs":[{"text":"After initial trials with different correction approaches in [4] we have been working with a Levenshtein distance (LD) correction algorithm introduced in [16]. The original version is a Python program that uses only LD 2, so it is able to correct two errors per word at maximum. This is a reasonable limit, while many of the OCR errors are in this range. We use the Gnu-AWK (GAWK) version of the algorithm which was implemented by Gregory Greffenstette3 with some modifications of our own. Levenshtein distance, also known as minimum edit distance, is the minimum number of editing operations necessary to transform one word into another. An editing operation is a character insertion, deletion, substitution or transposition.","refs":[{"start":61,"end":64,"marker":"bibr","target":"#b3"},{"start":154,"end":158,"marker":"bibr","target":"#b15"},{"start":452,"end":453,"marker":null,"target":"#foot_2"}]},{"text":"The original algorithm uses a frequency dictionary as a language model (LM) and makes corrections according to the model. We added another, much larger dictionary to the algorithm to verify first, that the word being processed is not already included in the lexicon and thus possibly a correct spelling. If it is not, the word will be sent to correction. We'll call this dictionary the verification dictionary (VD). We also added one simple rule, change of c to e ðc ! eÞ between non-vowels, as this is one of the most common OCR errors in the data. Some trash deletion was also added, but the core algorithm is the original GAWK implementation. The algorithm returns only the correction, not a list of correction candidates. If the length of the processed word is less or equal to three characters, correction will not be tried in our version. The dictionaries we use with the algorithm have been compiled from different sources using for example frequency list of Early modern Finnish from Kotus4 with about 530 000 words, four dictionaries from the 19 th century 5 and other available material, also from the Digi collection. We have been experimenting with different lexicons and different LD levels with the algorithm, and will report the results in the following.","refs":[{"start":997,"end":998,"marker":null,"target":"#foot_3"},{"start":1066,"end":1067,"marker":null,"target":"#foot_4"}]}]},{"title":"Results","paragraphs":[{"text":"Results of the newspaper data and wordlists are shown and discussed separately as they use different evaluation measures. Table 1 shows results of the newspaper material. We have tried different Levenshtein distance levels from the basic 2 up to 5, but report only the basic results and the results with LD 5, as there is no real difference in most of the cases between the different LD levels.","refs":[{"start":128,"end":129,"marker":"table","target":"#tab_0"}]},{"text":"Results of the newspaper material correction show that with lower quality data the correction algorithm works reasonably well, it is able to improve word accuracy with 6-10 % units in all three evaluation data sets. With better quality data the results are not that good: correction is able to keep the quality of the data at the same level in two cases, but in one case the quality deteriorates quite a lot, 5.3 % units. Overall the results are fair, but it seems that there is not much possibility for improvement with the used algorithm. Selection of dictionaries used with the algorithm has a modest impact on the results, but it seems that the best results are achieved when the LM dictionary is quite small, about 1.35 M words. Much larger LM dictionaries do not seem to give any gain in performance. Effect of the VD dictionary will be discussed with word list results.","refs":[]},{"text":"Results of the word list correction are shown in Tables 2 and3. Table 2 shows results of the 3850_L wordlist, Table 3 shows results of the 3850L_8000 M list.","refs":[{"start":56,"end":57,"marker":"table","target":"#tab_1"},{"start":61,"end":62,"marker":"table","target":null},{"start":70,"end":71,"marker":"table","target":"#tab_1"},{"start":116,"end":117,"marker":"table","target":null}]},{"text":"There are some clear and mainly expected trends in the results. Usage of the VD, verification dictionary, improves precision and hurts recall to some extent. This can also be seen in the number of false positives, which doubles or almost triples if no lexical check is done before sending the word to correction. Size of the VD is 4.96 M words.","refs":[]},{"text":"Recall in the 3850_L sample varies from about 0.44 to 0.49. Precision varies from 0.66 to 0.90, and F-score is round 0.55-0.59. In the 3850_8000 M sample recall varies from 0.27-0.34 and precision from 0.89-0.97, F-score being from 0.42 to 0.50. Language model dictionary that has both v and w versions of words containing either letter improves recall with about 1.0 % unit and precision with 2-3 % units. Punctuation and numbers do not occur much in the 3850_L sample and their inclusion or exclusion in the evaluation does not change results. In the 3850_8000 M sample results without punctuation and numbers are about 6-8 % units better than with punctuation and numbers.","refs":[]},{"text":"We can see that usage of LD 5 does not improve results much. Recall can be slightly better when using LD 5, but precision is worse with 3850_L and at the same level with 3850_8000 M. Usage of the VD is not clearly beneficial with the wordlists, although it gave the best results with the newspaper material. This may be due to different measures: word accuracy hides details of performance, and the improvement VD brings with the newspaper material is shown to be more ambiguous when precision and recall are used as measures.  ","refs":[]}]},{"title":"Discussion and Conclusion","paragraphs":[{"text":"We have reported in this paper first results of post-correction of OCRed data from a Finnish digitized newspaper and magazine collection, that contains 1.95 M pages of data and about 2.385 G words of Finnish from the period between 1771 and 1910. Our sample evaluation data are mainly from years between 1830 and 1870 and year 1882, which is the period of so called Early modern Finnish. Evaluations of the postcorrection were partly based on parallel text material gathered earlier [4] and partly on compiled word pair lists of the digitized material. The chosen post-correction method was a straightforward Levenshtein distance based algorithm with some additions. The results we have shown are fair, but not good enough for realistic postcorrection as the only correction method. However, they show that the quality of even quite poor OCR output can be improved with a quite simple approach. If the data were not so bad, we could perhaps be able to improve the quality of the Digi collection even with the current quite simple approach enough for our purposes. Our material contains lots of hard errors, and as it was seen in the results section, only the simplest ones seem to get corrected and usage of deeper LD levels does not help. Usage of the VD dictionary helps in correction, but increasing its size substantially does not bring much improvement. Without VD look-up the correction algorithm creates quite a lot of false positives that decrease precision. Size of the LM dictionary (1.35 M tokens) seems quite optimal. Including the v/w variation in the LM dictionary seems to be beneficial, too.","refs":[{"start":483,"end":486,"marker":"bibr","target":"#b3"}]},{"text":"Many of the OCR post-correction evaluations use data that has already a quite high correctness percentage [6] and thus they can also set high expectations for the results achieved. Examples of our data, the British Library data [10] and The Times of London [8] show that the quality level of a large OCRed 19 th century newspaper collection is not very high and thus it is only reasonable to set the aims in correction not too ambitious. If we can improve the quality of the data with usage of re-OCRing and isolated word post-correction cycle to the level of some 80-90 % word correctness overall, that would improve usability of the material a lot, and would also meet our current needs quite well. After that context sensitive real world spelling correction might also be possible, if that would be needed.","refs":[{"start":106,"end":109,"marker":"bibr","target":"#b5"},{"start":228,"end":232,"marker":"bibr","target":"#b9"},{"start":257,"end":260,"marker":"bibr","target":"#b7"}]},{"text":"The main value of our work so far has been the set-up of the whole correction and evaluation chain and gaining experience with the correction and the data. We have acquired invaluable experience concerning the quality of our material and gathered both useful tools and word list data to be used in the correction. With the experience we can plan further steps of the post-correction realistically.","refs":[]}]}],"tables":{"tab_0":{"heading":"Table 1 .","description":"Correction results of the newspaper material","rows":[["Collection","Original word","Correction","Correction","Best correction result"],["","accuracy","results with","results with","vs. original +/-, per"],["","results from","LD 2","LD 5","cent units"],["","[4]","","",""],["Suometar 1847","71.0 %","79.2 %","79 %","+8.2"],["Keski-Suomi","60.5 %","70.7 %","70.1 %","+10.2"],["1871","","","",""],["Sanan Saattaja","73.8 %","80 %","79.6 %","+6.2"],["Wiipurista 1841","","","",""],["Turun","80.4 %","80.5 %","80.6 %","+0.2"],["Wiikko-Sanomat","","","",""],["1831","","","",""],["Oulun","83 %","83.2 %","82.9 %","+0.2"],["Viikko-Sanomia","","","",""],["1841","","","",""],["Kirjallinen","82.1 %","76.8 %","76.6 %","-5.3"],["Kuukauslehti","","","",""],["1870","","","",""]]},"tab_1":{"heading":"Table 2 .","description":"Correction results of the 3850_L word list","rows":[["3850_L","Basic","LM W/V","Basic LM LD","LM W/V LD"],["","LM LD 2","LD 2","5","5"],["With VD","R = 0.43","","",""],["","P = 0.86","","",""],["","F = 0.57","","",""],["","FP = 46","","",""]]}},"abstract":{"title":"Abstract","paragraphs":[{"text":"There has been a huge interest in digitization of both hand-written and printed historical material in the last 10-15 years and most probably this interest will only increase in the ongoing Digital Humanities era. As a result of the interest we have lots of digital historical document collections available and will have more of them in the future.","refs":[]},{"text":"The National Library of Finland has digitized a large proportion of the historical newspapers published in Finland between 1771 and 1910 [1-3]; the collection, Digi, can be reached at http://digi.kansalliskirjasto.fi/. This collection contains approximately 1.95 million pages in Finnish and Swedish, the Finnish part being about 2.385 billion words. In the output of the Optical Character Recognition (OCR) process, errors are common especially when the texts are printed in the Gothic (Fraktur, blackletter) typeface. The errors lower the usability of the corpus both from the point of view of human users as well as considering possible elaborated text mining applications. Automatic spell checking and correction of the data is also difficult due to the historical spelling variants and low OCR quality level of the material.","refs":[]},{"text":"This paper discusses the overall situation of the intended post-correction of the Digi content and evaluation of the correction. We shall present results of our post-correction trials, and discuss some aspects of methodology of evaluation. These are the first reported evaluation results of post-correction of the data and the experiences will be used in planning of the post-correction of the whole material.","refs":[]}]}}