{"bibliography":{"title":"Extracting Dependency Relations from Digital Learning Content","authors":[{"person_name":{"surname":"Adorni","first_name":"Giovanni"},"affiliations":[{"department":"Department of Informatics, Bioengineering, Robotics and Systems Engineering","institution":"University of Genoa","laboratory":null}],"email":null},{"person_name":{"surname":"Dell'orletta","first_name":"Felice"},"affiliations":[{"department":"Istituto di Linguistica Computazionale Antonio Zampolli (ILCCNR)","institution":null,"laboratory":null}],"email":"felice.dellorletta@ilc.cnr.it"},{"person_name":{"surname":"Koceva","first_name":"Frosina"},"affiliations":[{"department":"Department of Informatics, Bioengineering, Robotics and Systems Engineering","institution":"University of Genoa","laboratory":null}],"email":"frosina.koceva@edu.unige.it"},{"person_name":{"surname":"Torre","first_name":"Ilaria"},"affiliations":[{"department":"Department of Informatics, Bioengineering, Robotics and Systems Engineering","institution":"University of Genoa","laboratory":null}],"email":"ilaria.torre@unige.it"},{"person_name":{"surname":"Venturi","first_name":"Giulia"},"affiliations":[{"department":"Istituto di Linguistica Computazionale Antonio Zampolli (ILCCNR)","institution":null,"laboratory":null}],"email":"giulia.venturi@ilc.cnr.it"}],"date":null,"ids":{"DOI":"10.1007/978-3-319-73165-0_11","arXiv":null},"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"keywords":[],"citations":{"b0":{"title":"Evaluating anaphora and coreference resolution to improve automatic keyphrase extraction","authors":[{"person_name":{"surname":"Basaldella","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Chiaradia","first_name":"G"},"affiliations":[],"email":null},{"person_name":{"surname":"Tasso","first_name":"C"},"affiliations":[],"email":null}],"date":{"year":"2016","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":804,"to_page":814}}},"b1":{"title":"A contrastive approach to multi-word term extraction from domain corpora","authors":[{"person_name":{"surname":"Bonin","first_name":"F"},"affiliations":[],"email":null},{"person_name":{"surname":"Dell'orletta","first_name":"F"},"affiliations":[],"email":null},{"person_name":{"surname":"Venturi","first_name":"G"},"affiliations":[],"email":null},{"person_name":{"surname":"Montemagni","first_name":"S"},"affiliations":[],"email":null}],"date":{"year":"2010","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b2":{"title":"T2k 2 : a system for automatically extracting and organizing knowledge from texts","authors":[{"person_name":{"surname":"Dell'orletta","first_name":"F"},"affiliations":[],"email":null},{"person_name":{"surname":"Venturi","first_name":"G"},"affiliations":[],"email":null},{"person_name":{"surname":"Cimino","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Montemagni","first_name":"S"},"affiliations":[],"email":null}],"date":{"year":"2014","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":2062,"to_page":2070}}},"b3":{"title":"Accurate methods for the statistics of surprise and coincidence","authors":[{"person_name":{"surname":"Dunning","first_name":"T"},"affiliations":[],"email":null}],"date":{"year":"1993","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"Comput. Linguist","series":null,"scope":{"volume":19,"pages":{"from_page":61,"to_page":74}}},"b4":{"title":"The C-value/NC value domain independent method for multi-word term extraction","authors":[{"person_name":{"surname":"Frantzi","first_name":"K"},"affiliations":[],"email":null},{"person_name":{"surname":"Ananiadou","first_name":"S"},"affiliations":[],"email":null}],"date":{"year":"1999","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"J. NLP","series":null,"scope":{"volume":6,"pages":{"from_page":145,"to_page":179}}},"b5":{"title":"Learning hierarchies","authors":[{"person_name":{"surname":"Gagné","first_name":"R"},"affiliations":[],"email":null}],"date":null,"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":118,"to_page":131}}},"b6":{"title":"A review of semi-automatic approaches to build concept maps","authors":[{"person_name":{"surname":"Kowata","first_name":"J"},"affiliations":[],"email":null},{"person_name":{"surname":"Cury","first_name":"D"},"affiliations":[],"email":null},{"person_name":{"surname":"Boeres","first_name":"M"},"affiliations":[],"email":null}],"date":{"year":"2010","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":40,"to_page":48}}},"b7":{"title":"Measuring prerequisite relations among concepts","authors":[{"person_name":{"surname":"Liang","first_name":"C"},"affiliations":[],"email":null},{"person_name":{"surname":"Wu","first_name":"Z"},"affiliations":[],"email":null},{"person_name":{"surname":"Huang","first_name":"W"},"affiliations":[],"email":null},{"person_name":{"surname":"Giles","first_name":"C"},"affiliations":[],"email":null}],"date":{"year":"2015","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":1668,"to_page":1674}}},"b8":{"title":"Prerequisite relation learning for concepts in MOOCs","authors":[{"person_name":{"surname":"Pan","first_name":"L"},"affiliations":[],"email":null},{"person_name":{"surname":"Li","first_name":"C"},"affiliations":[],"email":null},{"person_name":{"surname":"Li","first_name":"J"},"affiliations":[],"email":null},{"person_name":{"surname":"Tang","first_name":"J"},"affiliations":[],"email":null}],"date":{"year":"2017","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":"Long Papers","scope":{"volume":1,"pages":{"from_page":1447,"to_page":1456}}},"b9":{"title":"Using prerequisites to extract concept maps from textbooks","authors":[{"person_name":{"surname":"Wang","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Ororbia","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Wu","first_name":"Z"},"affiliations":[],"email":null},{"person_name":{"surname":"Williams","first_name":"K"},"affiliations":[],"email":null},{"person_name":{"surname":"Liang","first_name":"C"},"affiliations":[],"email":null},{"person_name":{"surname":"Pursel","first_name":"B"},"affiliations":[],"email":null},{"person_name":{"surname":"Giles","first_name":"C"},"affiliations":[],"email":null}],"date":{"year":"2016","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":317,"to_page":326}}},"b10":{"title":"Burst analysis of text document for automatic concept map creation","authors":[{"person_name":{"surname":"Yoon","first_name":"W"},"affiliations":[],"email":null},{"person_name":{"surname":"Lee","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Lee","first_name":"S"},"affiliations":[],"email":null}],"date":{"year":"2014","month":null,"day":null},"ids":{"DOI":"10.1007/978-3-319-07467-2_43","arXiv":null},"target":"https://doi.org/10.1007/978-3-319-07467-2","publisher":"Springer","journal":null,"series":null,"scope":{"volume":8482,"pages":{"from_page":407,"to_page":416}}}},"sections":[{"title":"Introduction","paragraphs":[{"text":"The 21th century is marked by the exponential growth of data and of digital contents. Digital libraries evolved from static storage and retrieval platforms to dynamic services to explore, exchange and share information and knowledge.","refs":[]},{"text":"In this paper, our focus is on the potential role of digital libraries for education. The idea is that digital resources can not only be explored and shared but they can be coupled with services that support learning processes. This usually requires that content is extracted, structured and enriched with annotations. Since the objective is supporting learning, the extraction of relevant concepts has to be complemented with the identification of prerequisite relations among these concepts. This enables the building of services that, for example, enable to find pieces of knowledge in the text and to extract also the related propaedeutic concepts and resources that allow such information to be properly understood (prerequisite relations).","refs":[]},{"text":"Manual annotation is of course the most effective approach, but it is time consuming and requires experts knowledge. Therefore, a challenge is the automatic learning of the knowledge structure of the content.","refs":[]},{"text":"While several methods exist (e.g., [1,3]) to face the issue of concept extraction, the identification of prerequisite relations among concepts is still an open research problem. In this paper we present methods and approaches for facing this issue.","refs":[{"start":35,"end":38,"marker":"bibr","target":"#b0"},{"start":38,"end":40,"marker":"bibr","target":"#b2"}]}]},{"title":"Research Issue and Background","paragraphs":[{"text":"The two main tasks for automatic concept map building are the concept extraction and the relations identification between concepts [7]. Even though there is a long-standing interest since at least 1971 Gagné's work on learning hierarchies [6], identifying prerequisite relations among concepts is an open issue.","refs":[{"start":131,"end":134,"marker":"bibr","target":"#b6"},{"start":239,"end":242,"marker":"bibr","target":"#b5"}]},{"text":"The prerequisite relation between two concepts A and B is a dependency relation which represents what a learner must know/study (concept A), before approaching concept B. Thus, A is a propaedeutic concept, i.e. a requirement, for B and the learner should first understand A in order to understand B.","refs":[]},{"text":"The prerequisite relation can represent a hyponymy or meronymy relation in the case where the hyponym/meronym concept is going to be further in-depth studied and therefor is itself a prerequisite to another concepts. The prerequisite relation usually requires experts to be evaluated since its semantics can be properly evaluated only by considering the whole graph and the learning goal.","refs":[]},{"text":"Notation. In the following we provide the conventions and definitions that will be used along the paper. We define a document D as a textual resource. The output of the concept extraction is the terminology T ∈ D with t ∈ T , where t is a domain-specific term, composed of one or more words (single nominal terms or complex nominal structures with modifiers). For each term, the process returns also its relevance r = [0, 1] (see Sect. 3 for definition).","refs":[]},{"text":"When D is structured into parts, sections (S), the output of the concept extraction can be T ∈ D and T ∈ S according to the needs. Subsections are managed as Sections. Thus we have concept-document and concept-section relationships. We denote these relationships as relevance functions F (•, •) which take the concept and D/S as arguments and have the relevance r as output.","refs":[]},{"text":"The final output of concepts and prerequisite relations extraction is a concept graph G. Similarly to [10], we represent G as a set of triples in the form","refs":[{"start":102,"end":106,"marker":"bibr","target":"#b9"}]},{"text":", where p is the prerequisite relationship and can take a value from 0 to 1, indicating the strength of the prerequisite relation between t 1 and t 2 (where t 1 is prerequisite of t 2 ).","refs":[]},{"text":"Term appearance in section is defined as a pair (t i , s j ), t i ∈ T and s j ∈ S.","refs":[]}]},{"title":"Concept Extraction","paragraphs":[{"text":"Our approach to the identification of prerequisite relations was tested on the handbook entitled Computer Science: An Overview: Global Edition, G. Brookshear and D. Brylow, Pearson 2015. In order to identify relevant concepts within the considered book, we exploited Text-To-Knowledge (T2K 2 )","refs":[]},{"text":"[3], a software platform developed at the Institute of Computational Linguistics \"A. Zampolli\" of the CNR in Pisa. T2K 2 relies on a battery of tools for Natural Language Processing, statistical text analysis and machine learning which are dynamically integrated to provide an accurate representation of the linguistic information and of the domain-specific content of multilingual text corpora. T2K 2 encompasses two main sets of modules, respectively devoted to carry out the linguistic pre-processing of the acquisition corpus and to extract and organize the domain knowledge contained in the linguistically annotated texts. Each section of the considered handbook was automatically enriched (i.e. annotated) with linguistic information at increasingly complex levels of analysis, represented by sentence splitting, tokenization, Part-Of-Speech tagging and lemmatization. According to the methodology described in [2], the automatically POStagged and lemmatized input text is searched for candidate domain-specific terms denoting domain entities expressed by either single nominal terms (e.g. internet, network, software) or complex nominal structures with modifiers (typically, adjectival and prepositional modifiers), where the latter are retrieved on the basis of a set of POS patterns (e.g. adjective + noun, noun + preposition + noun) encoding morpho-syntactic templates for multi-word terms (e.g. Internet Protocol, eXtensible Markup Language, client/server model ). The domain relevance of both single and multi-word terms t included in the extracted list T is weighted on the basis of the C-NC Value [5] aimed at assessing how much a term is likely to be conceptually independent from the context in which it appears. Accordingly, a higher C-NC rank is assigned to those multi-word terms that are more relevant for the domain of the document collection in input. The extracted domain-specific entities are organized according to co-occurrence relations, i.e., relations between entities co-occurring within the same context. The relevance of relations is weighted using the log-likelihood metric for binomial distributions as defined by [4]. According to this metric, for example, the term Internet is strongly related with Internet Protocol addresses, Simple Mail Transfer protocol, message, etc. The extracted relations between terms can be visualized in a 'knowledge graph' which can be exploited in a number of graph analyses. M1 in the next section is based on the knowledge graph.","refs":[{"start":917,"end":920,"marker":"bibr","target":"#b1"},{"start":1611,"end":1614,"marker":"bibr","target":"#b4"},{"start":2148,"end":2151,"marker":"bibr","target":"#b3"}]}]},{"title":"Prerequisite Relationship Identification","paragraphs":[{"text":"In this paper we propose two methods for identifying candidate prerequisite relationships (t1, t2, p), with p ∈ [0, 1]. The underlying principles are: -Co-occurrence of two concepts is a necessary but not sufficient condition to identify the prerequisite relation. The principle can be extended from the sentence level to a section level.","refs":[]},{"text":"-Temporal occurrence of terms and/or sections are taken into account to identify the direction of prerequisite relation, with different granularities.","refs":[]},{"text":"Since the methods exploit these principles in different ways, they are designed to be finally combined in order to exploit the benefits of both the approaches.","refs":[]}]},{"title":"Method 1 (M1) is based on temporal order and co-occurrence of terms. Steps:","paragraphs":[{"text":"-Building a list L of terms t ∈ T ordered according to their temporal appearance in D where the term t has the first significant density (which can be compute with different methods, e.g. Burst Analysis). -Transforming the undirected knowledge graph from Sect. 3 generated with log-likelihood metric into a directed graph G 1 , where direction is derived from the ordered list of terms L.","refs":[]},{"text":"Result: Candidate triples for prerequisite relations are the adjacent terms in G 1 (Fig. 1). The G 1 graph is represented as a n x n matrix M 1 , with n = |T |. Each element t ij represents the weight p of the prerequisite relationship between terms t i and t j , with p = [0, 1]. The strength of relationship p can be defined using different approaches as: NLP analysis, Lexical pattern and other heuristics. The goal of this approach is to identify, for each term, the cluster of terms that are likely or unlikely to be in prerequisite relationship with the term. T OC(s i , s j ) represents the order ≺ of section i and section j, where s i , s j ∈ S. The application of the method is represented in the examples in Fig. 2. Steps:","refs":[{"start":89,"end":90,"marker":"figure","target":"#fig_0"},{"start":724,"end":725,"marker":"figure","target":"#fig_1"}]},{"text":"-For each term t ∈ T , identifying the section s i where the relevance function F (t, q) has max value (i.e., identifying the section where the term has the higher relevance in the document); the assumption is that a concept is explained where it has maximum relevance. -For each (t v , s i ), where v = u, identifying the section s j where the relevance function F (t v , s j ) has max value (i) If s j ≺ s i ∧ ∃ (t u , s j ), its unlikely that t u is a prerequisite of t v based on the principle that in s j there should be at least one occurrence of the prerequisite (t u ), see Fig. 2 ","refs":[{"start":587,"end":588,"marker":"figure","target":"#fig_1"}]},{"text":"explained before t u and it also co-occurs in s i , see Fig. 2 (ii). (iii) If s j ≺ s i ∧ ∃ (t u , s j ) there is some probability that t v is a prerequisite of t u , since they could be highly related concepts but not as prerequisite relationship. Similarly, if s i ≺ s j ∧ ∃ (t u , s j ) there is some probability that t u is a prerequisite of t v , for the same reason as in the previous point, see Fig. 2 (iii). (iv) If s i = s j , thus t v and t u co-occur with maximum relevance in the same section, see Fig. 2 (iv), this means that the concepts are highly related but we cannot identify the prerequisite relationship unless further analysis is performed, such as: NLP, Lexical pattern and other heuristics. ","refs":[{"start":61,"end":62,"marker":"figure","target":"#fig_1"},{"start":407,"end":408,"marker":"figure","target":"#fig_1"},{"start":515,"end":516,"marker":"figure","target":"#fig_1"}]}]},{"title":"Discussion and Conclusion","paragraphs":[{"text":"In this section we discuss the proposed approach by comparing our methods with related approaches for concept and prerequisite extraction. An approach that exploits textbook internal information (T OC) to identify prerequisite relations is adopted in [10], even though they also exploit external knowledge (from Wikipedia) to extract the relevant concepts. Another approach that exploits Wikipedia is described in [8]. The authors define a metric (i.e., refD) that models the relation by measuring how differently two concepts refer to each other.","refs":[{"start":251,"end":255,"marker":"bibr","target":"#b9"},{"start":414,"end":417,"marker":"bibr","target":"#b7"}]},{"text":"In [9] the authors mine prerequisite relations among MOOC course concepts by defining three main features: semantic (incorporates wikipedia knowledge), contextual (similar to refD [8]) and structural distributional patterns. Unlike the above cases, our approach exploits only features from the text (co-occurrence, term density, temporal and T OC ordering) for concept and prerequisite extraction, without using external knowledge. With respect to [10], while the authors exploit T OC title match and order coherence, we identify candidate prerequisite relation by the joint usage of not only T OC order (M2) but also the temporal concept density order (M1), thus providing a more granular method. Moreover, while in [10] the information overlap is calculated by using Wikipedia title match and similarity functions, we use concept-section order analysis (M2) to identify three specific cases of concept redundancy of which (ii) identifies prerequisite candidate conceptually similar to refD in [8] where the sections in our case can be seen as the wikipedia articles in refD. Whereas most of the aforementioned methods for prerequisite extraction result in a concept hierarchy building, i.e. tree structure, the M2 (iii) give the bases towards a graph building by adding parallel prerequisite relations.","refs":[{"start":3,"end":6,"marker":"bibr","target":"#b8"},{"start":180,"end":183,"marker":"bibr","target":"#b7"},{"start":448,"end":452,"marker":"bibr","target":"#b9"},{"start":717,"end":721,"marker":"bibr","target":"#b9"},{"start":995,"end":998,"marker":"bibr","target":"#b7"}]},{"text":"Enhancement of M1 can be made by introducing metrics based on concept bursting intervals (e.g. [11]) for building the list L. In addition, by analyzing more than one book (with the same subject), both methods can be improved by reducing biases due to the author's subjective choices in structuring the book. We are working on testing the methods and the mentioned enhancements.","refs":[{"start":95,"end":99,"marker":"bibr","target":"#b10"}]}]}],"tables":{},"abstract":{"title":"Abstract","paragraphs":[{"text":"Digital Libraries present tremendous potential for developing e-learning applications, such as text comprehension and questionanswering tools. A way to build this kind of tools is structuring the digital content into relevant concepts and dependency relations among them. While the literature offers several approaches for the former, the identification of dependencies, and specifically of prerequisite relations, is still an open issue. We present an approach to manage this task.","refs":[]}]}}