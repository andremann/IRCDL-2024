{"bibliography":{"title":"Grimm in Wonderland: Prompt Engineering with Midjourney to Illustrate Fairytales","authors":[{"person_name":{"surname":"Ruskov","first_name":"Martin"},"affiliations":[{"department":null,"institution":"University of Milan","laboratory":null}],"email":"martin.ruskov@unimi.it"}],"date":null,"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"keywords":["Fairytales","Image generation","Prompt engineering","Action research"],"citations":{"b0":{"title":"A Computational History Approach to Interpretation and Analysis of Moral European Values: the VAST Research Project","authors":[{"person_name":{"surname":"Castano","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Ferrara","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Giannini","first_name":"G"},"affiliations":[],"email":null},{"person_name":{"surname":"Montanelli","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Periti","first_name":"F"},"affiliations":[],"email":null}],"date":{"year":"2021","month":null,"day":null},"ids":null,"target":"https://ceur-ws.org/Vol-2981/paper7.pdf","publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":1,"to_page":9}}},"b1":{"title":"Detecting the semantic shift of values in cultural heritage document collections (short paper)","authors":[{"person_name":{"surname":"Ferrara","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Montanelli","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Ruskov","first_name":"M"},"affiliations":[],"email":null}],"date":{"year":"2022","month":null,"day":null},"ids":null,"target":"https://ceur-ws.org/Vol-3286/04_paper.pdf","publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":35,"to_page":43}}},"b2":{"title":"Design guidelines for prompt engineering text-to-image generative models","authors":[{"person_name":{"surname":"Liu","first_name":"V"},"affiliations":[],"email":null},{"person_name":{"surname":"Chilton","first_name":"L"},"affiliations":[],"email":null}],"date":{"year":"2022","month":null,"day":null},"ids":{"DOI":"10.1145/3491102.3501825","arXiv":null},"target":null,"publisher":"Association for Computing Machinery","journal":null,"series":null,"scope":null},"b3":{"title":"A taxonomy of prompt modifiers for text-to-image generation","authors":[{"person_name":{"surname":"Oppenlaender","first_name":"J"},"affiliations":[],"email":null}],"date":{"year":"2022","month":null,"day":null},"ids":{"DOI":"10.48550/ARXIV.2204.13988","arXiv":null},"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b4":{"title":"Action Research in Software Engineering: Theory and Applications","authors":[{"person_name":{"surname":"Staron","first_name":"M"},"affiliations":[],"email":null}],"date":{"year":"2020","month":null,"day":null},"ids":{"DOI":"10.1007/978-3-030-32610-4","arXiv":null},"target":null,"publisher":"Springer International Publishing","journal":null,"series":null,"scope":null},"b5":{"title":"Efficient diffusion models for vision: A survey","authors":[{"person_name":{"surname":"Ulhaq","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Akhtar","first_name":"N"},"affiliations":[],"email":null},{"person_name":{"surname":"Pogrebna","first_name":"G"},"affiliations":[],"email":null}],"date":{"year":"2022","month":null,"day":null},"ids":{"DOI":"10.48550/ARXIV.2210.09292","arXiv":null},"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b6":{"title":"Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation","authors":[{"person_name":{"surname":"Ruiz","first_name":"N"},"affiliations":[],"email":null},{"person_name":{"surname":"Li","first_name":"Y"},"affiliations":[],"email":null},{"person_name":{"surname":"Jampani","first_name":"V"},"affiliations":[],"email":null},{"person_name":{"surname":"Pritch","first_name":"Y"},"affiliations":[],"email":null},{"person_name":{"surname":"Rubinstein","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Aberman","first_name":"K"},"affiliations":[],"email":null}],"date":{"year":"2022","month":null,"day":null},"ids":{"DOI":"10.48550/ARXIV.2208.12242","arXiv":null},"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b7":{"title":"Scaling autoregressive models for content-rich text-to-image generation","authors":[{"person_name":{"surname":"Yu","first_name":"J"},"affiliations":[],"email":null},{"person_name":{"surname":"Xu","first_name":"Y"},"affiliations":[],"email":null},{"person_name":{"surname":"Koh","first_name":"J"},"affiliations":[],"email":null},{"person_name":{"surname":"Luong","first_name":"T"},"affiliations":[],"email":null},{"person_name":{"surname":"Baid","first_name":"G"},"affiliations":[],"email":null},{"person_name":{"surname":"Wang","first_name":"Z"},"affiliations":[],"email":null},{"person_name":{"surname":"Vasudevan","first_name":"V"},"affiliations":[],"email":null},{"person_name":{"surname":"Ku","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Yang","first_name":"Y"},"affiliations":[],"email":null},{"person_name":{"surname":"Ayan","first_name":"B"},"affiliations":[],"email":null},{"person_name":{"surname":"Hutchinson","first_name":"B"},"affiliations":[],"email":null},{"person_name":{"surname":"Han","first_name":"W"},"affiliations":[],"email":null},{"person_name":{"surname":"Parekh","first_name":"Z"},"affiliations":[],"email":null},{"person_name":{"surname":"Li","first_name":"X"},"affiliations":[],"email":null},{"person_name":{"surname":"Zhang","first_name":"H"},"affiliations":[],"email":null},{"person_name":{"surname":"Baldridge","first_name":"J"},"affiliations":[],"email":null},{"person_name":{"surname":"Wu","first_name":"Y"},"affiliations":[],"email":null}],"date":{"year":"2022","month":null,"day":null},"ids":{"DOI":"10.48550/ARXIV.2206.10789","arXiv":null},"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b8":{"title":"Midjourney v4 alpha-release announcement on Discord","authors":[{"person_name":{"surname":"Holz","first_name":"D"},"affiliations":[],"email":null}],"date":{"year":"2022","month":null,"day":null},"ids":null,"target":"https://discord.com/channels/662267976984297473/952771221915840552/1038335529747480607","publisher":null,"journal":null,"series":null,"scope":null},"b9":{"title":"Training-free structured diffusion guidance for compositional text-to-image synthesis","authors":[{"person_name":{"surname":"Feng","first_name":"W"},"affiliations":[],"email":null},{"person_name":{"surname":"He","first_name":"X"},"affiliations":[],"email":null},{"person_name":{"surname":"Fu","first_name":"T.-J"},"affiliations":[],"email":null},{"person_name":{"surname":"Jampani","first_name":"V"},"affiliations":[],"email":null},{"person_name":{"surname":"Akula","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Narayana","first_name":"P"},"affiliations":[],"email":null},{"person_name":{"surname":"Basu","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Wang","first_name":"X"},"affiliations":[],"email":null},{"person_name":{"surname":"Wang","first_name":"W"},"affiliations":[],"email":null}],"date":{"year":"2022","month":null,"day":null},"ids":{"DOI":"10.48550/ARXIV.2212.05032","arXiv":null},"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b10":{"title":"Steps towards prompt-based creation of virtual worlds","authors":[{"person_name":{"surname":"Roberts","first_name":"J"},"affiliations":[],"email":null},{"person_name":{"surname":"Banburski-Fahey","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Lanier","first_name":"J"},"affiliations":[],"email":null}],"date":{"year":"2022","month":null,"day":null},"ids":{"DOI":"10.48550/ARXIV.2211.05875","arXiv":null},"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b11":{"title":"Generated faces in the wild: Quantitative comparison of stable diffusion","authors":[{"person_name":{"surname":"Borji","first_name":"A"},"affiliations":[],"email":null}],"date":{"year":"2022","month":null,"day":null},"ids":{"DOI":"10.48550/ARXIV.2210.00586","arXiv":null},"target":null,"publisher":null,"journal":"midjourney","series":null,"scope":{"volume":2,"pages":null}},"b12":{"title":"Action Research and Reflective Practice","authors":[{"person_name":{"surname":"Mcintosh","first_name":"P"},"affiliations":[],"email":null}],"date":{"year":"2010","month":null,"day":null},"ids":{"DOI":"10.4324/9780203860113","arXiv":null},"target":null,"publisher":"Routledge","journal":null,"series":null,"scope":null},"b13":{"title":"Samples from Each Step For each of the examples in Table 1, a more detailed illustration of the process is included in Table 3 and 4, featuring examples of previous failing steps in the form of attempted text prompt, resulting images and relevant comments","authors":[],"date":null,"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null}},"sections":[{"title":"Introduction","paragraphs":[{"text":"Moral values inherent in literary heritage are not explicit and might be perceived differently over time. The project VAST (Values Across Space and Time) sets out to study such variations in perceptions [1]. One way to record contemporary perceptions of fairytales is to ask online users what values they are able to identify in text snippets of interest [2]. It is challenging to engage people's attention online, and accompanying these questions with illustrations is expected to help improve engagement for participation. However, pre-existing images are not always readily available for any snippet of interest, and it is impractical to commission ad-hoc illustrations for the purposes of a study where participating users are expected to be exposed to them only for a short period. This opens an opportunity to use a text-to-image generator as a tool to enrich snippets of classical texts for the purposes of improving questionnaire engagement and retention. In turn, this allows for computer-assisted multimedia representation of content that is originally text only, despite apparent limitations, discussed further in this paper.","refs":[{"start":203,"end":206,"marker":"bibr","target":"#b0"},{"start":355,"end":358,"marker":"bibr","target":"#b1"}]},{"text":"In particular, here we set ourselves the task of generating illustrations for fairytales by the Grimm brothers and investigate how accurate we can meet the expectations set by classical texts. While current research into prompt engineering for text-to-image generators focuses typically on construction of creative expressions [3,4], we are rather interested in a believable representation of well-known narratives. We engage in an iterative study in the tradition of action research [5] while systematically exploring the solution space of text-to-image generation. We set ourselves the exploratory goal to generate at least 5 believable illustrations for each of 5 fairytales and achieve this goal. This allows us to derive a process-based methodology towards constructing believable representations of preexisting text snippets. We consider our results satisfactory for our purposes of illustrating Grimm's fairytales. Yet, we observe that we have not reached a point where such illustration would be possible for any starting text snippet.","refs":[{"start":327,"end":330,"marker":"bibr","target":"#b2"},{"start":330,"end":332,"marker":"bibr","target":"#b3"},{"start":484,"end":487,"marker":"bibr","target":"#b4"}]}]},{"title":"Background","paragraphs":[{"text":"In a work on design guidelines for text-to-image prompts, Liu and Chilton, use the VQGAN+CLIP model and expertiment with 9 prompt templates [3]. These templates are phrases in natural language, constructed around up to four building blocks: i) subject, ii) verb, iii) medium, and iv) style. In choosing to include medium, they generalise an improvement suggestion by the authors of the generation model. Examples for media that Lui and Chilton provide include painting, photo, cartoon, icon, etc. Oppenlaender used ethnographic methods in their studies [4]. In the first part of their work, they engaged in an autoethnographic study using VQGAN+CLIP. However this was reported only as a sort of onboarding into the community of prompt engineering practitioners and not a case of reflective practice with its own learned lessons about the process, context, or specific circumstances beyond the conclusions from their second part -the study of other practitioners. In this second, ethnographic part, Oppenalender looks at practices developed in the emerging communities and arrives at a taxonomy of 6 types of prompt modifiers: i) subject terms, ii) style modifiers, iii) image prompts, iv) quality boosters, v) repetition, and vi) magic terms [4]. Image prompts (i.e. using images as part of the prompt), in particular, are one of the ways practitioners try to enforce character consistency across generations. Notably, the last three prompt modifiers in Oppenlaender's taxonomy are subjectively introduced by practitioners and -due to the randomness of the generationextremely difficult to validate.","refs":[{"start":140,"end":143,"marker":"bibr","target":"#b2"},{"start":553,"end":556,"marker":"bibr","target":"#b3"},{"start":1242,"end":1245,"marker":"bibr","target":"#b3"}]},{"text":"Whereas the above studies are based on GAN (Generative Adversarial Network), a more recent and well-performing technology is diffusion models [6]. For example, with the DreamBooth model, image-based fine-tuning has been successfully used to enforce character consistency between images [7]. Yet, even in more advanced models, some typical problems persist. In analysis of one of these -the Parti model -its authors list a number of identified typical recurring limitations. Notably, among these are hallucinations, failures with representing counts of similar objects and visual and linguistic priors -the emergence of stereotypes unrelated to the prompt context [8]. Midjourney is among the most popular models among practitioners, even though it is commercial and little is known about its architecture. The current release of Midjourney -version 4 -is declared to introduce handling of more complexity, in particular \"Vastly more knowledge (of creatures, places, and more)\", \"Much better at getting small details right\", \"Handles more complex prompting (with multiple levels of detail)\", \"Better with multiobject / multi-character scenes\" [9]. Our preliminary testing showed partial indications that it does deliver on these claims, allegedly on par with with most recent models like Parti [8] and Structured Diffusion Guidance [10]. However, the studies of prompt engineering listed above focus on a single model and do not give insights as to whether they are transferable across models. Partly due to the only recent advent of text-to-image generation models that are able to deliver meaningful outputs for complex inputs, systematic attempts at comparison across models are inconclusive [11,6]. Ideas of how this could be done can come from a related tasks: face generation with GANs, where quantitative comparisons have been made [12].","refs":[{"start":142,"end":145,"marker":"bibr","target":"#b5"},{"start":286,"end":289,"marker":"bibr","target":"#b6"},{"start":663,"end":666,"marker":"bibr","target":"#b7"},{"start":1142,"end":1145,"marker":"bibr","target":"#b8"},{"start":1293,"end":1296,"marker":"bibr","target":"#b7"},{"start":1331,"end":1335,"marker":"bibr","target":"#b9"},{"start":1694,"end":1698,"marker":"bibr","target":"#b10"},{"start":1698,"end":1700,"marker":"bibr","target":"#b5"},{"start":1838,"end":1842,"marker":"bibr","target":"#b11"}]},{"text":"Due to inherently complex processes, working with a black-box phenomena is very common in social, organisational and design sciences. As a consequence, a range of participatory methods like action research, reflective practice and design research [13,5] are used. Typical for these is that researchers engage in a project as practitioners. In iterative steps they not only develop a product, but also reflect on developing a theory about the task at hand. An intended consequence of this approach is that the emerging theory is contextualised in the specific settings of the project. More specific to action research, two types of learning outcomes are delivered: one intended to be used by practitioners and one by researchers [5]. For reasons of space, here we do not report on our implementation of the action research cycle itself, but focus on the applied resulting text illustration process.","refs":[{"start":247,"end":251,"marker":"bibr","target":"#b12"},{"start":251,"end":253,"marker":"bibr","target":"#b4"},{"start":728,"end":731,"marker":"bibr","target":"#b4"}]}]},{"title":"Method","paragraphs":[{"text":"Text-to-image generators share a perceived range of affordances. Not only they take text as input and produce an image, but also let themselves amend with input modifiers. Yet, it remains an open question whether discovered patterns in prompt engineering for one model might be transferable to another. We know that VQGAN+CLIP and Stable Diffusion have very different architectures, and know little of those of Dall-E and Midjourney. Thus, it would be a stretch to assume that the prompt engineering learned for one model would be informative for others.","refs":[]},{"text":"Instead, we propose that the process of model exploration is a reusable form of knowledge in line with action research. Considering that generation models are black boxes, the experimentation with prompts is much more a field study \"in the wild\" than a controlled experiment. Thus, we propose that an iterative action research approach could produce knowledge that is more directly transferable across models than phenomenological research into interacting factors.","refs":[]},{"text":"Having the very specific task of illustrating fairytales, we start from Oppenlaender's taxonomy [4]. We consider the modifier types of quality boosters and magic terms to be of little relevance for our task. We also consider repetition modifiers out of the scope of this paper. Thus, we focus on subject terms and style modifiers. Finally, we also take advantage of a feature of Midjourney that allows the creation of variants of a produced image. This can be seen as a special case of image prompts. Subject Due to our starting point being a pre-existing text and the claimed progress of Midjourney v4, our subject terms do not always fit the simple subjects defined e.g. by Liu and Chilton's permutations [3]. Rather we derive our subject prompts from the original texts and simplify and adapt them aiming to improve results. A natural first step in this process is to identify where in the original text an important character or moment is introduced. Then we simplify its textual description by trying, whenever possible, to fit it in a simple sentence. In the process we also substitute pronouns with as specific nouns as possible. Examples can be seen in Table 1.","refs":[{"start":96,"end":99,"marker":"bibr","target":"#b3"},{"start":707,"end":710,"marker":"bibr","target":"#b2"},{"start":1167,"end":1168,"marker":"table","target":null}]}]},{"title":"Style","paragraphs":[{"text":"We intend style modifiers as a combination of Liu and Chilton's medium and style. Although these might also not implicitly be necessary for the purposes of our task, we use them to restrict the text-to-image generator. Due to the hallucinations typical for such systems, we seek the possibility to force the generator not to introduce excessive detail which might sharply hit believability. For this purpose we experiment with style modifiers like simple book illustration or minimalistic illustration to restrict hallucinations and lead the generator towards the expectations for the genre medium.","refs":[]}]},{"title":"Image prompts","paragraphs":[{"text":"We consider image prompts in a very particular sense, since we do not use the actual possibility to provide a reference image. Instead, we take advantage of an image variation feature provided by Midjourney. Under the premises that it functions conceptually similarly to what an image prompt would be expected to do, even if it is expected to generate results that are much more similar to the reference image than what would be expected from an image prompt.","refs":[]},{"text":"Without using image-based fine-tuning, consistency across images is a challenge. in the case of fairytales, it commonly occurs that -across different generation calls -the same character is depicted with different features like hair or skin colour. However, for the purposes of this research, we intend to present to users one image at a time, so we do not tackle this issue. Fine-tuning along the lines of what is done in DreamBooth or actual image prompts, remains beyond the scope of this study. Short time of exposure of the produced images to intended users allows for small inconsistencies between snippet context and image, as long as these do not strongly undermine believability. For the purposes of this preliminary study, we limit ourselves to self-assessing believability. For the same reasons, we consider five successful image generations per fairytale to be a satisfactory result. Again, due to the typical model hallucinations, we do not engage in upscaling images (increasing resolution), because this inevitably results in further unwanted artefacts. Instead, whenever in future a higher image resolution is necessary, we intend to resort to conventional (basic) resampling techniques.","refs":[]}]},{"title":"Results","paragraphs":[{"text":"In our exploratory study, starting from snippets of text and incrementally refining, we have made more than 650 requests, generating more than 2600 images. Without claiming an efficient exploration, this allowed us to illustrate 5 fairytales with successful generations for at least 5 different snippets per fairytale. Examples of this outcome can be seen in Table 1 and samples from the steps, preceding these outcomes in Appendix A. The remaining successful generations are provided in Appendix B and the full generated results are available at the author's Midjourney profile 1 . From the experience made, we deduce the following tentative four-staged process:","refs":[{"start":365,"end":366,"marker":"table","target":null},{"start":579,"end":580,"marker":null,"target":"#foot_0"}]},{"text":"1. Initial prompt Start with a prompt closely representing the original text trying to summarise it -preserving vocabulary at this stage -into as close as possible to a simple sentence.","refs":[]}]},{"title":"Composition adjustment","paragraphs":[{"text":"Refine prompt step-wise opportunistically, preferring small changes that would allow a fast feedback loop for finer control over change. Pay particular attention to possible misinterpretation to ambiguous words. We identify the possibility to control these at any of three levels:","refs":[]},{"text":"• Adjusting words, optionally simplifying or replacing them with synonyms, ones that might represent the context better. This might include reducing phrasal verbs to one representing the action, sacrificing narrative richness and fidelity for precision of expression. • Add or remove adjectives for entities (subject and objects) or adverbs for verbs • Add objects to represent the context better and/or force removal of unnecessary artefacts.","refs":[]}]},{"title":"Style refinement","paragraphs":[{"text":"Whenever superfluous hallucination of the generator is perceived, it could be suppressed by enforcing a style (in the case of fairytales we propose illustration) with modifiers along the lines of basic, simple, minimal, flatcolor.","refs":[]}]},{"title":"Variation selection","paragraphs":[{"text":"Once the desired composition is reached, work with the generation of variants, whenever the generator allows it, as the case of diffusion models like Midjourney. This might be tried also in cases where the composition is only \"nearly reached\". For example, when certain number of objects of a type are needed, but only an approximate count is reached, this step could be attempted in the hope that hallucinations accidentally adjust the count.","refs":[]},{"text":"In our investigation, this produced successful results, a sample of which is provided in Table 1. Even though we indicatively name our steps to describe their primary objective, the corresponding elements are not exclusively elaborated in that step. Rather, one should have low expectations of subsequent steps if the objective of previous steps was not approached to a satisfactory degree. Practitioners are invited to navigate the process freely according to their preferences. On one hand this means that we invite everyone to interrupt it at any step, should the result be considered satisfactory. On the other, we suggest moving back and forth in the process, or even jump steps whenever practitioners see fit.","refs":[{"start":95,"end":96,"marker":"table","target":null}]},{"text":"However, as the samples in Table 2 exemplify, the generation of images for other snippets was extremely challenging to produce and we were not successful in doing it. We hypothesise to have identified three particular reasons: difficulties with counts, inability to get distanced from stereotypical configurations and non-conventional situations. These are in line with limitations reported in the Parti model [8]. We subject these hypotheses to simple accessible tests.","refs":[{"start":33,"end":34,"marker":"table","target":"#tab_0"},{"start":410,"end":413,"marker":"bibr","target":"#b7"}]}]},{"title":"Original Text","paragraphs":[]},{"title":"Complete Prompt","paragraphs":[{"text":"Stage Image a little cap made of red velvet. Because it suited her so well, and she wanted to wear it all the time, she came to be known as Little Red Riding Hood the little cap made of red velvet suited the little girl so well, she came to be known as Little Red Riding Hood 1 After the full moon had come up... They followed the pebbles that glistened there like newly minted coins, showing them the way medieval boy and girl follow trace of pebbles in the woods 2","refs":[]},{"text":"The prince approached her, took her by the hand, and danced with her","refs":[]},{"text":"The Prince dances with Cinderella, basic book illustration 3 faithful Johannes, who was sitting at the front of the ship... saw three ravens flying through the air towards them three ravens flying by a frigate in open sea, simple book illustration 4","refs":[]}]},{"title":"Table 1","paragraphs":[{"text":"Samples of successful image generations for different fairytales. These represent different stages to show that sometimes satisfactory results can be reached early in the process. Samples from failing steps preceding these successes are shown in Appendix A","refs":[]},{"text":"There first reason we identify is the difficulty to cause the model to generate a specific number of similar objects. In certain cases this might not be critical. With repeated attempts it is possible to strike three ravens, or it might not be critical if the illustrated dwarfs are five or six, instead of seven. However, a well-known issue among practitioners is the difficulty to draw e.g. hands, often getting a wrong number of fingers.","refs":[]},{"text":"The second hypothesis is a presumed difficulty to generate scenes, different from a dominant stereotypical view. In previous literature this is typically associated to priors [8] and although in general this could be perceived as an advantage in our task, there are cases when it is undesirable. An examples can be seen in the first row of Table 2. It appears to be impossible to force the creation of a grave without a pre-existing tree on it. Our current hypothesis is that the model \"knows\" that the grave of Cinderella's mother has a tree on it, as this tree plays an important role further in the story. This hypothesis is put to question by the fact that even when the references of \"Cinderella\" and \"mother's\" are removed, the model continues to produce a tree. We also evaluate how another popular diffusion model behaves on this input. This particular issue was present, but not as persistent when generating with DALL-E.","refs":[{"start":175,"end":178,"marker":"bibr","target":"#b7"},{"start":346,"end":347,"marker":"table","target":"#tab_0"}]},{"text":"Examples for failure in representing non-conventional situations could be the extremely poor results for prompts derived from non-realistic texts (also referred to as impossible scenes [8]), such as examples 2 and 3 in Table 2. We hypothesise that this is due to the nature of training based on pre-existing image datasets where anything similar -albeit possibly present in the  dataset -would be an ignored outlier.","refs":[{"start":185,"end":188,"marker":"bibr","target":"#b7"},{"start":225,"end":226,"marker":"table","target":"#tab_0"}]}]},{"title":"Future Work","paragraphs":[{"text":"While our tentative four-staged process was developed and tested with Midjourney v4, we have kept it generic enough to be applicable also to other current generation models andmost importantly -future ones to come. This last point is key, because current state-of-the-art models are just arriving at being able to handle a level of complexity required to illustrate an existing text [10,8]. We also have indicated three hypothesised issues for text-to-image generation models, each of which could serve as a challenge for researchers and developers. We claim that an approach starting from intentions, related to a pre-exisitng text, helps shed light onto possible interpretations relevant to model limitations.","refs":[{"start":383,"end":387,"marker":"bibr","target":"#b9"},{"start":387,"end":389,"marker":"bibr","target":"#b7"}]},{"text":"In a subsequent iteration of this action research effort, the domain of studied texts could be further expanded and the exploratory success threshold (the goal) could be increased. While it was not clear whether this would be possible at the start of the study, now we have sufficient confidence to believe that it might be achievable.","refs":[]},{"text":"As stated by the rationale of this paper, a next step of this research is to perform an usability study with end users to investigate whether generated images actually improve user engagement when responding to online questions about values in fairytales. This study should also include questions about image believability. Whereas we have tried to limit any bias that the generator might introduce into images, the absence of such bias also needs to be validated. This can be done by comparing responses of end users that are exposed to the generated illustrations with ones that are not. Finally, we would like to identify metrics that would allow us to measure if user participation corresponds to image quality and believability.","refs":[]}]}],"tables":{"tab_0":{"heading":"Table 2","description":"Samples of noticeable failure for image generations, including with alternative generator models. Notice that styles that work well with Midjourney lead to oversimplificated results with DALL-E.","rows":[]}},"abstract":{"title":"Abstract","paragraphs":[{"text":"The quality of text-to-image generation is continuously improving, yet the boundaries of its applicability are still unclear. In particular, refinement of the text input with the objective of achieving better resultscommonly called prompt engineering -so far seems to have not been geared towards work with preexisting texts. We investigate whether text-to-image generation and prompt engineering could be used to generate basic illustrations of popular fairytales. Using Midjourney v4, we engage in action research with a dual aim: to attempt to generate 5 believable illustrations for each of 5 popular fairytales, and to define a prompt engineering process that starts from a pre-existing text and arrives at an illustration of it. We arrive at a tentative 4-stage process: i) initial prompt, ii) composition adjustment, iii) style refinement, and iv) variation selection. We also discuss three reasons why the generation model struggles with certain illustrations: difficulties with counts, bias from stereotypical configurations and inability to depict overly fantastic situations. Our findings are not limited to the specific generation model and are intended to be generalisable to future ones.","refs":[]}]}}