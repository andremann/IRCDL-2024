{"bibliography":{"title":"Reproducibility of the Neural Vector Space Model via Docker","authors":[{"person_name":{"surname":"Ferro","first_name":"Nicola"},"affiliations":[{"department":"Department of Information Engineering","institution":"University of Padua","laboratory":null}],"email":"nicola.ferro@unipd.it"},{"person_name":{"surname":"Marchesin","first_name":"Stefano"},"affiliations":[{"department":"Department of Information Engineering","institution":"University of Padua","laboratory":null}],"email":"stefano.marchesin@unipd.it"},{"person_name":{"surname":"Purpura","first_name":"Alberto"},"affiliations":[{"department":"Department of Information Engineering","institution":"University of Padua","laboratory":null}],"email":"alberto.purpura@unipd.it"},{"person_name":{"surname":"Silvello","first_name":"Gianmaria"},"affiliations":[{"department":"Department of Information Engineering","institution":"University of Padua","laboratory":null}],"email":"gianmaria.silvello@unipd.it"}],"date":null,"ids":{"DOI":"10.1007/978-3-030-39905-4_1","arXiv":null},"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"keywords":[],"citations":{"b0":{"title":"Reproducing a neural question answering architecture applied to the SQuAD benchmark dataset: challenges and lessons learned","authors":[{"person_name":{"surname":"Dür","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Rauber","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Filzmoser","first_name":"P"},"affiliations":[],"email":null}],"date":{"year":"2018","month":null,"day":null},"ids":{"DOI":"10.1007/978-3-319-76941-7_8","arXiv":null},"target":"https://doi.org/10.1007/978-3-319-76941-7","publisher":"Springer","journal":null,"series":null,"scope":{"volume":10772,"pages":{"from_page":102,"to_page":113}}},"b1":{"title":"Overview of CEN-TRE@CLEF 2019: sequel in the systematic reproducibility realm","authors":[{"person_name":{"surname":"Ferro","first_name":"N"},"affiliations":[],"email":null},{"person_name":{"surname":"Fuhr","first_name":"N"},"affiliations":[],"email":null},{"person_name":{"surname":"Maistro","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Sakai","first_name":"T"},"affiliations":[],"email":null},{"person_name":{"surname":"Soboroff","first_name":"I"},"affiliations":[],"email":null}],"date":{"year":"2019","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b2":{"title":"A docker-based replicability study of a neural information retrieval model","authors":[{"person_name":{"surname":"Ferro","first_name":"N"},"affiliations":[],"email":null},{"person_name":{"surname":"Marchesin","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Purpura","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Silvello","first_name":"G"},"affiliations":[],"email":null}],"date":{"year":"2019","month":null,"day":null},"ids":null,"target":"http://ceur-ws.org/Vol-2409/docker05.pdf","publisher":null,"journal":null,"series":null,"scope":{"volume":2409,"pages":{"from_page":37,"to_page":43}}},"b3":{"title":"Reproducibility of data-oriented experiments in e-science","authors":[{"person_name":{"surname":"Freire","first_name":"J"},"affiliations":[],"email":null},{"person_name":{"surname":"Fuhr","first_name":"N"},"affiliations":[],"email":null},{"person_name":{"surname":"Rauber","first_name":"A"},"affiliations":[],"email":null}],"date":{"year":"2016","month":null,"day":null},"ids":{"DOI":"10.4230/DagRep.6.1.108","arXiv":null},"target":"http://drops.dagstuhl.de/opus/volltexte/2016/5817","publisher":null,"journal":null,"series":null,"scope":{"volume":6,"pages":{"from_page":108,"to_page":159}}},"b4":{"title":"Focal elements of neural information retrieval models. an outlook through a reproducibility Study","authors":[{"person_name":{"surname":"Marchesin","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Purpura","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Silvello","first_name":"G"},"affiliations":[],"email":null}],"date":{"year":"2019","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"Inf. Process. Manag","series":null,"scope":{"volume":34,"pages":null}},"b5":{"title":"A neural vector space model implementation repository","authors":[{"person_name":{"surname":"Marchesin","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Purpura","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Silvello","first_name":"G"},"affiliations":[],"email":null}],"date":{"year":"2019","month":null,"day":null},"ids":null,"target":"https://github.com/giansilv/NeuralIR/","publisher":null,"journal":null,"series":null,"scope":null},"b6":{"title":"Overview of the NTCIR-14 CENTRE task","authors":[{"person_name":{"surname":"Sakai","first_name":"T"},"affiliations":[],"email":null},{"person_name":{"surname":"Ferro","first_name":"N"},"affiliations":[],"email":null},{"person_name":{"surname":"Soboroff","first_name":"I"},"affiliations":[],"email":null},{"person_name":{"surname":"Zeng","first_name":"Z"},"affiliations":[],"email":null},{"person_name":{"surname":"Xiao","first_name":"P"},"affiliations":[],"email":null},{"person_name":{"surname":"Maistro","first_name":"M"},"affiliations":[],"email":null}],"date":{"year":"2019","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b7":{"title":"Overview of the TREC 2018 CENTRE track","authors":[{"person_name":{"surname":"Soboroff","first_name":"I"},"affiliations":[],"email":null},{"person_name":{"surname":"Ferro","first_name":"N"},"affiliations":[],"email":null},{"person_name":{"surname":"Sakai","first_name":"T"},"affiliations":[],"email":null}],"date":{"year":"2018","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b8":{"title":"Neural vector spaces for unsupervised information retrieval","authors":[{"person_name":{"surname":"Van Gysel","first_name":"C"},"affiliations":[],"email":null},{"person_name":{"surname":"De Rijke","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Kanoulas","first_name":"E"},"affiliations":[],"email":null}],"date":{"year":"2018","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"ACM Trans. Inf. Syst","series":null,"scope":{"volume":36,"pages":{"from_page":1,"to_page":38}}}},"sections":[{"title":"Introduction","paragraphs":[{"text":"Reproducibility of models and systems is central for the verification of scientific results ans it is one of the cornerstones of the system of sciences. In the field of Information Retrieval, reproducibility has been the object of thorough analyses and efforts; only in the last two years we have seen the rise of many reproducibility-oriented events like the CENTRE evaluations at CLEF [2], NTCIR [7] and TREC [8], the SIGIR task force to implement ACM's policy on artifact review and badging and the Open-Source IR Replicability Challenge at SIGIR 2019 (OSIRRC 2019).","refs":[{"start":398,"end":401,"marker":"bibr","target":"#b6"},{"start":411,"end":414,"marker":"bibr","target":"#b7"},{"start":555,"end":568,"marker":"bibr","target":null}]},{"text":"In this paper, we advocate OSIRRC 2019s vision that is to build Dockerbased 1 infrastructures to replicate results on standard IR ad hoc test collections. Docker is a tool that allows for the creation and deployment of applications via images containing all the required dependencies. Relying on a Docker-based infrastructure to replicate the results of existing systems, helps researchers to avoid all the issues related to system requirements and dependencies. Indeed, Information Retrieval (IR) platforms such as Anserini, Terrier, or text matching libraries such as MatchZoo rely on a set of software tools, developed in Java or Python and based on numerous libraries for scientific computing, which have all to be available on the host machine in order for the applications to run smoothly.","refs":[]},{"text":"We explore the use of Docker images for the reproducibility of Neural IR (NeuIR) models, which is a challenging domain that has seen only a few reproducibility efforts so far [1,5]. NeuIR models are particularly hard to reproduce because they are highly sensitive to parameters, hyper-parameters, and preprocessing choices. Also, these models are usually compatible only with specific versions of the libraries that they rely on (e.g., Tensorflow) because these frameworks are constantly updated. The use of Docker images is a possible solution to avoid these deployment issues on different machines as it already includes all the libraries required by the contained application.","refs":[{"start":175,"end":178,"marker":"bibr","target":"#b0"},{"start":178,"end":180,"marker":"bibr","target":"#b4"}]},{"text":"For this reason, (i) we propose a Docker architecture that can be used as a framework to train, test, and evaluate NeuIR models; and, (ii) we show how this architecture can be employed to build a Docker image that replicates the Neural Vector Space Model (NVSM) [9], a state-of-the-art unsupervised neural model for ad hoc retrieval.","refs":[{"start":262,"end":265,"marker":"bibr","target":"#b8"}]}]},{"title":"Background","paragraphs":[{"text":"Repeatability, replicability, and reproducibility are fundamental aspects of computational sciences, both in supporting desirable scientific methodology as well as sustaining empirical progress. These concepts have been discussed and analyzed in depth in [4], that focuses on the core issues and approaches to reproducibility in several fields of computer science. Relevant to these concepts is the Platform, Research goal, Implementation, Method, Actor and Data (PRI-MAD) model, which tackles reproducibility from different angles. The PRIMAD paradigm has been adopted by the IR community, where it has been adapted to the context of IR evaluation -both system-oriented and user-oriented. In this context, our contribution lies between replicability and reproducibility. Indeed, we rely on the NVSM implementation available at [6] and described in [5] to replicate the results of a reproduced version of NVSM.","refs":[{"start":255,"end":258,"marker":"bibr","target":"#b3"},{"start":828,"end":831,"marker":"bibr","target":"#b5"},{"start":849,"end":852,"marker":"bibr","target":"#b4"}]},{"text":"NVSM is a state-of-the-art unsupervised model for ad hoc retrieval. The model achieves competitive results against traditional lexical models and outperforms state-of-the-art unsupervised semantic retrieval models, like the Word2Vec-based models. NVSM jointly learns distinct word and document representations by optimizing an unsupervised loss function which minimizes the distance between sequences of n-grams and the documents containing them. Such optimization objective imposes that n-grams extracted from a document should be predictive of that document. After training, the learned word and document representations are used to perform retrieval. Queries are seen as n-grams and matched against documents in the feature space. Documents are then ranked in decreasing order of the cosine similarity computed between query and document representations.","refs":[]}]},{"title":"Docker Image Architecture","paragraphs":[{"text":"We developed two Docker images reproducing NVSM, one CPU-based and another GPU-based.","refs":[]},{"text":"The CPU-based version of NVSM is written in Python and relies on Tensorflow v.1.13.1. For this reason, we developed a Docker image based on the official Python 3.5 runtime container, on top of which we install the Python packages required by the algorithm -such as Tensorflow, Python NLTK, and Whoosh -we also install a C compiler, i.e. gcc, in order to use the official trec eval package2 to evaluate the retrieval model during training. Since this docker image still relies for some functions (i.e. random number generation) on the host machine, despite being very similar, the results are not exactly the same across different computers -while they are consistent on the same machine.","refs":[{"start":388,"end":389,"marker":null,"target":"#foot_0"}]},{"text":"The GPU-based version of NVSM is based on Tensorflow, which is a machine learning library that allows us to employ the GPU on the host machine in order to perform operations more efficiently. There are many advantages of employing GPUs for scientific computations, but their usage makes a sizable difference especially when training deep learning models. The training of such models requires in fact to perform a large number of matrix operations that can be easily parallelized and do not require powerful hardware.","refs":[]},{"text":"In our experiments, we observed that nvsm gpu does not produce fully consistent results on the same machine. In fact, TensorFlow uses the Eigen library, which in turn uses CUDA atomic functions to implement reduction operations, such as tf.reduce sum etc. Those operations are non-deterministic and each operation can introduce small variations. Despite this problem, we still believe that the advantages brought by the usage of a GPU in terms of reduction of computational time -combined with the fact that we detected only very small variations in the Mean Average Precision at Rank 1000 (MAP), Normalized Discounted Cumulative Gain at Rank 100 (nDCG@100), Precision at Rank 10 (P@10), and Recall -make this implementation of the algorithm a valid alternative to the CPU-based one.","refs":[]}]},{"title":"Evaluation","paragraphs":[{"text":"To test our docker image we consider the Robust04 collection, which is composed of TIPSTER corpus Disk 4&5 minus CR. The collection counts 528,155 documents, with a vocabulary of 760,467 different words. The topics considered for the evaluation are topics 301-450, 601-700 from Robust04. Only the field title of topics is used for retrieval. The set of topics is split into validation (V) and test (T) sets. Relevance judgments are restricted accordingly. The execution times and memory occupation statistics were computed on an 2018 Alienware Area-51 with an Intel Core i9-7980XE CPU @ 2.60 GHz with 36 cores, 64 GB of RAM and two GeForce GTX 1080Ti GPUs.","refs":[]},{"text":"To train the NVSM model, we set the following parameters and hyperparameters: word representation size k w = 300, number of negative examples z = 10, learning rate α = 0.001, regularization lambda λ = 0.01, batch size m = 51200, dimensionality of the document representations k d = 256 and n-gram size n = 16. We train the model for 15 iterations over the document collection and we select the model iteration that performs best in terms of MAP. each other, since the Kendall's τ values are all close to 0. In other words, when considering the top 100 results in each run, the same documents are rarely in the same positions in the selected rankings. This result, combined with the fact that the runs achieve all similar MAP, nDCG@100, P@10, and Recall values, leads to the conclusion that the relevant documents are high in the rankings, but are not in the same positions. In other words, NVSM performs a permutation of the documents in the runs, maintaining however the relative order between relevant and non-relevant documents. ","refs":[]}]},{"title":"Final Remarks","paragraphs":[{"text":"In this work, we performed a replicability study of the Neural Vector Space Model (NVSM) retrieval model using Docker. First, we presented the architecture and the main functions of a Docker image designed for the replicability of Neural IR (NeuIR) models. Secondly, we described the image components and the engineering challenges to obtain deterministic results with Docker using popular machine learning libraries such as Tensorflow. We also share two Docker images of the NVSM model: the first, which relies only on the CPU of the host machine to perform its operations, the second, which is able to also exploit the GPU of the host machine, when available. We observed some differences between the runs computed by the nvsm cpu Docker images on different machines and between the runs computed by the nvsm cpu and nvsm gpu Docker images on the same machine. The differences between nvsm cpu images on different machines are related to the nondeterminism of the results, as Docker relies on the host machine for some basic operations which influence the model optimization process through the generation of different pseudo-random number sequences. On the other hand, the differences between nvsm gpu images on the same machine are due to the implementation of some functions in the CUDA and Tensorflow libraries. We observed that these operations influence in a sizeable way the ordering of the same documents across different runs, but not the overall distribution of relevant and non-relevant documents in the ranking. Similar differences, that are even more accentuated, can be found between nvsm cpu and nvsm gpu images on the same machine. Therefore, even though these differences may seem marginal in offline evaluation settings, where the focus is on average performance, they are extremely relevant for user-oriented online settings -as they can have a sizeable impact on the user experience and should thus be taken into consideration when deciding whether to use NeuIR models in real-world scenarios.","refs":[]}]}],"tables":{"tab_0":{"heading":"Table 2 .","description":"RMSE between the NVSM CPU Docker image and the average of the 3 runs computed with the NVSM GPU Docker image.","rows":[["","NVSM GPU (average)"],["RMSE (MAP)","0.034"],["RMSE (nDCG@100) 0.054"],["RMSE (P@10)","0.140"]]},"tab_1":{"heading":"Table 3 .","description":"Kendall's τ correlation coefficient values between the NVSM GPU and NVSM CPU runs.","rows":[["","GPU (run 0) GPU (run 1) GPU (run 2) CPU"],["GPU (run 0) 1.0","0.025","0.025","0.018"],["GPU (run 1) 0.025","1.0","0.089","0.014"],["GPU (run 2) 0.025","0.089","1.0","0.009"],["CPU","0.018","0.014","0.009","1.0"]]}},"abstract":{"title":"Abstract","paragraphs":[{"text":"In this work we describe how Docker images can be used to enhance the reproducibility of Neural IR models. We report our results reproducing the Vector Space Neural Model (NVSM) and we release a CPU-based and a GPU-based Docker image. Finally, we present some insights about reproducing Neural IR models.","refs":[]}]}}