{"bibliography":{"title":"Supporting Tabular Data Characterization in a Large Scale Data Infrastructure by Lexical Matching Techniques","authors":[{"person_name":{"surname":"Candela","first_name":"Leonardo"},"affiliations":[{"department":"Istituto di Scienza e Tecnologie dell'Informazione \"Alessandro Faedo","institution":"Consiglio Nazionale delle Ricerche Via G. Moruzzi","laboratory":null}],"email":"candela@isti.cnr.it"},{"person_name":{"surname":"Coro","first_name":"Gianpaolo"},"affiliations":[{"department":"Istituto di Scienza e Tecnologie dell'Informazione \"Alessandro Faedo","institution":"Consiglio Nazionale delle Ricerche Via G. Moruzzi","laboratory":null}],"email":"coro@isti.cnr.it"},{"person_name":{"surname":"Pagano","first_name":"Pasquale"},"affiliations":[{"department":"Istituto di Scienza e Tecnologie dell'Informazione \"Alessandro Faedo","institution":"Consiglio Nazionale delle Ricerche Via G. Moruzzi","laboratory":null}],"email":"pagano@isti.cnr.it"}],"date":null,"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"keywords":["Large-scale data infrastructure","Lexical similarity","Tabular data management","Data curation"],"citations":{"b0":{"title":"Linked data -the story so far","authors":[{"person_name":{"surname":"Bizer","first_name":"C"},"affiliations":[],"email":null},{"person_name":{"surname":"Heath","first_name":"T"},"affiliations":[],"email":null},{"person_name":{"surname":"Berners-Lee","first_name":"T"},"affiliations":[],"email":null}],"date":{"year":"2009","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"International Journal on Semantic Web & Information Systems","series":null,"scope":{"volume":5,"pages":{"from_page":1,"to_page":22}}},"b1":{"title":"Deploying generalpurpose virtual research environments for humanities research","authors":[{"person_name":{"surname":"Blanke","first_name":"T"},"affiliations":[],"email":null},{"person_name":{"surname":"Candela","first_name":"L"},"affiliations":[],"email":null},{"person_name":{"surname":"Hedges","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Priddy","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Simeoni","first_name":"F"},"affiliations":[],"email":null}],"date":{"year":"2010","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"Philosophical Transactions of the Royal Society A","series":null,"scope":{"volume":368,"pages":{"from_page":3813,"to_page":3828}}},"b2":{"title":"Research data: Who will share what, with whom, when, and why?","authors":[{"person_name":{"surname":"Borgman","first_name":"C"},"affiliations":[],"email":null}],"date":{"year":"2010","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b3":{"title":"The Conundrum of Sharing Research Data","authors":[{"person_name":{"surname":"Borgman","first_name":"C"},"affiliations":[],"email":null}],"date":{"year":"2011","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"Journal of the American Society for Information Science and Technology","series":null,"scope":{"volume":null,"pages":{"from_page":1,"to_page":40}}},"b4":{"title":"DILI-GENT: integrating Digital Library and Grid Technologies for a new Earth Observation Research Infrastructure","authors":[{"person_name":{"surname":"Candela","first_name":"L"},"affiliations":[],"email":null},{"person_name":{"surname":"Akal","first_name":"F"},"affiliations":[],"email":null},{"person_name":{"surname":"Avancini","first_name":"H"},"affiliations":[],"email":null},{"person_name":{"surname":"Castelli","first_name":"D"},"affiliations":[],"email":null},{"person_name":{"surname":"Fusco","first_name":"L"},"affiliations":[],"email":null},{"person_name":{"surname":"Guidetti","first_name":"V"},"affiliations":[],"email":null},{"person_name":{"surname":"Langguth","first_name":"C"},"affiliations":[],"email":null},{"person_name":{"surname":"Manzi","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Pagano","first_name":"P"},"affiliations":[],"email":null},{"person_name":{"surname":"Schuldt","first_name":"H"},"affiliations":[],"email":null},{"person_name":{"surname":"Simi","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Springmann","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Voicu","first_name":"L"},"affiliations":[],"email":null}],"date":{"year":"2007","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"International Journal on Digital Libraries","series":null,"scope":{"volume":7,"pages":{"from_page":59,"to_page":80}}},"b5":{"title":"History, Evolution and Impact of Digital Libraries","authors":[{"person_name":{"surname":"Candela","first_name":"L"},"affiliations":[],"email":null},{"person_name":{"surname":"Castelli","first_name":"D"},"affiliations":[],"email":null},{"person_name":{"surname":"Pagano","first_name":"P"},"affiliations":[],"email":null}],"date":{"year":"2011","month":null,"day":null},"ids":null,"target":null,"publisher":"IGI Global","journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":1,"to_page":30}}},"b6":{"title":"From Heterogeneous Information Spaces to Virtual Documents","authors":[{"person_name":{"surname":"Candela","first_name":"L"},"affiliations":[],"email":null},{"person_name":{"surname":"Castelli","first_name":"D"},"affiliations":[],"email":null},{"person_name":{"surname":"Pagano","first_name":"P"},"affiliations":[],"email":null},{"person_name":{"surname":"Simi","first_name":"M"},"affiliations":[],"email":null}],"date":{"year":"2005","month":null,"day":null},"ids":null,"target":null,"publisher":"Springer","journal":null,"series":null,"scope":{"volume":3815,"pages":{"from_page":11,"to_page":22}}},"b7":{"title":"D4Science-II -An e-Infrastructure Ecosystem for Science","authors":[{"person_name":{"surname":"Castelli","first_name":"D"},"affiliations":[],"email":null}],"date":{"year":"2009","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"ERCIM News","series":null,"scope":{"volume":79,"pages":{"from_page":9,"to_page":9}}},"b8":{"title":"eScience and the humanities","authors":[{"person_name":{"surname":"Crane","first_name":"G"},"affiliations":[],"email":null},{"person_name":{"surname":"Babeu","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Bamman","first_name":"D"},"affiliations":[],"email":null}],"date":{"year":"2007","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"International Journal on Digital Libraries","series":null,"scope":{"volume":7,"pages":{"from_page":117,"to_page":122}}},"b9":{"title":"SHARE: a web portal for creating and sharing executable research papers","authors":[{"person_name":{"surname":"Gorp","first_name":"P"},"affiliations":[],"email":null},{"person_name":{"surname":"Mazanek","first_name":"S"},"affiliations":[],"email":null}],"date":{"year":"2011","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"Procedia CS","series":null,"scope":{"volume":4,"pages":{"from_page":589,"to_page":597}}},"b10":{"title":"Error detecting and error correcting codes","authors":[{"person_name":{"surname":"Hamming","first_name":"R"},"affiliations":[],"email":null}],"date":{"year":"1950","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"Bell System Technical Journal","series":null,"scope":{"volume":29,"pages":{"from_page":147,"to_page":160}}},"b11":{"title":"The Fourth Paradigm: Data-Intensive Scientific Discovery","authors":[{"person_name":{"surname":"Hey","first_name":"T"},"affiliations":[],"email":null},{"person_name":{"surname":"Tansley","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Tolle","first_name":"K"},"affiliations":[],"email":null}],"date":{"year":"2009","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b12":{"title":"Advances in record linkage methodology as applied to the 1985 census of tampa florida","authors":[{"person_name":{"surname":"Jaro","first_name":"M"},"affiliations":[],"email":null}],"date":{"year":"1989","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"Journal of the American Statistical Society","series":null,"scope":{"volume":84,"pages":{"from_page":414,"to_page":420}}},"b13":{"title":"Taxicab Geometry","authors":[{"person_name":{"surname":"Krause","first_name":"E"},"affiliations":[],"email":null}],"date":{"year":"1987","month":null,"day":null},"ids":null,"target":null,"publisher":"Dover Publications","journal":null,"series":null,"scope":null},"b14":{"title":"Situated Learning: Legitimate Peripheral Participation","authors":[{"person_name":{"surname":"Lave","first_name":"J"},"affiliations":[],"email":null},{"person_name":{"surname":"Wenger","first_name":null},"affiliations":[],"email":null}],"date":{"year":"1991","month":null,"day":null},"ids":null,"target":null,"publisher":"Cam","journal":null,"series":null,"scope":null},"b15":{"title":"Binary Codes Capable of Correcting Deletions, Insertions and Reversals","authors":[{"person_name":{"surname":"Levenshtein","first_name":"V"},"affiliations":[],"email":null}],"date":{"year":"1966","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"Soviet Physics Doklady","series":null,"scope":{"volume":10,"pages":{"from_page":707,"to_page":710}}},"b16":{"title":"The Soundex Indexing System","authors":[],"date":{"year":"2007","month":null,"day":null},"ids":null,"target":null,"publisher":"National Archives and Records Administration","journal":null,"series":null,"scope":null},"b17":{"title":"A general method applicable to the search for similarities in the amino acid sequence of two proteins","authors":[{"person_name":{"surname":"Needleman","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Wunsch","first_name":"C"},"affiliations":[],"email":null}],"date":{"year":"1970","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"Journal of Molecular Biology","series":null,"scope":{"volume":48,"pages":{"from_page":443,"to_page":453}}},"b18":{"title":"The collage authoring environment","authors":[{"person_name":{"surname":"Nowakowski","first_name":"P"},"affiliations":[],"email":null},{"person_name":{"surname":"Ciepiela","first_name":"E"},"affiliations":[],"email":null},{"person_name":{"surname":"Harezlak","first_name":"D"},"affiliations":[],"email":null},{"person_name":{"surname":"Kocot","first_name":"J"},"affiliations":[],"email":null},{"person_name":{"surname":"Kasztelnik","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Bartynski","first_name":"T"},"affiliations":[],"email":null},{"person_name":{"surname":"Meizner","first_name":"J"},"affiliations":[],"email":null},{"person_name":{"surname":"Dyk","first_name":"G"},"affiliations":[],"email":null},{"person_name":{"surname":"Malawski","first_name":"M"},"affiliations":[],"email":null}],"date":{"year":"2011","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"Procedia CS","series":null,"scope":{"volume":4,"pages":{"from_page":608,"to_page":617}}},"b19":{"title":"The design and realisation of the my experiment virtual research environment for social sharing of workflows","authors":[{"person_name":{"surname":"Roure","first_name":"D"},"affiliations":[],"email":null},{"person_name":{"surname":"Goble","first_name":"C"},"affiliations":[],"email":null},{"person_name":{"surname":"Stevens","first_name":"R"},"affiliations":[],"email":null}],"date":{"year":"2009","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"Future Generation Comp. Syst","series":null,"scope":{"volume":25,"pages":{"from_page":561,"to_page":567}}},"b20":{"title":"Integration of complex archaeology digital libraries: An ETANA-DL experience","authors":[{"person_name":{"surname":"Shen","first_name":"R"},"affiliations":[],"email":null},{"person_name":{"surname":"Vemuri","first_name":"N"},"affiliations":[],"email":null},{"person_name":{"surname":"Fan","first_name":"W"},"affiliations":[],"email":null},{"person_name":{"surname":"Fox","first_name":"E"},"affiliations":[],"email":null}],"date":{"year":"2008","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"Information Systems","series":null,"scope":{"volume":33,"pages":{"from_page":699,"to_page":723}}},"b21":{"title":"Identification of common molecular subsequences","authors":[{"person_name":{"surname":"Smith","first_name":"T"},"affiliations":[],"email":null},{"person_name":{"surname":"Waterman","first_name":"M"},"affiliations":[],"email":null}],"date":{"year":"1981","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"Journal of Molecular Biology","series":null,"scope":{"volume":147,"pages":{"from_page":195,"to_page":197}}},"b22":{"title":"Taming Big Data","authors":[{"person_name":{"surname":"Stapleton","first_name":"L"},"affiliations":[],"email":null}],"date":{"year":"2011","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"IBM Data Management Magazine","series":null,"scope":{"volume":16,"pages":{"from_page":12,"to_page":18}}},"b23":{"title":"Digital libraries for scientific data discovery and reuse: from vision to practical reality","authors":[{"person_name":{"surname":"Wallis","first_name":"J"},"affiliations":[],"email":null},{"person_name":{"surname":"Mayernik","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Borgman","first_name":"C"},"affiliations":[],"email":null},{"person_name":{"surname":"Pepe","first_name":"A"},"affiliations":[],"email":null}],"date":{"year":"2010","month":null,"day":null},"ids":null,"target":null,"publisher":"ACM","journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":333,"to_page":340}}},"b24":{"title":"Communities of Practice: Learning, Meaning and Identity","authors":[{"person_name":{"surname":"Wenger","first_name":"E"},"affiliations":[],"email":null}],"date":{"year":"1998","month":null,"day":null},"ids":null,"target":null,"publisher":"Cambridge University Press","journal":null,"series":null,"scope":null},"b25":{"title":"String Comparator Metrics and Enhanced Decision Rules in the Fellegi-Sunter Model of Record Linkage","authors":[{"person_name":{"surname":"Winkler","first_name":"W"},"affiliations":[],"email":null}],"date":{"year":"1990","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":354,"to_page":359}}}},"sections":[{"title":"Introduction","paragraphs":[{"text":"Digital Libraries have evolved a lot during the last twenty years while maintaining and further strengthening their central role in knowledge sharing [6]. Digital Libraries are revolutionizing the whole knowledge management lifecycle. They are no longer perceived as a means to discover cultural heritage only, rather are nowadays conceived as innovative, dynamic, and ubiquitous research supporting environments. In such environments communities of practice [15,25] are expected to be able, through their Web browsers, to seamlessly access and exploit data, services, and processing resources managed by diverse systems in separate administration domains.","refs":[{"start":150,"end":153,"marker":"bibr","target":"#b5"},{"start":459,"end":463,"marker":"bibr","target":"#b14"},{"start":463,"end":466,"marker":"bibr","target":"#b24"}]},{"text":"This evolution continues to enlarge the domains Digital Libraries are called to serve that presently include eScience, cultural heritage, and others [5,21,2,9,12,24]. Current Digital Library developers are called to develop complex systems that have to give solutions to \"traditional\" issues, e.g., existing data providers federation, distributed retrieval, and long-term preservation, as well as \"new\" issues, e.g., social network models, large-scale computing, and micro information. Furthermore, they have to face scaled-up versions of the above issues with respect to various axes, e.g., number and variety of actors to be served, size and variety of content to be managed, and diversity of systems and technologies to be integrated. Very often the content they are requested to manage falls under the \"data\" category and their implementation actually requires the realization of Data Infrastructures.","refs":[{"start":149,"end":152,"marker":"bibr","target":"#b4"},{"start":152,"end":155,"marker":"bibr","target":"#b20"},{"start":155,"end":157,"marker":"bibr","target":"#b1"},{"start":157,"end":159,"marker":"bibr","target":"#b8"},{"start":159,"end":162,"marker":"bibr","target":"#b11"},{"start":162,"end":165,"marker":"bibr","target":"#b23"}]},{"text":"The term \"data\" itself, although very common, is difficult to define since it may be given different meanings, both in the digital and in the real world. Actually, the act of recognising or understanding that \"something\" -e.g., observations, statistics, artefacts, records -constitutes data is an intellectual activity that is usually driven by a certain goal. Data is collected for many purposes, via different approaches and very often it is difficult to interpret once exploited in contexts other than its initial one [3,4]. Digital Libraries are called to manage data ranging from traditional research outputs, mainly papers and experimental data, to living reports [7,5], executable research papers [10,19], and scientific workflows [20]. Very often such data fall into the category of \"big data\" [23], i.e., data characterised by (i) volume, i.e., its dimension in terms of bytes is huge; (ii) velocity, i.e., its speed requirements for collecting, processing and using is demanding; and (iii) variety, i.e., its heterogeneity in terms of data types to be managed and data sources to be merged is high.","refs":[{"start":521,"end":524,"marker":"bibr","target":"#b2"},{"start":524,"end":526,"marker":"bibr","target":"#b3"},{"start":670,"end":673,"marker":"bibr","target":"#b6"},{"start":673,"end":675,"marker":"bibr","target":"#b4"},{"start":704,"end":708,"marker":"bibr","target":"#b9"},{"start":708,"end":711,"marker":"bibr","target":"#b18"},{"start":738,"end":742,"marker":"bibr","target":"#b19"},{"start":802,"end":806,"marker":"bibr","target":"#b22"}]},{"text":"This paper discusses one of the problems arising when dealing with tabular data1 management where management needs (i) to support collaboration among multiple users and organizations; (ii) to appeal to a broad audience of users who are not technically skilled; and (iii) to guarantee data completeness and correctness as to enable effective data analysis; i.e., to solve the problem of identifying, verifying and associating the actual controlled vocabularies that might have been used by the data provider while producing the dataset. Tabular data are mainly stored in CSV (Comma Separated Values) files where little or no emphasis is posed on representing and standardizing the characterization of the single columns they consist of. However, knowing the \"type\" of values a column is expected to contain (controlled vocabulary, code list or reference dataset rather than basic types such as string or integer) is a fundamental aspect when datasets have to be effectively managed for, e.g., certification of compliance, comparison, integration and analysis. To this aim, this paper proposes an approach for supporting an end user during the operations to transform a \"raw dataset \" -i.e., a dataset consisting of its data only -into a \"characterized dataset \" -i.e., a dataset where each column is characterized by the controlled vocabulary from which its values have been selected. The effectiveness of such an approach is discussed in the context of a Data Infrastructure.","refs":[{"start":79,"end":80,"marker":null,"target":"#foot_0"}]},{"text":"The remainder of the paper is organized as follows. Section 2 characterizes the major challenges of the problem identified above. Section 3 describes the proposed approach. Section 4 assesses the effectiveness of the proposed approach. Finally, Section 5 concludes the paper and summarizes its results.","refs":[]}]},{"title":"The Tabular Data Characterization Problem","paragraphs":[{"text":"Data-intensive science as well as approaches expecting to rely on data require three basic activities: data capture, curation, and analysis. In these scenarios, data come in all scales and shapes covering: large international experiments; cross-laboratory, single-laboratory, and individual observations; and also individuals lives [12].","refs":[{"start":332,"end":336,"marker":"bibr","target":"#b11"}]},{"text":"In these settings it is fundamental to equip collected datasets with additional information aiming at characterizing each dataset and making it possible to interpret the dataset even in contexts other than its initial one. This additional information may range from bibliographic-oriented metadata to provenance-, coverage-, certification-oriented metadata. Enriched and standardized datasets are, in fact, simpler to be managed and allow for exploiting more predefined functionalities as to get high performances on analysis and processing.","refs":[]},{"text":"Tabular data represent a very common format for many datasets in many different scenarios, e.g., statistical data, surveys, observations. A fundamental piece of information that should equip tabular data is the one characterizing the \"data type\" of any column a dataset contains. However, the actual notion of data type goes well beyond the expected ones like string or integer. In fact, the compilation of datasets commonly relies on existing controlled vocabularies, code lists and reference datasets2 . For instance, in compiling a dataset on catch statistics or specimen records it is worth to refer to reference datasets for species names and zones. Such reference datasets usually contain a complete record for each of the instances the reference dataset is about, as well as links with other reference datasets. By linking a dataset with the reference datasets its values come from, the actual information contained in the dataset is multiplied. The motivations of this are similar to those of Linked Data [1].","refs":[{"start":502,"end":503,"marker":null,"target":"#foot_1"},{"start":1013,"end":1016,"marker":"bibr","target":"#b0"}]},{"text":"Although reference datasets are used or alluded during datasets capture phase, any information about them is usually discarded when the tabular dataset is stored in a CSV file for management purposes. Moreover, this capture phase is usually performed in very diverse technological and organizational settings, thus leading to a very heterogeneous set of tabular datasets. Because of this, it is expected that a curation phase reconciles the \"raw dataset\" with its \"characterized\" / \"curated\" version when the datasets are aggregated in a common information space aiming at promoting their consumption.","refs":[]},{"text":"Common issues that may arise when a user wants to \"curate\" a given dataset are the following:","refs":[]},{"text":"-The raw dataset contains entries which might be misspelled with respect to the intended reference values; -The raw dataset contains too many entries to be controlled by hand; -The reference datasets are too many to be manually searched and then be associated with the dataset under curation; -Potentially, many reference datasets might be associated to a given dataset (high level of ambiguity).","refs":[]},{"text":"A complete comparison between a raw dataset and all the reference datasets would need high computational requirements. Moreover, it is not appropriate if a quick (almost real time) response time is expected, as it happens when the user is asking a web application to propose a reference dataset suitable for the dataset she/he is managing.","refs":[]},{"text":"On the other hand, even a \"greedy\" approach is not so easy to identify because of the issues just discussed, e.g., a simple match between string data cannot be used because it is incapable to overcome the misspelling problems.","refs":[]},{"text":"In the remainder of the paper, an approach for supporting an end user during the curation phase is proposed. It consists in an \"helper\" facilitating the identification of the reference datasets that have been actually used while compiling the \"raw dataset\". This approach is conceived to be fast and effective with respect to the issues discussed above.","refs":[]}]},{"title":"An Approach for Tabular Data Characterization","paragraphs":[{"text":"The proposed approach is based on two algorithms: (i) a revised version of the Minimum Edit Distance (MED) and (ii) a constant complexity ranking procedure aiming at proposing a ranked list of suitable reference datasets given a column of a tabular dataset.","refs":[]},{"text":"The Minimum Edit Distance (or Levenshtein Distance) algorithm was firstly introduced in [16]. It is a metric for measuring the amount of difference between two character sequences. It is defined as the minimum number of edits needed to transform one string into the other, the allowed edit operations being insertion, deletion, or substitution of a single character. The algorithm is based on a dynamic programming procedure introduced in [18] and has a computational complexity that is linear with respect to the product of the length (number of characters) of the strings to be compared. However, there exist several approaches for computing the \"distance\" between two strings or sequences of symbols. Some well known similarity metrics, i.e., measures for similarity or dissimilarity between two text strings for approximate matching or comparison, include: (i) the Hamming distance [11], which calculates the number of positions at which the corresponding symbols are different; (ii) the Needleman-Wunsch distance [18], which is used in bioinformatics to align protein or nucleotide sequences; and (iii) the Smith-Waterman distance [22], which is a variation of the previous one and performs local sequences alignment. Other techniques are used in various domains ranging from biology to phonetics, e.g., (i) the Jaro-Winkler distance [13,26], which is mainly used in the area of duplicates detection; (ii) the Block or L1 distance [14], which introduces a new geometry for distance calculation, where Euclidean geometry is replaced by a new metric in which the distance between two points is the sum of the absolute differences of their coordinates; and (iii) the Soundex distance [17], which is a phonetic algorithm for indexing names by sound, as pronounced in English. Among the existing algorithms, we selected the MED one as it is the most common method for string comparison, its implementation is straightforward and it fits well with the characteristics of the proposed approach.","refs":[{"start":88,"end":92,"marker":"bibr","target":"#b15"},{"start":439,"end":443,"marker":"bibr","target":"#b17"},{"start":886,"end":890,"marker":"bibr","target":"#b10"},{"start":1018,"end":1022,"marker":"bibr","target":"#b17"},{"start":1136,"end":1140,"marker":"bibr","target":"#b21"},{"start":1339,"end":1343,"marker":"bibr","target":"#b12"},{"start":1343,"end":1346,"marker":"bibr","target":"#b25"},{"start":1436,"end":1440,"marker":"bibr","target":"#b13"},{"start":1686,"end":1690,"marker":"bibr","target":"#b16"}]},{"text":"The constant complexity ranking procedure proposed is called Lexical Guesser. This is an approach defined by relying on the edit distance, which uses the lexical similarity scores for limiting the computational extent of the ranking procedure of a given column of a dataset. From that point on, a given column of a dataset which has been selected for curation purposes is called \"target dataset \". The Lexical Guesser uses similarities, instead of exact matching, in order to avoid to perform all the comparisons between the target dataset entries and all the entries of all the recognized reference datasets. The basic underlying ideas are:","refs":[]},{"text":"if the target dataset contains entries which are misspelled, errors can be recovered by using MED (actually, a revised version of it); if the target dataset is syntactically correct, then the computation can be limited by assuming that by picking some random chunks (samples) from the right reference dataset, these chunks will probably be lexically similar to the target dataset. For instance, a target dataset containing entries like 'North Atlantic Ocean', 'South Pacific Ocean', etc. would always get a non-minimal score when compared to the 'Oceans English Names' reference dataset because the latter also contains entries like 'Indian Ocean' or 'North Pacific Ocean' which share some lexical similarities with the target dataset entries. It is assumed that the recall of the search for the target dataset can include all those reference datasets presenting lexical similarities (over a certain threshold ); the proposed approach is expected to be an helper for an activity that should remain semiautomatic, i.e., the algorithm reduces the search space of the possible reference data, while the final choice about the reference dataset to use is a duty of the user.","refs":[]},{"text":"According to the above premises, the MED algorithm was modified and then incorporated into a ranking procedure realising the Lexical Guesser.","refs":[]},{"text":"Actually, the MED algorithm has been enriched with a set of check rules and parameters aiming at enhancing its performances for the overall classification process. From a preliminary analysis, it has been noticed that the standard MED algorithm is not sufficient for calculating distances in the target scenarios. Some boosting rules have been added to raise or lower the scores in some cases.","refs":[]},{"text":"The distance between two strings x and y is calculated as follows:","refs":[]},{"text":"5 min minln maxln * 1.5, 0.9 if contains(x, y) ∨ contains(y, x) 1 -MED(x,y) maxln otherwise (1) where:","refs":[{"start":92,"end":95,"marker":"bibr","target":"#b0"}]}]},{"title":"Maxln = max(length(x), length(y)); minln = min(length(x), length(y)).","paragraphs":[{"text":"The constant values in the formula above are the result of an experimental activity. The limitation to 0.9 for the value of d(x, y) when a string contains another one is a penalty score which aims to lower the distance value in the cases when strings are really close but not equal.","refs":[]},{"text":"The ranking procedure consists in computing a similarity score S(T, R i ) between the target dataset T , i.e., the values of a given dataset column, and every recognized reference dataset R i as a product of three factors, namely (i) a distance score D(T, R i ), (ii) a coverage score C(T, R i ), and (iii) a weight score W (R i ), by actually relying on samples of both the datasets, i.e., T and R i . A score α is computed to estimate the representativeness of the sample R i as follows:","refs":[]},{"text":"Given a target dataset T , for each reference dataset R i the similarity score S(T, R i ) is calculated by the following formula:","refs":[]},{"text":"where 1. the distance score D(T, R i ) is computed as the average distance between all the pairs of the selected samples","refs":[]},{"text":"where the distance is greater than an \"acceptance treshold\" τ as follows:","refs":[]},{"text":"2. the coverage score C(T, R i ) is computed by multiplying the α score aiming at indicating the representativeness of the sample R i by a factor aiming at indicating the similarity between R i and T as follows:","refs":[]},{"text":"where","refs":[]},{"text":"respect to the size of the whole set of recognized datasets and (ii) mitigating the impact of \"big\" dataset via logarithmic transformation as follows:","refs":[]},{"text":"It is evident that the higher is each factor value, the higher the similarity score. This means that a very high score could imply a good overall similarity among the single entries but even that the elements in T cover a big percentage of the R i set. Given a target dataset T , the list of recommended reference datasets is produced by sorting the set of reference dataset according to the values of the similarity score S(T, R i ) and pruning those whose score differs from the top-ranked element in the list (i.e., the best score) for more than a given customizable threshold (Maximum Difference from Best Threshold or MDBT).","refs":[]},{"text":"Overall, the complexity of the procedure depends from the number of string comparisons to be performed. If k is the number of reference datasets recognized and |T | = m and ∀i, |R i | = n, then the overall number of comparisons to be performed is k * m * n. However, because of its characteristics, the proposed approach is incline for parallelization both with respect to the reference datasets (every S(T, R i ) can be calculated by an independent process) as well as with respect to the single reference dataset (independent processes can be used to calculate factors of the same S(T, R i )).","refs":[]},{"text":"The procedure can then be tuned in order to get results in an acceptable time, e.g., by establishing proper values for n and m as well as for the thresholds and the rest of parameters discussed above. The higher is the number of comparisons, the higher will be the complexity of the calculation as well as the accuracy. These aspects are discussed in the next Section.","refs":[]}]},{"title":"Experiment and Results","paragraphs":[{"text":"The experiment we performed to validate the approach is based on tabular datasets and reference datasets expected to be managed in the context of the large scale data infrastructure implemented by D4Science and D4Science-II projects [8]. In particular, the settings are those resulting from an environment aiming at providing fisheries statisticians with a set of tools to manage tabular data on catch statistics. Tabular data usually are time series coming from observations about fishery periodic catches in terms of quantities and costs. When an user wants to manage a new time series, in order to use all the facilities offered by the environments for time series analysis and consumption, she/he has to curate such dataset by recognizing the reference datasets it exploits. Such operation involves the correction of misspelled entries, the identification of the data types for the columns and a validation of the coherence of the dataset contents. In this phase, the user is expected to rely on facilities helping the identification of the most suitable reference datasets. These facilities are based on the Lexical Guesser.","refs":[{"start":233,"end":236,"marker":"bibr","target":"#b7"}]},{"text":"In this scenario, the set of recognized reference datasets is about information on marine species, e.g., species names, geographical areas, economic zones. It consists in 326 reference datasets, containing from 5 to 39,000 elements. These reference datasets can be classified as follows:","refs":[]},{"text":"no overlap -reference datasets that are disjoint from each other; medium overlap -reference datasets that present a medium degree of intersection with other ones, i.e., 20-50% of their entries overlap with entries in at least another reference dataset (e.g., FAO area names, the ocean and sub-ocean names and the geographical names); high overlap -reference datasets that have a large degree of intersection with others, i.e., 80-90% of their entries overlap with entries in at least another reference dataset (e.g., species names coming from different species databases).","refs":[]},{"text":"Each experiment reported in the remainder of this paper was executed by using 50 different target datasets for 20 times per input. The average score is reported in the tables.","refs":[]},{"text":"In order to test the performances of the proposed approach, the following well known measures have been exploited:","refs":[]},{"text":"P recision = T rueP ositives T rueP ositives + F alseP ositives (7) Recall = T rueP ositives T rueP ositives + F alseN egatives (8)","refs":[{"start":64,"end":67,"marker":"bibr","target":"#b6"}]},{"text":"where:","refs":[]},{"text":"-True Negatives indicates the number of classifications which are correctly classified as not suitable, -True Positives indicates the number of reported classifications which are really suitable for labeling an unknown target dataset; -False Negatives and False Positives are defined by complement of the above.","refs":[]}]},{"title":"The experiment configuration was set as follows:","paragraphs":[{"text":"one sample of 25 elements was taken from a target dataset; a sample of 625 elements was taken from each reference datasets; the threshold for pruning the ranked list of S(T, R i ), i.e., MDBT, was set to 30%;","refs":[]},{"text":"The performance of the following tree approaches have been assessed:","refs":[]},{"text":"-Lexical Guesser -i.e., the approach proposed in Sec. 3, based on the ranking of similarity scores S(T, R i ) by formula 2; -Simple Matcher -Constant Complexity -i.e., an approach based on the same ranking procedure (with pruning) where the distance d(x, y) is based on exact string matching; -Simple Matcher -High Complexity -i.e., an approach based on the ranking of S(T, R i ) for all the reference datasets where the distance d(x, y) is based on exact string matching. Table 1 reports the results for target datasets that match exactly one reference dataset. In the case of no overlap, all the approaches get a 100% of accuracy. The task is quite trivial and the ranking procedure with pruning does not influence the performances. In the case of medium overlap, the 'Simple Matcher -High Complexity' approach is expected to perform better than the others, while errors are experienced with the 'Simple Matcher -Constant Complexity'. The 'Lexical Guesser' performs as good as the 'Simple Matcher -High Complexity', thus the flexibility of d(x, y) does not help in this case. In the case of medium overlap, the Lexical Guesser performs worst than the others, however the performances are still acceptable for the application scopes. Table 2 presents the performances when the experiment focuses on target datasets containing misspelled entries and entries that do not occur at all in reference datasets for the 50 to 100% of their entries. The 'Lexical Guesser' always outperforms the other two approaches. Moreover, the 'Simple Matcher -Constant Complexity' introduces errors. Performances are appreciable both in terms of accuracy and recall, which means that the approach is always able to return the right reference datasets to the user. The recall of approaches based on simple matching is always lower than that of the Lexical Guesser because in some cases the target dataset may be ambiguous, so that more than one reference dataset is suitable for it. In this case the choice necessarily is on the user's side, as she/he only knows the real nature of her/his data. A simple match tends to find few columns, while the proposed approach uses the flexibility of the comparisons in order to propose more reference datasets. The precision score indicates that in presence of either low or high ambiguity, the Lexical Guesser is able to extract the correct information, while with medium ambiguity, the statistical nature of the algorithm begins to be evident. This happens because for some samples lexical similarities are found, while for others they are not retrieved. As for the F1 measure, it can be noted that it gives an estimation of the overall functioning, and the value for the Lexical Matcher is always higher.","refs":[{"start":479,"end":480,"marker":"table","target":"#tab_0"},{"start":1241,"end":1242,"marker":"table","target":"#tab_1"}]}]},{"title":"Conclusion","paragraphs":[{"text":"The evolution of Digital Libraries calls for innovative, dynamic, and ubiquitous research supporting environments where communities of practice can seamlessly access data, software, and processing resources managed by diverse systems in separate administration domains through their Web browsers. In these environments data are multiform and their management demand for new methods. This paper has discussed one of the problems arising when dealing with tabular data management where management occurs in scenarios characterized by these needs: (i) supporting collaboration among multiple users and organizations; (ii) appealing to a broad audience of users who are not technically skilled; and (iii) guaranteeing data completeness and correctness as to enable effective data analysis; i.e., giving solution to the problem of identifying, verifying and associating the actual reference datasets that might have been used by the data provider while producing the dataset.","refs":[]},{"text":"It has been proposed an approach supporting an end user during the massaging of a \"raw dataset \" to transform it into a \"characterized dataset \" defined by associating the proper reference datasets that might have been used while capturing the data. This approach is based on (i) a similarity measure aiming at estimating the similarity among the entries of the target dataset and the entries of the reference dataset by overcoming misspelling issues and (ii) a ranking approach appropriate for a real time use and aiming at providing the end user with a sorted list of reference datasets suitable for a given target dataset.","refs":[]},{"text":"The experimental results show that the proposed approach actually outperforms other approaches in presence of misspelled entries, even if it looses in performances with respect to an approach based on exact string matching when user's data completely agree with some of the reference dataset.","refs":[]}]}],"tables":{"tab_0":{"heading":"Table 1 .","description":"Results on a column of a dataset that exactly matches a reference dataset (results are expressed in percentages)","rows":[["","Lexical Guesser","Simple Matcher","Simple Matcher"],["","","Constant Complexity High Complexity"],["","","No overlap",""],["Accuracy","100","100","100"],["Precision","100","100","100"],["Recall","100","100","100"],["F1","100","100","100"],["","Medium overlap (20%-50%)",""],["Accuracy","99.18","99.18","99.40"],["Precision","28.89","28.89","38.89"],["Recall","100","100","100"],["F1","44.44","44.44","44.44"],["","High overlap (80%-90%)",""],["Accuracy","99.77","99.85","99.92"],["Precision","70.83","75","87.50"],["Recall","100","100","100"],["F1","79.17","83.33","91.67"]]},"tab_1":{"heading":"Table 2 .","description":"Results on a column of a dataset which does not match exactly reference datasets (results are expressed in percentages)","rows":[["","Lexical Guesser","Simple Matcher","Simple Matcher"],["","","Constant Complexity High Complexity"],["","No Superpositions",""],["Accuracy","100","99.69","94.89"],["Precision","100","66.67","35.29"],["Recall","100","44.44","55.56"],["F1","100","53.33","30.37"],["","Medium Superpositions (20%-50%)"],["Accuracy","99.54","99.39","99.54"],["Precision","58.33","100","100"],["Recall","100","45.83","62.50"],["F1","73.33","60","70"],["","High Superpositions (80%-90%)",""],["Accuracy","99.54","99.23","99.23"],["Precision","100","50","50"],["Recall","50","16.67","16.67"],["F1","65","25","25"]]}},"abstract":{"title":"Abstract","paragraphs":[{"text":"Digital Libraries continue to evolve towards research environments supporting access and management of multiform Information Objects spread across multiple data sources and organizational domains. This evolution has introduced the need to deal with Information Objects having traits different from those characterizing Digital Libraries at their early stages and to revise the services supporting their management. Tabular data represent a class of Information Objects that require to be efficiently managed because of their core role in many eScience scenarios. This paper discusses the tabular data characterization problem, i.e., the problem of identifying the reference dataset of any column of the dataset. In particular, the paper presents an approach based on lexical matching techniques to support users during the data curation phase by providing them with a ranked list of reference datasets suitable for a dataset column.","refs":[]}]}}