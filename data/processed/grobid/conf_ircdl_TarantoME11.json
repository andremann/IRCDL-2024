{"bibliography":{"title":"Probabilistic Inference over Image Networks","authors":[{"person_name":{"surname":"Taranto","first_name":"Claudio"},"affiliations":[{"department":"Department of Computer Science","institution":"University of Bari \"Aldo Moro\"","laboratory":null},{"department":"Department of Computer Science","institution":"University of Bari \"Aldo Moro\"","laboratory":null}],"email":"claudio.taranto@di.uniba.it"},{"person_name":{"surname":"Mauro","first_name":"Nicola"},"affiliations":[{"department":"Department of Computer Science","institution":"University of Bari \"Aldo Moro\"","laboratory":null},{"department":"Department of Computer Science","institution":"University of Bari \"Aldo Moro\"","laboratory":null}],"email":null},{"person_name":{"surname":"Esposito","first_name":"Floriana"},"affiliations":[{"department":"Department of Computer Science","institution":"University of Bari \"Aldo Moro\"","laboratory":null},{"department":"Department of Computer Science","institution":"University of Bari \"Aldo Moro\"","laboratory":null}],"email":"esposito@di.uniba.it"}],"date":null,"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"keywords":[],"citations":{"b0":{"title":"The DELOS Digital Library Reference Model","authors":[{"person_name":{"surname":"Candela","first_name":"L"},"affiliations":[],"email":null},{"person_name":{"surname":"Castelli","first_name":"D"},"affiliations":[],"email":null},{"person_name":{"surname":"Ferro","first_name":"N"},"affiliations":[],"email":null},{"person_name":{"surname":"Ioannidis","first_name":"Y"},"affiliations":[],"email":null},{"person_name":{"surname":"Koutrika","first_name":"G"},"affiliations":[],"email":null},{"person_name":{"surname":"Meghini","first_name":"C"},"affiliations":[],"email":null},{"person_name":{"surname":"Pagano","first_name":"P"},"affiliations":[],"email":null},{"person_name":{"surname":"Ross","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Soergel","first_name":"D"},"affiliations":[],"email":null},{"person_name":{"surname":"Agosti","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Dobreva","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Katifori","first_name":"V"},"affiliations":[],"email":null},{"person_name":{"surname":"Schuldt","first_name":"H"},"affiliations":[],"email":null}],"date":{"year":"2007","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b1":{"title":"In defense of nearest-neighbor based image classification","authors":[{"person_name":{"surname":"Boiman","first_name":"O"},"affiliations":[],"email":null},{"person_name":{"surname":"Shechtman","first_name":"E"},"affiliations":[],"email":null},{"person_name":{"surname":"Irani","first_name":"M"},"affiliations":[],"email":null}],"date":{"year":"2008","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":1,"to_page":8}}},"b2":{"title":"Introduction to Statistical Relational Learning","authors":[{"person_name":{"surname":"Getoor","first_name":"L"},"affiliations":[],"email":null},{"person_name":{"surname":"Taskar","first_name":"B"},"affiliations":[],"email":null}],"date":{"year":"2007","month":null,"day":null},"ids":null,"target":null,"publisher":"The MIT Press","journal":null,"series":null,"scope":null},"b3":{"title":"Link mining: a survey","authors":[{"person_name":{"surname":"Getoor","first_name":"L"},"affiliations":[],"email":null},{"person_name":{"surname":"Diehl","first_name":"C"},"affiliations":[],"email":null}],"date":{"year":"2005","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"SIGKDD Explorations Newsletter","series":null,"scope":{"volume":7,"pages":{"from_page":3,"to_page":12}}},"b4":{"title":"Link-based Classification","authors":[{"person_name":{"surname":"Getoor","first_name":"L"},"affiliations":[],"email":null}],"date":{"year":"2005","month":null,"day":null},"ids":null,"target":null,"publisher":"Springer","journal":null,"series":null,"scope":null},"b5":{"title":"Image indexing using color correlograms","authors":[{"person_name":{"surname":"Huang","first_name":"J"},"affiliations":[],"email":null},{"person_name":{"surname":"Kumar","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Mitra","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Zhu","first_name":"W"},"affiliations":[],"email":null},{"person_name":{"surname":"Zabih","first_name":"R"},"affiliations":[],"email":null}],"date":{"year":"1997","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":762,"to_page":768}}},"b6":{"title":"Problog: a probabilistic prolog and its application in link discovery","authors":[{"person_name":{"surname":"Raedt","first_name":"L"},"affiliations":[],"email":null},{"person_name":{"surname":"Kimmig","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Toivonen","first_name":"H"},"affiliations":[],"email":null}],"date":{"year":"2007","month":null,"day":null},"ids":null,"target":null,"publisher":"AAAI Press","journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":2468,"to_page":2473}}},"b7":{"title":"Sampling Strategies for Bag-of-Features Image Classification","authors":[{"person_name":{"surname":"Nowak","first_name":"E"},"affiliations":[],"email":null},{"person_name":{"surname":"Jurie","first_name":"F"},"affiliations":[],"email":null},{"person_name":{"surname":"Triggs","first_name":"B"},"affiliations":[],"email":null}],"date":{"year":"2006","month":null,"day":null},"ids":null,"target":null,"publisher":"Springer","journal":null,"series":null,"scope":{"volume":3954,"pages":{"from_page":490,"to_page":503}}},"b8":{"title":"Probabilistic latent semantic analysis","authors":[{"person_name":{"surname":"Hofmann","first_name":"T"},"affiliations":[],"email":null}],"date":{"year":"1995","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b9":{"title":"Latent dirichlet allocation","authors":[{"person_name":{"surname":"Blei","first_name":"D"},"affiliations":[],"email":null},{"person_name":{"surname":"Ng","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Jordan","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Lafferty","first_name":"J"},"affiliations":[],"email":null}],"date":{"year":"2003","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"Journal of Machine Learning Research","series":null,"scope":{"volume":3,"pages":{"from_page":993,"to_page":1022}}},"b10":{"title":"A vector space model for automatic indexing","authors":[{"person_name":{"surname":"Salton","first_name":"G"},"affiliations":[],"email":null},{"person_name":{"surname":"Wong","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Yang","first_name":"C"},"affiliations":[],"email":null}],"date":{"year":"1975","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"Communication ACM","series":null,"scope":{"volume":18,"pages":{"from_page":613,"to_page":620}}},"b11":{"title":"A Probabilistic Approach to Object Recognition Using Local Photometry and Global Geometry","authors":[{"person_name":{"surname":"Burl","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Weber","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Perona","first_name":"P"},"affiliations":[],"email":null}],"date":{"year":"1998","month":null,"day":null},"ids":null,"target":null,"publisher":"Springer","journal":null,"series":null,"scope":{"volume":1407,"pages":{"from_page":628,"to_page":641}}},"b12":{"title":"Object class recognition by unsupervised scale-invariant learning","authors":[{"person_name":{"surname":"Fergus","first_name":"R"},"affiliations":[],"email":null},{"person_name":{"surname":"Perona","first_name":"P"},"affiliations":[],"email":null},{"person_name":{"surname":"Zisserman","first_name":"A"},"affiliations":[],"email":null}],"date":{"year":"2003","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":264,"to_page":271}}},"b13":{"title":"Thirty years of graph matching in pattern recognition","authors":[{"person_name":{"surname":"Conte","first_name":"D"},"affiliations":[],"email":null},{"person_name":{"surname":"Foggia","first_name":"P"},"affiliations":[],"email":null},{"person_name":{"surname":"Sansone","first_name":"C"},"affiliations":[],"email":null},{"person_name":{"surname":"Vento","first_name":"M"},"affiliations":[],"email":null}],"date":{"year":"2004","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"International Journal of Pattern Recognition and Artificial Intelligence","series":null,"scope":{"volume":null,"pages":{"from_page":265,"to_page":298}}},"b14":{"title":"Collective classification in network data","authors":[{"person_name":{"surname":"Sen","first_name":"P"},"affiliations":[],"email":null},{"person_name":{"surname":"Namata","first_name":"G"},"affiliations":[],"email":null},{"person_name":{"surname":"Bilgic","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Getoor","first_name":"L"},"affiliations":[],"email":null},{"person_name":{"surname":"Gallagher","first_name":"B"},"affiliations":[],"email":null},{"person_name":{"surname":"Eliassi-Rad","first_name":"T"},"affiliations":[],"email":null}],"date":{"year":"2008","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"AI Magazine","series":null,"scope":{"volume":29,"pages":{"from_page":93,"to_page":106}}},"b15":{"title":"A simple relational classifier","authors":[{"person_name":{"surname":"Macskassy","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Provost","first_name":"F"},"affiliations":[],"email":null}],"date":{"year":"2003","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":64,"to_page":76}}},"b16":{"title":"Classification in networked data: A toolkit and a univariate case","authors":[{"person_name":{"surname":"Macskassy","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Provost","first_name":"F"},"affiliations":[],"email":null}],"date":{"year":"2007","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"Journal of Machine Learning Research","series":null,"scope":{"volume":8,"pages":{"from_page":935,"to_page":983}}},"b17":{"title":"Enhanced hypertext categorization using hyperlinks","authors":[{"person_name":{"surname":"Chakrabarti","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Dom","first_name":"B"},"affiliations":[],"email":null},{"person_name":{"surname":"Indyk","first_name":"P"},"affiliations":[],"email":null}],"date":{"year":"1998","month":null,"day":null},"ids":null,"target":null,"publisher":"ACM","journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":307,"to_page":318}}},"b18":{"title":"Link-based classification","authors":[{"person_name":{"surname":"Lu","first_name":"Q"},"affiliations":[],"email":null},{"person_name":{"surname":"Getoor","first_name":"L"},"affiliations":[],"email":null}],"date":{"year":"2003","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":496,"to_page":503}}},"b19":{"title":"Approximate image color correlograms","authors":[{"person_name":{"surname":"Taranto","first_name":"C"},"affiliations":[],"email":null},{"person_name":{"surname":"Di Mauro","first_name":"N"},"affiliations":[],"email":null},{"person_name":{"surname":"Ferilli","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Esposito","first_name":"F"},"affiliations":[],"email":null}],"date":{"year":"2010","month":null,"day":null},"ids":null,"target":null,"publisher":"ACM","journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":1127,"to_page":1130}}},"b20":{"title":"Abducing through negation as failure: stable models within the independent choice logic","authors":[{"person_name":{"surname":"Poole","first_name":"D"},"affiliations":[],"email":null}],"date":{"year":"2000","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"J. Log. Program","series":null,"scope":{"volume":44,"pages":{"from_page":5,"to_page":35}}},"b21":{"title":"A Graphical Method for Parameter Learning of Symbolic-Statistical Models","authors":[{"person_name":{"surname":"Kameya","first_name":"Y"},"affiliations":[],"email":null},{"person_name":{"surname":"Ueda","first_name":"N"},"affiliations":[],"email":null},{"person_name":{"surname":"Sato","first_name":"T"},"affiliations":[],"email":null}],"date":{"year":"1999","month":null,"day":null},"ids":null,"target":null,"publisher":"Springer","journal":null,"series":null,"scope":{"volume":1721,"pages":{"from_page":264,"to_page":276}}},"b22":{"title":"CLP(BN): Constraint logic programming for probabilistic knowledge","authors":[{"person_name":{"surname":"Costa","first_name":"V"},"affiliations":[],"email":null},{"person_name":{"surname":"Page","first_name":"D"},"affiliations":[],"email":null},{"person_name":{"surname":"Qazi","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Cussens","first_name":"J"},"affiliations":[],"email":null}],"date":{"year":"2003","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":517,"to_page":524}}},"b23":{"title":"Probabilistic Graphical Models: Principles and Techniques","authors":[{"person_name":{"surname":"Koller","first_name":"D"},"affiliations":[],"email":null},{"person_name":{"surname":"Friedman","first_name":"N"},"affiliations":[],"email":null}],"date":{"year":"2009","month":null,"day":null},"ids":null,"target":null,"publisher":"The MIT Press","journal":null,"series":null,"scope":null},"b24":{"title":"A statistical learning method for logic programs with distribution semantics","authors":[{"person_name":{"surname":"Sato","first_name":"T"},"affiliations":[],"email":null}],"date":{"year":"1995","month":null,"day":null},"ids":null,"target":null,"publisher":"MIT Press","journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":715,"to_page":729}}},"b25":{"title":"On the Efficient Execution of ProbLog Programs","authors":[{"person_name":{"surname":"Kimmig","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Santos Costa","first_name":"V"},"affiliations":[],"email":null},{"person_name":{"surname":"Rocha","first_name":"R"},"affiliations":[],"email":null},{"person_name":{"surname":"Demoen","first_name":"B"},"affiliations":[],"email":null},{"person_name":{"surname":"De Raedt","first_name":"L"},"affiliations":[],"email":null}],"date":{"year":"2008","month":null,"day":null},"ids":null,"target":null,"publisher":"Springer","journal":null,"series":null,"scope":{"volume":5366,"pages":{"from_page":175,"to_page":189}}},"b26":{"title":"DNF sampling for problog inference","authors":[{"person_name":{"surname":"Shterionov","first_name":"D"},"affiliations":[],"email":null},{"person_name":{"surname":"Kimmig","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Mantadelis","first_name":"T"},"affiliations":[],"email":null},{"person_name":{"surname":"Janssens","first_name":"G"},"affiliations":[],"email":null}],"date":{"year":"2010","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b27":{"title":"Pattern Recognition and Machine Learning","authors":[{"person_name":{"surname":"Bishop","first_name":"C"},"affiliations":[],"email":null}],"date":{"year":"2006","month":null,"day":null},"ids":null,"target":null,"publisher":"Springer","journal":null,"series":null,"scope":null},"b28":{"title":"Learning generative visual models from few training examples: an incremental bayesian approach tested on 101 object categories","authors":[{"person_name":{"surname":"Fei-Fei","first_name":"L"},"affiliations":[],"email":null},{"person_name":{"surname":"Fergus","first_name":"R"},"affiliations":[],"email":null},{"person_name":{"surname":"Perona","first_name":"P"},"affiliations":[],"email":null}],"date":{"year":"2004","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null}},"sections":[{"title":"Introduction","paragraphs":[{"text":"Digital Libraries organized digital collections of multimedia objects available online in computer processable form [1]. These libraries also comprise services and infrastructures to manage, store, retrieve and share objects. Digital images represent a component of multimedia objects involved in the digital library universe. The Machine Learning scientific community has developed methods and models to address the emerging issues arising from the management of these data types. One of main issues concerns the image classification, which addresses the need to automate the process of assigning a class label to a given image. This problem has been mainly tackled by adopting a learning-based or a non-parametric classifier [2]. The former approach requires a training phase in which a learning system induces a model able to identify new unlabelled images, while the latter does not require a learning phase but adopts methods directly applicable to the images to be classified. Both learning approaches require a feature based representation of the images. Furthermore, non-parametric algorithms adopt a similarity measure defined on the feature space aiming to calculate the similarity degree between two images.","refs":[{"start":116,"end":119,"marker":"bibr","target":"#b0"},{"start":727,"end":730,"marker":"bibr","target":"#b1"}]},{"text":"Given D a set of images, the majority of the existing methods adopted for image classification try to estimate the class probability of a query image q ∈ D, P (C q |D \\{q}), assuming that all the images in D are independent and identically distributed (i.i.d.). In particular, the probability P (C q |D \\ {q}) is assumed to be factored as in the following way i αP (C q |x i ), with x i ∈ D.","refs":[]},{"text":"This paper proposes a Statistical Relational Learning (SRL) [3] method to exploit the relationships among images in order to improve the image classification accuracy by means of a link-based classifier [4,5]. The main idea is to assume that the images x i ∈ D are not mutually independent and to try to elicit the hidden information representing the probabilistic connections between two images taking into account the possible relationships. To reach this goal, images are represented by means of a complex probabilistic network, where each image corresponds to a node and the connection degree among images is represented by a probabilistic edge. The relationship degree among images may be computed adopting a similarity measure based on their feature based representation. Correlograms [6], a statistics expressing how the colors of an image are spatially correlated, are used in this paper as features representing the images.","refs":[{"start":60,"end":63,"marker":"bibr","target":"#b2"},{"start":203,"end":206,"marker":"bibr","target":"#b3"},{"start":206,"end":208,"marker":"bibr","target":"#b4"},{"start":791,"end":794,"marker":"bibr","target":"#b5"}]},{"text":"The main goal of this paper is to verify whether modelling image classification problem using a SRL language can improve the accuracy of a classical k-nearest neighbour (k-NN) approach. The focus is not on proposing a new image classification algorithm, but on showing that a SRL model can improve a classical classification method. We adopted the probabilistic logic ProbLog [7] as SRL model to describe the structure of the probabilistic network arising from the abstraction process we adopted to represent an image collection. The experimental results obtained from a real world dataset confirmed the validity of the proposed approach.","refs":[{"start":376,"end":379,"marker":"bibr","target":"#b6"}]}]},{"title":"Related Work","paragraphs":[{"text":"The increasing interest in the last years about the problem of image classification provided a lot of models. Here we briefly review some of them that may be related to our proposed statistical relational approach. The Bag of Words model [8] (BOW) has been inherited from the area of document analysis. It describes each image as a collection of features, without analysing neither its geometric structure nor features' position in the image. Given a shared vocabulary for the allowed features, named codebook, each image is then encoded as a distribution of codebook names. In order to classify new images, Probabilistic Latent Semantic Analysis (pLSA) [9], Latent Dirichlet Allocation (DLA) [10] or Vector Space Model (VSM) [11] may be used. The limitation of this approach is that all the spatial relationships among the features are ignored. Part-based methods [12] try to solve the problems of the BOW model. The main idea is to consider an object consisting of a set of N parts connected to each other and describing each component considering specific geometric characteristics (e.g., the constellation model [13]). The adopted feature model, such as color histogram, correlogram and sift, will be enriched by considering location and dimension properties. In the training phase the system induces a model that will be able to classify objects considering their parts. In order to model the relationships among the images, or among the objects within an image, a graph based representation is usually adopted. This structure provides a powerful model [14], where nodes store local information while the edges encode the spatial relationships, with a companion set of tools, inherited from the graph theory, useful to manage and inspect it.","refs":[{"start":238,"end":241,"marker":"bibr","target":"#b7"},{"start":654,"end":657,"marker":"bibr","target":"#b8"},{"start":693,"end":697,"marker":"bibr","target":"#b9"},{"start":726,"end":730,"marker":"bibr","target":"#b10"},{"start":865,"end":869,"marker":"bibr","target":"#b11"},{"start":1116,"end":1120,"marker":"bibr","target":"#b12"},{"start":1558,"end":1562,"marker":"bibr","target":"#b13"}]},{"text":"Given a graph based representation, collective classification methods [15] can be used. In collective classification relationships among objects must be taken into account in order to enhance the predictive accuracy of the model: the labelling of an object should depend on the labels of its neighbours. Weighted-Vote Relational Neighbor (WVRN) [16], estimates the probability of a class given a node by summing the probabilities of nodes in the neighbourhood of the same class. Class-Distribution Relational Neighbour (CDRN) [17] uses a vectorial representation of the neighbours by assigning to each node v of known label a vector whose elements represent the probability that a neighbour node have a given label. The Network-Only Bayes (NOB) classifier [18] adopts a naive Bayes approach to compute the label probabilities of a node, assumed to be independent, conditioned on the labels of its neighbours. Finally, the Network-Only Link-Based (NOLB) classifier [19] learns a multiclass logistic regression model using the label distribution in the neighbourhood of nodes with known labels. These methods are based on a link-based classification, but they consider the data as already represented as a network, and then relegating their use for dataset containing explicit relations. While, one of our goals is to elicit the relationships between objects, thereby transforming a set of i.i.d. data in a network and describe the connections found using a relational logic formalism.","refs":[{"start":70,"end":74,"marker":"bibr","target":"#b14"},{"start":345,"end":349,"marker":"bibr","target":"#b15"},{"start":526,"end":530,"marker":"bibr","target":"#b16"},{"start":756,"end":760,"marker":"bibr","target":"#b17"},{"start":964,"end":968,"marker":"bibr","target":"#b18"}]}]},{"title":"Image Correlograms","paragraphs":[{"text":"There is a large number of features that may be used to describe an image. In [6] has been presented an approach, named color correlogram, that combines both global and local image information. This statistics describes how pixels with a given color are spatially distributed in an image, and it is generally more accurate and effective than histogram-based methods. However the time required to compute this statistics may be very high. In [20] has been proposed a new sampling method to approximate the distribution of correlograms proving that the computational time to compute the statistics may be reduce by taking high the similarity based accuracy. In this paper we used image correlograms as a feature representation, even if the generality of the proposed relational approach allows the adoption of any kind of feature.","refs":[{"start":78,"end":81,"marker":"bibr","target":"#b5"},{"start":441,"end":445,"marker":"bibr","target":"#b19"}]},{"text":"A correlogram is defined as a table indexed by color pairs, where the k-th entry for the component (i, j) specifies the probability of finding a pixel whose color is j away k pixel from a pixel whose color is i, where k is a distance chosen from a set D.","refs":[]},{"text":"Let I be an n 1 × n 2 image, whose colors are quantised into m classes C = {c 1 , ...., c m } (bins). Given a pixel p xy ∈ I, I (p) denotes its color and","refs":[]},{"text":"In the following, we assume to use the L ∞ -norm to measure the distance between two pixels. In particular, given p x1y1 and p x2y2 two pixels, their distance is computed as follows [6]:","refs":[{"start":182,"end":185,"marker":"bibr","target":"#b5"}]},{"text":"The histogram h of an image I is defined as","refs":[]},{"text":"Let d ∈ D be a distance, the correlogram of an image I is defined as follows:","refs":[]},{"text":"where c i , c j ∈ C are two color classes. The size of the color correlogram matrix is |C| 2 . A specialisation is the autocorrelogram defined as α","refs":[]}]},{"title":"The Probabilistic Logic ProbLog","paragraphs":[{"text":"Among the many languages considered in SRL there are some based on Logic Programming such as ICL [21], PRISM [22], CLP(BN) [23] and ProbLog [7].","refs":[{"start":97,"end":101,"marker":"bibr","target":"#b20"},{"start":109,"end":113,"marker":"bibr","target":"#b21"},{"start":123,"end":127,"marker":"bibr","target":"#b22"},{"start":140,"end":143,"marker":"bibr","target":"#b6"}]},{"text":"The representation and use of probability theory makes SRL algorithms suitable for combining domain knowledge and data, expressing relationships, avoiding overfitting a model to training data, and learning from incomplete datasets. The probabilistic formalism provides a natural treatment for the stochastic nature of some complex domains. As for classical probabilistic graphical models [24], such as Bayesian networks and Markov networks, statistical relational languages exploit the structure underlying many distributions we want to encode. The same structure often allows the distribution to be effectively used for inference, answering queries using the distribution as a model of the world. Finally, the SRL framework facilitates the effective learning from data of models providing a good approximation to a past experience. ProbLog is a probabilistic framework that extends Prolog with probabilistic facts and answers several kinds of probabilistic queries, which has been used for learning in the context of large networks where edges are labelled with probabilities. The ProbLog's semantics is an instance of the distributional semantics defined in [25]. A ProbLog program defines a distribution over all its possible nonprobabilistic subprograms. Facts are labelled with probabilities and treated as mutually independent random variables indicating whether or not the corresponding fact belongs to a randomly sampled program. The success probability of a query is defined as the probability that it succeeds in such a random subprogram.","refs":[{"start":388,"end":392,"marker":"bibr","target":"#b23"},{"start":1160,"end":1164,"marker":"bibr","target":"#b24"}]},{"text":"A ProbLog program T = p 1 :: c 1 , . . . , p n :: c n ∪ BK defines a probability distribution over subprograms L ⊆ LT = {c 1 , . . . , c n }:","refs":[]},{"text":"While in Prolog the result of query is binary, i.e. success or failure, in ProbLog the result of a query is the probability of success, i.e. the probability that the query succeeds in a random sample. In particular, the success probability P s (q|T ) of a query q in a ProbLog program T is defined as","refs":[]},{"text":"where P (q|L) = 1 if there exists a θ substitution such that L ∪ BK |= qθ, and P (q|L) = 0 otherwise.","refs":[]},{"text":"The explanation probability P x (q|T ) is defined as the probability of the most likely explanation of the proof of the query q:","refs":[]},{"text":"where E(q) is the set of all explanations for the query q. The ProbLog framework includes different inference methods and its implementation is based on the use of tries and reduced ordered binary decision diagrams (ROBDDs). The execution of ProbLog programs uses SLD-resolution to collect all the proofs for a query. In the case of exact inference, for each successful proof of the query, the probabilistic facts used in the proof are gathered and BDDs are used to solve the disjoint sum problem and to obtain the correct probability of the query. In particular, since probability computation and learning in ProbLog are based on propositional logic, compressing Boolean (propositional) functions by BDDs accelerates them. Another solution adopted to overcome the combinatorial explosion of the BDD in ProbLog is to use approximation methods for inference as those proposed in [26,27].","refs":[{"start":878,"end":882,"marker":"bibr","target":"#b25"},{"start":882,"end":885,"marker":"bibr","target":"#b26"}]}]},{"title":"Image Network Representation","paragraphs":[{"text":"The methods usually used for the image classification task assume the images as independent and identically distributed. Here, we want to show that adopting a SRL language modelling a complex network may be used to solve the image classification problem providing better results than those obtained with classical methods. The first step corresponds to choose a representation language for the images. This is generally done by extracting for each image x ∈ D some features. In this paper, we describe each image I with its corresponding correlogram statistics γ","refs":[]},{"text":"ci,cj (I), computed as reported in Equation 1. After the feature extraction process, a pairwise image comparison approach should be defined. In particular, it is necessary to define a measure able to identify similar images.","refs":[{"start":44,"end":45,"marker":"formula","target":"#formula_3"}]},{"text":"Given a set of K classes, denoted as C k , a discriminant [28] is a function that takes as input an observation x ∈ D and classifies it as belonging to one of the K classes. In the general case of K > 2 classes, we consider a single K-class discriminant comprising K linear functions φ k and then assigning an observation x to a class C k if φ k (x) > φ j (x) for all j = k.","refs":[{"start":58,"end":62,"marker":"bibr","target":"#b27"}]},{"text":"The well known k-NN technique represents one of the most adopted nonparametric approaches to estimate the class of a given unseen observation. In particular, to classify a new observation, the k-NN method identifies k nearest observations from the training data and then assigns the new observation to the class having the largest number of representatives among this set [28]. The particular case of k = 1 corresponds to the nearest neighbour rule, where a test observation is simply assigned to the same class as the nearest observation from the training set. In order to compute the nearest neighbours, a distance function between two observations must be used. For instance, the distance between two observations x, y ∈ D may be given by dist(x, y) = ( i |x iy i | p ) 1/p , where x i and y i are the components of the representation of the observations x and y.","refs":[{"start":372,"end":376,"marker":"bibr","target":"#b27"}]},{"text":"The number of edges in the network built in this way grows as the degree of each node increases. The degree of a node x corresponds to the number of edges connecting similar images to x. Since we have to build a probabilistic network (edges labelled with probabilities), given a distance function d, returning a value between 0 and 1, it is possible to build the network, where each image is represented as a node, and each edge connecting two similar images x and y is labelled with the value 1d(x,y).","refs":[{"start":494,"end":499,"marker":"figure","target":"#fig_2"},{"start":499,"end":500,"marker":"figure","target":null}]},{"text":"In a classical approach, where there is not an underlying network structure, computing the similarity between two images x and y corresponds to apply the distance function only to x and y. In this way we consider x and y as independent, ignoring, for instance, the fact that there may be an image z that is similar to both x and y. The idea is to have a network that allows us to exploit all the available connections among the images belonging to the data set.","refs":[]},{"text":"After having explained how to build the probabilistic network, it is natural to convert it to a ProbLog program as that reported in the Example 1. In particular, each network's edge corresponds to a probabilistic fact p::edge/2, where p is the similarity degree between the two images that are the arguments of the edge predicate. Hence, given the ProbLog program, it is possible to solve a query q asking the similarity degree between two nodes by adopting an inference method. In order to exploit the relations among the images, the ProbLog program has been enriched with the following definition.","refs":[]},{"text":"The query path(x,y,k) is true for all the possible paths, with a length at most equal to k, starting from the node x and ending to the node y. Since the probability of this query is calculated collecting all its proofs (all the possible paths) then it corresponds to the similarity between two nodes computed considering the subnetwork containing the nodes related to those involved in the query. The value of the k parameter expresses the dimension of the considered subnetwork: k=1 corresponds to a classical approach. ","refs":[]}]},{"title":"Experiments","paragraphs":[{"text":"In this section we present some experimental results to asses whether the SRL proposed approach works better than the classical one. In order to do this, the Caltech 101 dataset [29] provided by the California Institute of Technology has been used. We chosen five classes (Airplaines, Faces, Watch, Motorbikes and Leopards) and 30 images have been randomly selected for each class obtaining a dataset of 150 instances. Each image has been represented by the correlograms calculated on given distances corresponding to 1, 3, 5 or 7. Having a correlogrambased representation, the distance between two images, x and y, is computed as follows, as reported in [6]:","refs":[{"start":178,"end":182,"marker":"bibr","target":"#b28"},{"start":655,"end":658,"marker":"bibr","target":"#b5"}]},{"text":"Let D be a set of images labelled with one of the k classes Q = {q 1 , . . . , q k }, and #q i = |{x ∈ D|cl(x) = q i }|, where cl is a function returning the class of an image.","refs":[]},{"text":"To rank images with respect to a query image q the similarity is calculated as 1d c (x i , q). As in classical k-NN-based approaches, for each query image q, the error of the ranking result on the first n ranked images is calculated using the following formula","refs":[]},{"text":"where 1 is the indicator function that evaluates to 1 if cl(x t ) = cl(q) and 0 otherwise, and x t are the first n elements of the ranked images list with respect to q. In our experiment #q i = 30 for each of the five selected classes. Furthermore, the error, computed using the Equation 3, refers to the first 29 (n) ranked images. In particular, when err n q = 0 means that all the first n images in the ranked images list have the same class of q.","refs":[{"start":288,"end":289,"marker":"formula","target":"#formula_12"}]},{"text":"In order to evaluate the SRL based approach, its results are compared to that obtained by the classical method. As already said, the probabilistic network is built using the function reported in Equation 2. We have to decide how much edges, connecting similar images, to insert in the network for each node (image). Given an image x, the α parameter represents the number of the best similar images to x (its neighbours). For each of these neighbours the similarity probability is computed and added to the probabilistic fact edge/2 in the ProbLog program. In the following experiments a value of α set to 2, 3, 4 or 5 has been used.","refs":[{"start":204,"end":205,"marker":"formula","target":"#formula_10"}]},{"text":"Figure 1 shows (on the left) the probabilistic network constructed by setting to 4 the number of neighbours for each image. In the graph all the nodes filled with the same color, from a grayscale, correspond to the same class. A portion of the network (on the right) with the nodes represented by the corresponding image is depicted in the same figure. We can note that although only the four most similar images are considered as neighbourhood, the network complexity is evident. The application of a classical approach neglects all the relationships shown by the network resulting in a less accurate classification result.","refs":[{"start":7,"end":8,"marker":"figure","target":"#fig_0"}]},{"text":"The validation of the proposed SRL approch has been done by comparing each image with all the others and ranking the results adopting the ProbLog exact inference. Using the α parameter involves the insertion of only a subset of all the possible connections. Indeed, in the case of few connections, given two image x and y, the ProbLog success probability for path(x,y,1) may be 0, because there may be no link between x and y in the network. The absence of the edge between x and y means that y is not present among the first best α similar images (and viceversa). A similar problem concerns the choice of the parameter k in the query path/3.","refs":[]},{"text":"In order to solve the problem of the 0 estimated probabilities, due to connections' absence, the error has been computed considering the first images in the ranked images list whose probability is not equal to 0. In particular, let z < n the position in the ranked images list such that all the images x i , i ≤ z, have The errors obtained in the first z images has been used to estimate the total error probability over all the n > z images as follows:","refs":[]},{"text":"where","refs":[]},{"text":"In particular in Equation 4, k-1 k (nz) represents the expected number of errors in the remaining (nz) images, where we assumed that classifications are uniformly distributed among the k classes ( k-1 k is the probability to incorrectly classify an image).","refs":[{"start":26,"end":27,"marker":"formula","target":"#formula_14"}]},{"text":"Table 1 displays the result obtained on the Airplanes class. In the column headings, d is the distance used for computing the correlogram matrix, α is the number of neighbors selected for each image to build network, and err n q is the number of errors obtained in the first 29 best ranked results with a classical approach. For each value of the k parameter of the path query solved by ProbLog, z is the position in the ranked list corresponding to the last image with rank different to 0, srlerr z q is the number of error in the first z positions, and srlerr z q is the expected number of errors in the first 29 positions. As we can see, the obtained errors with the SRL approach is inversely proportional to both the α and k parameters, and they are fewer than those obtained with the classical method. This result proves the validity of our approach and the advantage of using an SRL language. Table 2 shows the results obtained on the other classes: Faces, Watch, Motorbikes and Leopard, where we take fixed the parameter k to 4. The errors obtained with other classes confirm the observations made on the class Airplanes. The computational time of the proposed approach is very high when compared to a classical k-NN. This is due to the ProbLog inference procedure that represents its bottleneck. z serr z q serr n q err n q z serr z q serr n q err n q z serr z q serr n q err n q z serr z q serr n q    ","refs":[{"start":6,"end":7,"marker":"table","target":"#tab_0"},{"start":905,"end":906,"marker":"table","target":"#tab_1"}]}]},{"title":"Conclusion","paragraphs":[{"text":"In this paper we considered the image classification problem. In particular our goal was to see if it is possible to increase the classification accuracy obtained using a classical k-NN approach, where the relationships among the images are ignored. In order to capture the connections among images we have used a SRL language. Connections between two images have been weighted with a probability. Experiments have been conducted on a real-word data set comparing the classical method with respect to our presented approach. The results show the validity of the proposed method.","refs":[]}]}],"tables":{"tab_0":{"heading":"Table 1 .","description":"Results on the Airplanes class","rows":[["d α err n q","k=2 z srlerr z q srlerr","n q","k=3 z srlerr z q srlerr","n q","k=4 z srlerr z q srlerr","n q"],["","2","","8.40 0.10","0.60 16.67 0.19","0.45 25.43 0.27","0.34"],["1","3 4","0.41","12.73 0.13 20.73 0.17","0.51 25.20 0.27 0.35 28.93 0.19","0.34 29.00 0.27 0.19 29.00 0.12","0.27 0.12"],["","5","","25.10 0.22","0.30 29.00 0.19","0.19 29.00 0.16","0.16"],["","2","","7.40 0.06","0.61 14.17 0.10","0.46 20.63 0.15","0.34"],["3","3 4","0.41","13.90 0.14 18.63 0.22","0.48 25.90 0.22 0.42 28.77 0.28","0.28 28.90 0.21 0.28 29.00 0.26","0.21 0.26"],["","5","","23.83 0.25","0.35 29.00 0.25","0.25 29.00 0.22","0.22"],["","2","","7.73 0.06","0.60 14.27 0.12","0.47 22.17 0.21","0.35"],["5","3 4","0.42","12.43 0.13 19.37 0.18","0.51 24.30 0.24 0.38 28.90 0.21","0.33 28.97 0.27 0.21 29.00 0.18","0.27 0.18"],["","5","","24.70 0.24","0.32 29.00 0.22","0.22 29.00 0.20","0.20"],["","2","","7.27 0.07","0.62 12.97 0.13","0.50 19.17 0.24","0.43"],["7","3 4","0.41","12.87 0.13 20.97 0.21","0.50 24.73 0.27 0.38 29.00 0.22","0.34 28.87 0.29 0.22 29.00 0.16","0.29 0.16"],["","5","","25.27 0.28","0.35 29.00 0.24","0.24 29.00 0.19","0.19"]]},"tab_1":{"heading":"Table 2 .","description":"Results on the Faces, Watch, Motorbikes and Leopard classes with","rows":[["k= 4"]]}},"abstract":{"title":"Abstract","paragraphs":[{"text":"Digital Libraries contain collections of multimedia objects providing services for the management, sharing and retrieval. Involved objects have two levels of complexity: the former refers to the inner object complexity while the latter takes into account the implicit/explicit relationships among objects. Traditional machine learning classifiers do not consider the relationships among objects assuming them independent and identically distributed. Recently, link-based classification methods have been proposed, that try to classify objects exploiting their relationships (links). In this paper, we deal with objects corresponding to digital images, even if the proposed approach can be naturally applied to different kind of multimedia objects. Relationships can be expressed among the features of the same image or among features belonging to different images. The aim of this work is to verify whether a link-based classifier based on a Statistical Relational Learning (SRL) language can improve the accuracy of a classical k-nearest neighbour approach. Experiments will show that the modelling of the relationships in a real-word dataset using a SRL model reduces the classification error.","refs":[]}]}}