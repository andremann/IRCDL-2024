{"bibliography":{"title":"WibNED: Wikipedia Based Named Entity Disambiguation","authors":[{"person_name":{"surname":"Gentile","first_name":"Anna"},"affiliations":[{"department":"Dipartimento di Informatica","institution":"Università di Bari Via E","laboratory":null}],"email":"al.gentile@di.uniba.it"},{"person_name":{"surname":"Basile","first_name":"Pierpaolo"},"affiliations":[{"department":"Dipartimento di Informatica","institution":"Università di Bari Via E","laboratory":null}],"email":"basilepp@di.uniba.it"},{"person_name":{"surname":"Semeraro","first_name":"Giovanni"},"affiliations":[{"department":"Dipartimento di Informatica","institution":"Università di Bari Via E","laboratory":null}],"email":"semeraro@di.uniba.it"}],"date":null,"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"keywords":[],"citations":{"b0":{"title":"An adapted lesk algorithm for word sense disambiguation using wordnet","authors":[{"person_name":{"surname":"Banerjee","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Pedersen","first_name":"T"},"affiliations":[],"email":null}],"date":{"year":"2002","month":null,"day":null},"ids":null,"target":null,"publisher":"Springer-Verlag","journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":136,"to_page":145}}},"b1":{"title":"Meta multilanguage text analyzer","authors":[{"person_name":{"surname":"Basile","first_name":"P"},"affiliations":[],"email":null},{"person_name":{"surname":"De Gemmis","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Gentile","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Iaquinta","first_name":"L"},"affiliations":[],"email":null},{"person_name":{"surname":"Lops","first_name":"P"},"affiliations":[],"email":null},{"person_name":{"surname":"Semeraro","first_name":"G"},"affiliations":[],"email":null}],"date":{"year":"2008","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":137,"to_page":140}}},"b2":{"title":"Using encyclopedic knowledge for named entity disambiguation","authors":[{"person_name":{"surname":"Bunescu","first_name":"R"},"affiliations":[],"email":null},{"person_name":{"surname":"Pasca","first_name":"M"},"affiliations":[],"email":null}],"date":{"year":"2006","month":null,"day":null},"ids":null,"target":null,"publisher":"ACL","journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":9,"to_page":16}}},"b3":{"title":"Large-scale named entity disambiguation based on Wikipedia data","authors":[{"person_name":{"surname":"Cucerzan","first_name":"S"},"affiliations":[],"email":null}],"date":{"year":"2007","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":708,"to_page":716}}},"b4":{"title":"WordNet: An Electronic Lexical Database","authors":[{"person_name":{"surname":"Fellbaum","first_name":"C"},"affiliations":[],"email":null}],"date":{"year":"1998","month":null,"day":null},"ids":null,"target":null,"publisher":"MIT Press","journal":null,"series":null,"scope":null},"b5":{"title":"Uber sinn und bedeutung","authors":[{"person_name":{"surname":"Frege","first_name":"G"},"affiliations":[],"email":null}],"date":{"year":"1892","month":null,"day":null},"ids":null,"target":null,"publisher":"Vandenhoeck & Ruprecht","journal":null,"series":null,"scope":{"volume":4,"pages":null}},"b6":{"title":"Lexical and semantic resources for nlp: From words to meanings","authors":[{"person_name":{"surname":"Gentile","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Basile","first_name":"P"},"affiliations":[],"email":null},{"person_name":{"surname":"Iaquinta","first_name":"L"},"affiliations":[],"email":null},{"person_name":{"surname":"Semeraro","first_name":"G"},"affiliations":[],"email":null}],"date":{"year":"2008","month":null,"day":null},"ids":null,"target":null,"publisher":"Springer","journal":null,"series":"Lecture Notes in Computer Science","scope":{"volume":5179,"pages":{"from_page":277,"to_page":284}}},"b7":{"title":"Message understanding conference-6: A brief history","authors":[{"person_name":{"surname":"Grishman","first_name":"R"},"affiliations":[],"email":null},{"person_name":{"surname":"Sundheim","first_name":"B"},"affiliations":[],"email":null}],"date":{"year":"1996","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":466,"to_page":471}}},"b8":{"title":"Exploiting wikipedia as external knowledge for named entity recognition","authors":[{"person_name":{"surname":"Kazama","first_name":"J"},"affiliations":[],"email":null},{"person_name":{"surname":"Torisawa","first_name":"K"},"affiliations":[],"email":null}],"date":{"year":"2007","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":698,"to_page":707}}},"b9":{"title":"Fast methods for kernel-based text analysis","authors":[{"person_name":{"surname":"Kudo","first_name":"T"},"affiliations":[],"email":null},{"person_name":{"surname":"Matsumoto","first_name":"Y"},"affiliations":[],"email":null}],"date":{"year":"2003","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":24,"to_page":31}}},"b10":{"title":"Automatic sense disambiguation using machine readable dictionaries: How to tell a pine cone from an ice cream cone","authors":[{"person_name":{"surname":"Lesk","first_name":"M"},"affiliations":[],"email":null}],"date":{"year":"1986","month":null,"day":null},"ids":null,"target":null,"publisher":"ACM","journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":24,"to_page":26}}},"b11":{"title":"Exploiting semantic role labeling, wordnet and wikipedia for coreference resolution","authors":[{"person_name":{"surname":"Ponzetto","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Strube","first_name":"M"},"affiliations":[],"email":null}],"date":{"year":"2006","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b12":{"title":"Wikirelate! computing semantic relatedness using wikipedia","authors":[{"person_name":{"surname":"Strube","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Ponzetto","first_name":"S"},"affiliations":[],"email":null}],"date":{"year":"2006","month":null,"day":null},"ids":null,"target":null,"publisher":"AAAI Press","journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":1419,"to_page":1424}}},"b13":{"title":"A proposal to automatically build and maintain gazetteers for named entity recognition by using wikipedia","authors":[{"person_name":{"surname":"Toral","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Munoz","first_name":"R"},"affiliations":[],"email":null}],"date":{"year":"2006","month":null,"day":null},"ids":null,"target":null,"publisher":"ACL","journal":null,"series":null,"scope":null},"b14":{"title":"Entity ranking in wikipedia","authors":[{"person_name":{"surname":"Vercoustre","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Thom","first_name":"J"},"affiliations":[],"email":null},{"person_name":{"surname":"Pehcevski","first_name":"J"},"affiliations":[],"email":null}],"date":{"year":"2008","month":null,"day":null},"ids":null,"target":null,"publisher":"ACM","journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":1101,"to_page":1106}}},"b15":{"title":"Analyzing and accessing wikipedia as a lexical semantic resource","authors":[{"person_name":{"surname":"Zesch","first_name":"T"},"affiliations":[],"email":null},{"person_name":{"surname":"Gurevych","first_name":"I"},"affiliations":[],"email":null},{"person_name":{"surname":"Mühlhäuser","first_name":"M"},"affiliations":[],"email":null}],"date":{"year":"2007","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null}},"sections":[{"title":"Introduction","paragraphs":[{"text":"A proper name is a word or a list of words that refers to a real world object. According to Frege, a proper name has a reference (Bedeutung) and a sense (Sinn) [6]. The reference is the object that the expression refers to (different linguistic expressions can have the same reference). The sense is the cognitive significance, the way by which the referent is presented. Linguistic Expressions with the same reference may have different senses, so it is necessary to disambiguate between them. In Natural Language Processing field Named Entity Disambiguation is the task that aims to solve this issue. NLP operations include text normalization, tokenization, stop words elimination, stemming, Part Of Speech tagging, lemmatization. Further steps, as Word Sense Disambiguation (WSD) or Named Entity Recognition (NER), are aimed at enriching texts with semantic information. Named Entity Disambiguation (NED) is the procedure that solves the correspondence between real-world entities and mentions within text. The proposed approach automatically associates each entity in a text with a unique identifier, a URI from Wikipedia1 , which is used as an \"entity-provider\". The contribution of this work is twofold: firstly a novel knowledge based approach for NED is proposed; secondly the work shows a method to build a testbed dataset from Wikipedia. The suggested solution is completely knowledge-based, with the advantage that no training data is needed: indeed, manually annotated data for this task is not easily available, so acquiring such data can be expensive.","refs":[{"start":160,"end":163,"marker":"bibr","target":"#b5"},{"start":1125,"end":1126,"marker":null,"target":"#foot_0"}]},{"text":"The work is structured as follows: Section 2 proposes an overview of Named Entity Disambiguation task, together with a description of available solutions to exploit Wikipedia for the issue of Named Entities. Section 3 presents the proposed Wikipedia based Named Entity Disambiguation algorithm, named WibNED. Section 4 presents the dataset used for experiments, which are then described in section 5. Conclusions close the paper.","refs":[]}]},{"title":"Related Work","paragraphs":[{"text":"Named Entity Disambiguation is the problem of mapping mentions of entities in a text with the object they are referencing. It is a step further from Named Entity Recognition (NER), which involves the identification and classification of so-called named entities: expressions that refer to people, places, organizations, products, companies, and even dates, times, or monetary amounts, as stated in the Message Understanding Conferences (MUC) [8]. The NED process aims to create a mapping between the surface form of an entity and its unique dictionary meaning. It can be assumed to have a dictionary of all possible entity entries. In this work we use Wikipedia as such a dictionary. Many studies that exploit Wikipedia as a knowledge source have recently emerged [12,13,16]. In particular, Wikipedia turned to be very useful for the problem of Named Entities due to its greater coverage than other popular resources, such as WordNet [5] that, resembling more to a dictionary, has little coverage on named entities [13]. Lots of previous works exploited Wikipedia for the task of NER, e.g. to extract gazetteers [14] or as an external knowledge of features to use in a Conditional Random Field NER-tagger [9], to improve entity ranking in the field of Information Retrieval [15]. On the other hand, little has been carried out on the field of NED. The most related works on NED based on Wikipedia are those by Bunescu and Pasca [3] and Cucerzan [4]. Bunescu and Pasca consider the problem of NED as a ranking problem. The authors define a scoring function that takes into account the standard cosine similarity between words in the context of the query and words in the page content of Wikipedia entries, together with correlations between pages learned from the structure of the knowledge source (mostly using Wikipedia Categories assigned to the pages). Cucerzan proposes a very similar approach: the vectorial representation of the document is compared with the vectorial representation of the Wikipedia entities. In more details the proposed system represents each entity of Wikipedia as an extended vector with two principal components, corresponding to context and category information; then it builds the same kind of vector for each document. The disambiguation process consists of maximizing the Context Agreement, that is the overlap between the document vector for the entity to disambiguate and each possible entity vector. Both described works are based on the Vector Space Model, which means that a pre-computation on the Wikipedia knowledge resource is needed to build the vector representation. The proposed solution, differently to previous methods, exploits words in the context of an entity in a simple way, calculating the gloss overlapping between context and dictionary entries.","refs":[{"start":442,"end":445,"marker":"bibr","target":"#b7"},{"start":764,"end":768,"marker":"bibr","target":"#b11"},{"start":768,"end":771,"marker":"bibr","target":"#b12"},{"start":771,"end":774,"marker":"bibr","target":"#b15"},{"start":934,"end":937,"marker":"bibr","target":"#b4"},{"start":1015,"end":1019,"marker":"bibr","target":"#b12"},{"start":1112,"end":1116,"marker":"bibr","target":"#b13"},{"start":1205,"end":1208,"marker":"bibr","target":"#b8"},{"start":1274,"end":1278,"marker":"bibr","target":"#b14"},{"start":1428,"end":1431,"marker":"bibr","target":"#b2"},{"start":1445,"end":1448,"marker":"bibr","target":"#b3"}]},{"text":"For the task of NED little resources and benchmark data are publicly available. On the other hand lots of data is available for the task of Named Entity Recognition: multilingual benchmarking and evaluations have been performed within several events, such as the Message Understanding Conferences (MUC) series organized by DARPA, the International Conference on Language Resources and Evaluation (LREC), the Computational Natural Language Learning (CoNLL) workshops, the Automatic Content Extraction (ACE) series organized by NIST, the Multilingual Entity Task Conference (MET), the Information Retrieval and Extraction Exercise (IREX). The problem with data shared within these events is that entities are not labelled with a URI, but only classified within a set of predefined entity classes, which means that is not directly reusable for the task of NED. Some useful data for NED has been provided by Cucerzan, but the dataset only contains information about Named Entities and not all the text, so it is not useful for the purpose of this work. Moreover we wanted to evaluate our algorithm for the italian language and the available dataset is in english. For these reasons a specific dataset has been built to validate the proposal, automatically extracted from italian Wikipedia articles containing ambiguous entities, maintaing both entities and other words within the text.","refs":[]}]},{"title":"WibNED: Wikipedia based Named Entity Disambiguation","paragraphs":[{"text":"The goal of a WSD algorithm consists in assigning a word w i occurring in a document d with its appropriate meaning or sense s, by exploiting the context C in which w i is found. The context C for w i is defined as a set of words that precede and follow w i . The sense s is selected from a predefined set of possibilities, usually known as sense inventory.","refs":[]},{"text":"The Lesk algorithm is a classical algorithm for Word Sense Disambiguation for all words in unrestricted text. It was introduced by Mike Lesk in 1986 [11]. The basic assumption is that words in a given neighbourhood will probably share a common topic. Apart from knowledge about the context (the immediate surrounding words), the algorithm requires a machine readable dictionary, with an entry for each possible sense for a word. The original algorithm takes into account words pairwise and computes overlap among sense definitions: the sense pair with the highest overlap score is chosen.","refs":[{"start":149,"end":153,"marker":"bibr","target":"#b10"}]},{"text":"The proposed algorithm, named WibNED, is an adaptation of Lesk dictionary-based WSD algorithm [1]. In the WibNED algorithm the words to disambiguate are only those representing an Entity.","refs":[{"start":94,"end":97,"marker":"bibr","target":"#b0"}]},{"text":"WibNED takes as input a document d = {w 1 , . . . , w j , e j+1 , w j+2 , . . . , w h , e h+1 , w h+2 , . . . } and returns a list of Wikipedia URIs X = {s 1 , s 2 , . . . , s k } in which each element s i is obtained by disambiguating the target entity e i on the ground of the information obtained from Wikipedia for each candidate URI (Wikipedia page content of the URI) and words in the context C of e i . We define the context C of the target entity e i to be a window of n words to the left and another n words to the right, for a total of 2n surrounding words. In the current version of the algorithm if other entities occur in the context of the target entity, they are considered as words and not as entities.","refs":[]},{"text":"Algorithm 1 describes the complete procedure for the disambiguation of entities. The input for the algorithm is an ordered list W W = (w 1 , . . . , w j , e j+1 , w j+2 , . . . , w h , e h+1 , w h+2 , . . .) of words in a document, processed with a NLP tool: w i are common words while e i are tagged as Named Entities.","refs":[]},{"text":"The NLP tool used to process the document collection is META [2], that performs the following operations:","refs":[{"start":61,"end":64,"marker":"bibr","target":"#b1"}]},{"text":"tokenization; part of speech (POS) tagging; stop words elimination; word sense disambiguation by using WordNet; -Entity Recognition performed with Yamcha [10], a NER annotator which uses a Support Vector Machine techniques.","refs":[{"start":154,"end":158,"marker":"bibr","target":"#b9"}]},{"text":"Each document of the collection is then transformed in an ordered list of common words w i and Named Entities e i : W = (w 1 , . . . , w j , e j+1 , w j+2 , . . . , w h , e h+1 , w h+2 , . . .)","refs":[]},{"text":"The list W is the input for the main procedure, named WibNED, that finds the proper Wikipedia URI for each polysemous entity e i in W . WibNED uses several subprocedures. The subprocedure QueryWiki finds all possible Wikipedia pages answering the query e i . Each element of the returning set contains the page URI and a short textual description of the page. The subprocedure pickPage computes the overlap between the context of the target entity and the description obtained from the Wikipedia page for each candidate sense. It returns the candidate entity which maximizes the overlap.","refs":[]}]},{"title":"Dataset","paragraphs":[{"text":"The dataset used for Experiments consists of 752 documents extracted from italian Wikipedia. The evaluation of a NED algorithm which gives as output Wikipedia URIs needs a dataset containing ambigous entites, already tagged with the correct URI belonging to Wikipedia.","refs":[]},{"text":"We implemented a procedure to build an automatic annotated corpus, starting from a list of ambigous entities (i.e. entities whose surface form has an associated Disambiguation Page2 in Wikipedia). We used a list of 100 ambiguos surface form, taken from italian Wikipedia, A = (a 1 , . . . , a 100 ) . For each a i we accessed the related Disambiguation Page on Wikipedia and we picked the most significative senses for a i , s ai = (s 1 , . . . , s j ), with j ≤ 4, considering only senses referring to Named Entities and using heuristics to reject poor senses. For example, considering the disambiguation page for the italian word \"mosca\", the sense referring to Mosca (Moskva), the capital and the largest city of Russia, has been stored whereas the sense referring to Muscomorpha, a group of flies, has been ignored because it is a common noun word. Starting from S = (s a1 , . . . , s at , . . . , s a100 ) for each s j in s at we choosed up to 5 generic Wikipedia articles that contain at least a link to the sense s j . Each article has been processed using META [2] and has been stored as a single file, using a IOB like format, similar to CoNLL 2003 Named Entity Recognition corpus 3 : each row contains a token, its Part Of Speech tag, its lemma and a final tag which is valued as O if the token has not been recognized as an entity, B-<Wikipedia URI for the entity> if the token is the beginning of an entity and I-<Wikipedia URI for the entity> if the token continues an entity.","refs":[{"start":180,"end":181,"marker":null,"target":"#foot_2"},{"start":1069,"end":1072,"marker":"bibr","target":"#b1"}]},{"text":"Each text contains on the average 89 entities. In table 1 we report a piece of a document to show an example of text. It is a document included in the corpus, specifically it is an article taken from italian Wikipedia about The Beastie Boys. In this piece of text there is only one entity, represented by the two words John Berry. Together with entity annotations, the corpus also contains the Part Of Speech and the stem for each word, thus allowing to refine and improve computation over the corpus. ","refs":[]}]},{"title":"Experiments","paragraphs":[{"text":"We performed the experiment following the methods generally used to evaluate Word Sense Disambiguation (WSD) algorithms. The entity WSD is not an end in itself but rather an intermediate task which contributes to an overall task such as information retrieval, ontology building, etc. This opens the possibility of two types of evaluation for WSD work (using terminology borrowed from biology): in vitro evaluation, where WSD systems are tested independently of any application, using specially constructed benchmarks; and evaluation in vivo, where, rather then being evaluated in isolation, results are evaluated in terms of their contribution to the overall performance of a system designed for a particular application (e.g., information retrieval). In this instance we adopt in vitro evaluation in order to evaluate the accuracy and the potentialities of the algorithm in an independent way. In vitro evaluation, despite its artificiality, enables close examination of the problems plaguing a given task. In its most basic form this type of evaluation involves comparison of the output of a system for a given input, using measures such as precision and recall. Alternatively, in vitro evaluation can focus on study of the behavior and performance of systems on a series of test suites representing the range of linguistic problems likely to arise in attempting WSD. Considerably deeper understanding of the factors involved in the disambiguation task is required before appropriate test suites for typological evaluation of WSD results can be devised. The in vitro evaluation demands the creation of a manually sense-tagged reference corpus containing an agreed-upon set of sense distinctions. The idea is to build a corpus, find the entity into the corpus and annotate the entity with relative sense. In our experiments we use Wikipedia as Sense Inventory for the entities because it's the same used by the WibNED algorithm. WibNED is implemented in JAVA, by using Lexical Collector web service [7] as accessing point to the Wikipedia Sense Inventory. We run the WibNED algorithm on the dataset described in Section 4 in order to evaluate its effectiveness. We used the accuracy metric that describes the ratio between entities correctly labelled by WibNED and total number of entities within the document.","refs":[{"start":2000,"end":2003,"marker":"bibr","target":"#b6"}]},{"text":"Experimetal results are showed in figure 1: on the x-axis are reported single documents while on the y-axis is reported the accuracy of WiBNED for each document. Documents on x-axis are ordered according to ascending accuracy. Some statistics are reported in table 2. The table provides the total number of entities within the corpus, the total number of correctly labelled entities by WiBNED Fig. 1. Accuracy of WiBNED on 752 documents algorithm, the average number of entities on single documents, the average number of correctly labelled entities per document. Results about accuracy are showed: total accuracy which is calculated considering the total number of entities and total number of correctly labelled entities, regardless of distribution between documents, average accuracy considering the mean of accuracy of all documents. The minimal and maximal values reported for accuracy are listed in the table.","refs":[{"start":41,"end":42,"marker":"figure","target":null},{"start":265,"end":266,"marker":"table","target":"#tab_1"},{"start":398,"end":399,"marker":"figure","target":null}]},{"text":"The results are quite encouraging if we consider that WibNED is a very simple algorithm based only on the string-matching between the words in Wikipedia definition and the words within the context of the target entity. The algorithm achieves on average 28,2% of accuracy. Taking into account more informative features borrowed from Wikipedia, such as category labels associated to each article or internal links, could improve results of the algorithm.","refs":[]}]},{"title":"Conclusions","paragraphs":[{"text":"In this paper we presented the WibNED algorithm for Named Entity Disambiguation and we evaluated it on italian language using an ad hoc builded corpus, automatically annotated with Wikipedia URIs.","refs":[]},{"text":"The task of disambiguating Named Entities within a text and the problem of identity and reference are important issues for many research field, most of all for NLP: the WibNED algorithm associates unique references to words and uses popular URLs (Wikipedia's) as \"canonical\" URIs.","refs":[]},{"text":"An ongoing work is focused on improving accuracy of WiBNED algorithm, relying on more Wikipedia features, such as links and categories instead of using only words within the disambiguation process.","refs":[]}]}],"tables":{"tab_0":{"heading":"Table 1 .","description":"Corpus example","rows":[["Il","RS Il","O"],["nome","SS nome","O"],["\"","XPO \"","O"],["Beastie","SP beastie","O"],["\"","XPO \"","O"],[",","XPW ,","O"],["inventato","VSP inventare","O"],["dall","SN dall","O"],["'","XP '","O"],["ex","SN ex","O"],["componente SS componente O"],["John","SPN John","B-http:it.wikipedia.org/wiki/John Berry"],["Berry","SPN Berry","I-http:it.wikipedia.org/wiki/John Berry"],[",","XPW ,","O"],["é","VI essere","O"],["l","SN l","O"],["'","XP '","O"],["acronimo AS acronimo O"],["della","ES della","O"],["frase","SS frase","O"],["\"","XPO \"","O"],["Boys","YF Boys","O"],["Entering","YF Entering","O"],["Anarchistic YF Anarchistic O"],["States","YF States","O"],["Towards","YF Towards","O"],["Inner","SPN Inner","O"],["Excellence SPN Excellence O"],["\"","XPO \"","O"]]},"tab_1":{"heading":"Table 2 .","description":"WibNED Results","rows":[["Total Number of Entities","67029"],["Total Number of correctly labelled Entities","19131"],["Average number of Entities per Document","89"],["Average number of Correctly labelled Entities per Document 25"],["Total Accuracy","0.285"],["Average Accuracy on single Document","0,282"],["Minimal Accuracy","0,000"],["Maximal Accuracy","0,833"]]}},"abstract":{"title":"Abstract","paragraphs":[{"text":"Natural Language is a mean to express and discuss concepts, which are taken to be abstractions from perceptions of the experienced real world: what texts describe consist of objects and events. Objects of the real world are identified by proper names, which are words, thus raising the problem of proper linkage between the textual reference and the real object. This work adresses the problem of automatically association of meanings to words within an unstructured text and focuses the attention on words representing Named Entities. The proposed solution consists of a Knowledge based algorithm for Named Entity Disambiguation: we used an ad hoc builded corpus, extracted form Wikipedia's articles to prove the soundness of the algorithm.","refs":[]}]}}