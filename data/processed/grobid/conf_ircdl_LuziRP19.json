{"bibliography":{"title":"The OpenUP Pilot on Research Data Sharing, Validation and Dissemination in Social Sciences","authors":[{"person_name":{"surname":"Luzi","first_name":"Daniela"},"affiliations":[{"department":"Institute for Research on Population and Social Policies","institution":"National Research Council","laboratory":null}],"email":"d.luzi@irpps.cnr.it"},{"person_name":{"surname":"Ruggieri","first_name":"Roberta"},"affiliations":[{"department":"Institute for Research on Population and Social Policies","institution":"National Research Council","laboratory":null}],"email":null},{"person_name":{"surname":"Pisacane","first_name":"Lucio"},"affiliations":[{"department":"Institute for Research on Population and Social Policies","institution":"National Research Council","laboratory":null}],"email":null}],"date":null,"ids":{"DOI":"10.1007/978-3-030-11226-4_20","arXiv":null},"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"keywords":["Open dataset review and validation","Open data","Data quality"],"citations":{"b0":{"title":"OpenUP","authors":[],"date":{"year":"2018","month":"07","day":"31"},"ids":null,"target":"http://openup-h2020.eu/","publisher":null,"journal":null,"series":null,"scope":null},"b1":{"title":"Project OpenUP-Deliverable D6.2 -Interim Use Case Evaluation Report","authors":[{"person_name":{"surname":"Vignoli","first_name":"M"},"affiliations":[],"email":null}],"date":{"year":"2017","month":"11","day":"30"},"ids":null,"target":"http://openup-h2020.eu/wp-content/uploads/2017/01/OpenUP_D6.2_Interim-Use-Case-Evaluation-Report.pdf","publisher":null,"journal":null,"series":null,"scope":null},"b2":{"title":"Project OpenUP-Deliverable D6.3 -Final Use Case Evaluation Report","authors":[{"person_name":{"surname":"Blümel","first_name":"C"},"affiliations":[],"email":null}],"date":{"year":"2018","month":"09","day":"14"},"ids":null,"target":"http://openup-h2020.eu/wp-content/uploads/2018/09/OpenUP_D6.3_Final-Use-Case-Evaluation-Report.pdf","publisher":null,"journal":null,"series":null,"scope":null},"b3":{"title":"Citation and peer review of data: moving towards formal data publication","authors":[{"person_name":{"surname":"Lawrence","first_name":"B"},"affiliations":[],"email":null},{"person_name":{"surname":"Jones","first_name":"C"},"affiliations":[],"email":null},{"person_name":{"surname":"Matthews","first_name":"B"},"affiliations":[],"email":null},{"person_name":{"surname":"Pepler","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Callaghan","first_name":"S"},"affiliations":[],"email":null}],"date":{"year":"2011","month":null,"day":null},"ids":{"DOI":"10.2218/ijdc.v6i2.205","arXiv":null},"target":"https://doi.org/10.2218/ijdc.v6i2.205","publisher":null,"journal":"Int. J. Digital Curation","series":null,"scope":{"volume":6,"pages":{"from_page":4,"to_page":37}}},"b4":{"title":"Peer review of datasets: when, why, and how","authors":[{"person_name":{"surname":"Mayernik","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Callaghan","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Leigh","first_name":"R"},"affiliations":[],"email":null},{"person_name":{"surname":"Tedds","first_name":"J"},"affiliations":[],"email":null},{"person_name":{"surname":"Worley","first_name":"S"},"affiliations":[],"email":null}],"date":{"year":"2015","month":null,"day":null},"ids":{"DOI":"10.1175/BAMS-D-13-00083.1","arXiv":null},"target":"https://doi.org/10.1175/BAMS-D-13-00083","publisher":null,"journal":"Bull. Am. Meteorol. Soc","series":null,"scope":{"volume":96,"pages":{"from_page":191,"to_page":201}}},"b5":{"title":"Data journals: a survey","authors":[{"person_name":{"surname":"Candela","first_name":"L"},"affiliations":[],"email":null},{"person_name":{"surname":"Castelli","first_name":"D"},"affiliations":[],"email":null},{"person_name":{"surname":"Manghi","first_name":"P"},"affiliations":[],"email":null},{"person_name":{"surname":"Tani","first_name":"A"},"affiliations":[],"email":null}],"date":{"year":"2015","month":null,"day":null},"ids":{"DOI":"10.1002/asi.23358","arXiv":null},"target":"https://doi.org/10.1002/asi.23358","publisher":null,"journal":"J. Assoc. Inf. Sci. Technol","series":null,"scope":{"volume":66,"pages":{"from_page":1747,"to_page":1762}}},"b6":{"title":"What Constitutes Peer Review of Data: A Survey of Published Peer Review Guidelines","authors":[{"person_name":{"surname":"Carpenter","first_name":"T"},"affiliations":[],"email":null}],"date":{"year":"2017","month":"04","day":null},"ids":null,"target":"http://arxiv.org/abs/1704.02236","publisher":null,"journal":null,"series":null,"scope":null},"b7":{"title":"Are scientific data repositories coping with research data publishing?","authors":[{"person_name":{"surname":"Assante","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Candela","first_name":"L"},"affiliations":[],"email":null},{"person_name":{"surname":"Castelli","first_name":"D"},"affiliations":[],"email":null},{"person_name":{"surname":"Tani","first_name":"A"},"affiliations":[],"email":null}],"date":{"year":"2016","month":null,"day":null},"ids":{"DOI":"10.5334/dsj-2016-006","arXiv":null},"target":"https://doi.org/10.5334/dsj-2016-006","publisher":null,"journal":"Data Sci. J","series":null,"scope":{"volume":15,"pages":{"from_page":1,"to_page":24}}},"b8":{"title":"Guidelines on recommending data repositories as partners in publishing research data","authors":[{"person_name":{"surname":"Callaghan","first_name":"S"},"affiliations":[],"email":null}],"date":{"year":"2014","month":null,"day":null},"ids":{"DOI":"10.2218/ijdc.v9i1.309","arXiv":null},"target":"https://doi.org/10.2218/ijdc.v9i1.309","publisher":null,"journal":"Int. J. Digital Curation","series":null,"scope":{"volume":9,"pages":{"from_page":152,"to_page":163}}},"b9":{"title":"Data sharing by scientists: practices and perceptions","authors":[{"person_name":{"surname":"Tenopir","first_name":"C"},"affiliations":[],"email":null}],"date":{"year":"2011","month":null,"day":null},"ids":{"DOI":"10.1371/journal.pone.0021101","arXiv":null},"target":"https://doi.org/10.1371/journal.pone.0021101","publisher":null,"journal":"PLoS One","series":null,"scope":{"volume":6,"pages":{"from_page":21101,"to_page":21101}}},"b10":{"title":"Social scientists' data sharing behaviours: investigating the roles of individual motivations, institutional pressures, and data repositories","authors":[{"person_name":{"surname":"Kim","first_name":"Y"},"affiliations":[],"email":null},{"person_name":{"surname":"Adler","first_name":"M"},"affiliations":[],"email":null}],"date":{"year":"2015","month":null,"day":null},"ids":{"DOI":"10.1016/j.ijinfomgt.2015.04.007","arXiv":null},"target":"https://doi.org/10.1016/j.ijinfomgt.2015.04.007","publisher":null,"journal":"Int. J. Inf. Manage","series":null,"scope":{"volume":35,"pages":{"from_page":408,"to_page":418}}},"b11":{"title":"Social scientists' satisfaction with data reuse","authors":[{"person_name":{"surname":"Faniel","first_name":"I"},"affiliations":[],"email":null},{"person_name":{"surname":"Kriesberg","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Yakel","first_name":"E"},"affiliations":[],"email":null}],"date":{"year":"2015","month":null,"day":null},"ids":{"DOI":"10.1002/asi.23480","arXiv":null},"target":"https://doi.org/10.1002/asi.23480","publisher":null,"journal":"J. Assoc. Inf. Sci. Technol","series":null,"scope":{"volume":67,"pages":{"from_page":1404,"to_page":1416}}},"b12":{"title":"Researcher perspectives on publication and peer review of data","authors":[{"person_name":{"surname":"Kratz","first_name":"J"},"affiliations":[],"email":null},{"person_name":{"surname":"Strasser","first_name":"C"},"affiliations":[],"email":null}],"date":{"year":"2015","month":null,"day":null},"ids":{"DOI":"10.1371/journal.pone.0117619","arXiv":null},"target":"https://doi.org/10.1371/journal.pone.0117619","publisher":null,"journal":"PLoS ONE","series":null,"scope":{"volume":10,"pages":{"from_page":117619,"to_page":117619}}},"b13":{"title":"OpenUP survey on researchers' current perceptions and practices in peer review, impact measurement and dissemination of research results survey","authors":[{"person_name":{"surname":"Stančiauskas","first_name":"V"},"affiliations":[],"email":null},{"person_name":{"surname":"Banelytė","first_name":"V"},"affiliations":[],"email":null}],"date":{"year":"2017","month":"04","day":"19"},"ids":{"DOI":"10.5281/zenodo.556157","arXiv":null},"target":"https://doi.org/10.5281/zenodo.556157","publisher":null,"journal":null,"series":null,"scope":null},"b14":{"title":"Scholarship in the Digital Age: Information, Infrastructure, and the Internet","authors":[{"person_name":{"surname":"Borgman","first_name":"C"},"affiliations":[],"email":null}],"date":{"year":"2007","month":null,"day":null},"ids":null,"target":null,"publisher":"MIT Press","journal":null,"series":null,"scope":null},"b15":{"title":"","authors":[],"date":{"year":"2018","month":"09","day":"19"},"ids":null,"target":"https://brill.com/view/journals/rdj/rdj-overview.xml","publisher":null,"journal":"Research Data Journal","series":null,"scope":null},"b16":{"title":"DANS -Data Archiving and Network Services","authors":[],"date":{"year":"2018","month":"09","day":"19"},"ids":null,"target":"https://dans.knaw.nl/en","publisher":null,"journal":null,"series":null,"scope":null},"b17":{"title":"UK Data Archive","authors":[],"date":{"year":"2018","month":"09","day":"19"},"ids":null,"target":"http://www.data-archive.ac.uk/","publisher":null,"journal":null,"series":null,"scope":null},"b18":{"title":"GESIS -Gesellschaft Sozialwissenschaftlicher Infrastruktureinrichtungen","authors":[],"date":{"year":"2018","month":"09","day":"19"},"ids":null,"target":"https://www.gesis.org/en/home/","publisher":null,"journal":null,"series":null,"scope":null},"b19":{"title":"CESSDA -Consortium of European Social Science Data Archives","authors":[],"date":{"year":"2018","month":"09","day":"19"},"ids":null,"target":"https://www.cessda.eu/","publisher":null,"journal":null,"series":null,"scope":null},"b20":{"title":"ICPSR -Interuniversity Consortium for Political and Social Research","authors":[],"date":{"year":"2018","month":"09","day":"19"},"ids":null,"target":"https://www.icpsr.umich.edu/icpsrweb/","publisher":null,"journal":null,"series":null,"scope":null},"b21":{"title":"Data Seal of Approval","authors":[],"date":{"year":"2018","month":"09","day":"19"},"ids":null,"target":"https://www.datasealofapproval.org/en/","publisher":null,"journal":null,"series":null,"scope":null},"b22":{"title":"DDI -Data Documentation Initiative","authors":[],"date":{"year":"2018","month":"09","day":"19"},"ids":null,"target":"http://www.dcc.ac.uk/resources/metadata-standards/ddi-data-documentation-initiative","publisher":null,"journal":null,"series":null,"scope":null},"b23":{"title":"Data without peer: examples of data peer review in the earth sciences","authors":[{"person_name":{"surname":"Callaghan","first_name":"S"},"affiliations":[],"email":null}],"date":{"year":"2015","month":"09","day":"19"},"ids":{"DOI":"10.1045/january2015-callaghan","arXiv":null},"target":"http://datareviews.dans.knaw.nl/","publisher":null,"journal":"D-Lib Mag","series":null,"scope":{"volume":21,"pages":{"from_page":9,"to_page":9}}},"b24":{"title":"Data resource profile: the human mortality database (HMD)","authors":[{"person_name":{"surname":"Barbieri","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Wilmoth","first_name":"J"},"affiliations":[],"email":null},{"person_name":{"surname":"Shkolnikov","first_name":"V"},"affiliations":[],"email":null}],"date":{"year":"2015","month":null,"day":null},"ids":{"DOI":"10.1093/ije/dyv105","arXiv":null},"target":"https://doi.org/10.1093/ije/dyv105","publisher":null,"journal":"Int. J. Epidemiol","series":null,"scope":{"volume":44,"pages":{"from_page":1549,"to_page":1556}}},"b25":{"title":"Special methods used for selected population","authors":[],"date":{"year":"2018","month":"09","day":"19"},"ids":null,"target":"https://www.mortality.org/Public/Docs/SpecialMethods.pdf","publisher":null,"journal":null,"series":null,"scope":null}},"sections":[{"title":"Introduction","paragraphs":[{"text":"The study presented is part of OpenUP [1] (OPENing UP new methods, indicators and tools for peer review, dissemination of research results and impact measurement), a European funded project that addresses the currently transforming science landscape on innovative peer review, dissemination and impact measuring. The project test the achieved results in a set of seven pilots [2,3]. They are related to the three project's pillars and are applied to specific research areas and communities: arts and humanities, social sciences, energy and life sciences.","refs":[{"start":38,"end":41,"marker":"bibr","target":"#b0"},{"start":376,"end":379,"marker":"bibr","target":"#b1"},{"start":379,"end":381,"marker":"bibr","target":"#b2"}]},{"text":"The results presented in this paper are related to the pilot study that investigated the applicability of peer review and/or OPR to datasets in Social sciences. The pilot aims at identifying strong and weak elements in the process of dataset review and validation and intends to outline best practices that facilitate transparency of the process as well as data dissemination, reliability and reuse.","refs":[]},{"text":"In particular the paper reviews data sharing and evaluation practices in Social sciences, on which the selection of the pilot community is based, and reports on the interviews with the management team of the selected community. Lessons learned that can help identifying requisites and best practices for OPR of research data are reported in the conclusions.","refs":[]}]},{"title":"Methods","paragraphs":[{"text":"To reconstruct the context of data sharing and evaluation in Social sciences, a landscape scan was performed to identify relevant literature, guidelines and scientific communities' practices. This first phase provided the basis for the selection of potential communities to be involved in the pilot, representative for this heterogeneous scientific field, as well as the identification of specific characteristics and problematic issues. For these reasons the landscape scans focused on surveys related to the researchers' motivation and constraints towards data sharing and on desk research enabled the identification of types of research data, dataset providers and modes of validating and sharing/publishing datasets in Social sciences.","refs":[]},{"text":"After considering the involvement of different research communities in Social sciences, a collaboration was formalized with the Human Mortality Database (HMD).","refs":[]},{"text":"HMD is an open database that provides detailed, consistent and high quality data to researchers, students, journalists, policy analysts, and others interested in the history of human longevity and its prospects for the future (https://www.mortality.org/). HMD constitutes a good example of a well-known source of information providing open access to data in the scientific community of demographers. Moreover, the collaborative organization of HMD, as well as the high number and variety of data users worldwide, represented a good premise to get further insights into data managing practices and reuse.","refs":[]},{"text":"The second phase of the analysis was devoted to the development of an interview schema that aimed at exploring the following HMD features: origin, motivations and organizational features of the scientific community, data quality assessment process, opinion on Open access of data. A further step of the project concerned the development of a users' survey carried out in collaboration with HMD to get insights into users' perspectives and feedback on data reuse. The results of the HMD survey are not included in this paper and will be presented in further publications. The interviews with the selected community were conducted on the 31st of January and 1st of February 2018 at the Max Planck Institute for Demographic Research in Rostock, Germany. They were performed according to interviewees' role in HMD, and for the selection of interviewees we also considered gender balance. The two directors, two researchers in their role of country responsible (in charge of analysing data for specific countries) were interviewed. These interviews covered the majority of HMD staff (4 out of 7). The summary content presented in this paper was revised, commented and approved by the interviewees.","refs":[]},{"text":"The paper presents the results of the landscape scans related to the main characteristics of data management and validation in Social sciences. Moreover, a summary of the interviews is provided highlighting in particular scientific motivations for data sharing, organizational features as well as the quality assessment process as important success factors for data publishing.","refs":[]}]},{"title":"Data Sharing and Evaluation Practices in Social Sciences","paragraphs":[{"text":"Similar to peer review of publications, data peer review is a quality assessment process of a dataset performed by experts in the field. Data quality assessment is a complex process that has to consider the different phases of the data lifecycle, starting from the development of a Data Management Plan (DMP) at the initial stage of a scientific project to the publication of its results. Data publication (some authors speak about Publication with capital letter, [4,5]) that undergoes a peer review process validating data quality is currently performed in data journals and data repositories. However, analyses by Candela et al. [6], and Carpenter [7] show that there is room for improvement, as peer review activities in data journals vary widely and are mostly focused on metadata rather than data themselves, aiming at assessing the documentation and metadata description that facilitates data reuse. Assante et al. [8], in the analysis of generalist repositories (Zenodo, Dryad, Figshare etc.), also come to the conclusion that different criteria and quality control mechanisms are implemented based on various policies and/or guidelines. Best practices of data validation can be taken from the publication in trusted data repositories [9], in which the data quality control can be considered a form of review by experts in the field carried out in a prepublication phase. This form of pre-publication review is then confirmed by the use of the dataset by data consumers in the post-publication phase through formal citations and/or statistics of use (see Fig. 1).","refs":[{"start":465,"end":468,"marker":"bibr","target":"#b3"},{"start":468,"end":470,"marker":"bibr","target":"#b4"},{"start":617,"end":635,"marker":"bibr","target":null},{"start":651,"end":654,"marker":"bibr","target":"#b6"},{"start":922,"end":925,"marker":"bibr","target":"#b7"},{"start":1243,"end":1246,"marker":"bibr","target":"#b8"},{"start":1568,"end":1569,"marker":"figure","target":"#fig_0"}]}]},{"title":"Researchers' Motivation and Constraints","paragraphs":[{"text":"Several surveys have investigated researchers' attitudes to analyse barriers and facilitators towards data sharing. General surveys allow us to compare social scientists' attitudes with other disciplines. Tenopir et al. [10] found that social scientists have a lower propensity to share the data they produce compared to the STEM researchers. These results have been confirmed in other surveys [11,12]. This may depend on the nature of data, especially when qualitative data are involved, privacy and confidential issues, lack of technical standards and easy-to use platforms. Social scientists share the same concerns as scientists in other disciplines, such as not being recognised for making the data available, misuse of data, costs and time-consuming activities required.","refs":[{"start":220,"end":224,"marker":"bibr","target":"#b9"},{"start":394,"end":398,"marker":"bibr","target":"#b10"},{"start":398,"end":401,"marker":"bibr","target":"#b11"}]},{"text":"Concerning peer review, Kratz and Strasser's [13] survey shows that researchers of different disciplinary fields are still unsure on how data peer review should work and in which context it should occur, even if they expect that data in repositories are subject to validation. The OpenUP survey [14] indicates that social scientists together with ICT researchers are the ones that are less satisfied with the traditional peer review process of publications.","refs":[{"start":45,"end":49,"marker":"bibr","target":"#b12"},{"start":295,"end":299,"marker":"bibr","target":"#b13"}]}]},{"title":"Types of Dataset Providers","paragraphs":[{"text":"A specific characteristic in this field is that a significant portion of data is produced for purposes other than research [15]. These are data created by governmental bodies that have to comply with transparency regulations (such as the UK Freedom of information act) and make data they collect publicly available. Examples of these data comprise census figures, cohort and longitudinal studies, cross national surveys, economic indicators, etc. Among governmental bodies it is worth mentioning the data produced by national statistical offices that apply standard procedures to collect and process data, provide detailed supplementary documentation to describe the dataset, and also guarantee long-term preservation. These data collection constitutes trustworthy information, on which many other studies are based representing an important data source not only for social scientists.","refs":[{"start":123,"end":127,"marker":"bibr","target":"#b14"}]},{"text":"While these sources of information can be compared to the big data produced by STEM, long-tail data are produced by social scientists to investigate local phenomena in small collaborative groups, often within interdisciplinary projects or individually. They are usually facing privacy issues that make the dataset sharing more complex.","refs":[]}]},{"title":"Modes of Sharing/Publishing Datasets","paragraphs":[{"text":"In Social sciences, data publication model is mostly related to dataset submission in a data repository. In fact, there is only one data journal covering Humanities and Social sciences: \"Research Data Journal (RDJ)\" [16]. It was created by DANS [17] in 2016 with the aim to increase the visibility of data stored in the archive and to provide more extensive and detailed documentation. This journal conforms to well-established data journals in other disciplines such as Earth System Science Data, Geoscience Data Journal, and Scientific Data, it assigns a DOI to the article and provides the related DOI assigned to the dataset stored. On the other hand, DANS archive does not provide a standard description to cite the article. Currently eleven data papers have been published and three papers refer to the field of Social sciences.","refs":[{"start":216,"end":220,"marker":"bibr","target":null},{"start":245,"end":249,"marker":"bibr","target":"#b16"}]},{"text":"Considering trusted data repositories, their main feature is that of data centres that act at national level as main information sources in this field. Worth mentioning are the UK Data Archive [18], GESIS [19] and DANS. The majority of these national centralized data centres are also part of two consortia, CESSDA [20] at a European level and ICPSR [21] at international level. These consortia provide a single access to international data and also develop and coordinate initiatives on standards, protocols and best practices to support data management and dissemination. Most of them provide access to data produced by governmental bodies and by research groups.","refs":[{"start":193,"end":197,"marker":"bibr","target":"#b17"},{"start":205,"end":209,"marker":"bibr","target":"#b18"},{"start":315,"end":319,"marker":"bibr","target":"#b19"},{"start":350,"end":354,"marker":"bibr","target":"#b20"}]}]},{"title":"Modes of Validating Datasets","paragraphs":[{"text":"The above-mentioned data repositories are certified by the Data Seal of Approval [22] that has identified 16 requirements based on 5 criteria: data availability on the Internet, accessibility (clear rights and licenses), usability (format), reliability and identification of dataset through a persistent identifier. Note that these also correspond to the criteria used to evaluate the data themselves, similar to the FAIR principles. Given that data validation represents an iterative process that encompasses the entire research lifecycle, these trusted repositories provide guidelines on how to develop a data management plan at the very beginning of a research to assure data quality. Moreover, they require data producers to establish copyright and appropriate licenses, to use proper data formats and metadata schemas to facilitate access and reuse.","refs":[{"start":81,"end":85,"marker":"bibr","target":"#b21"}]},{"text":"Trusted data repositories provide guidelines and/or template for a correct data ingestion according to the metadata schema of DDI [23], a standard supported by the Social sciences community that facilitate data replication and/or reproduction. They assure long-term data preservation and curation, develop data discovery tools (such as landing pages; [24]), suggest users a data citation format that acknowledge data provenance. Moreover, there are different initiatives to increase data re-use and make it traceable. The suggestion of a data citation format represents an important means to support data citations that are an indirect appraisal of the quality of the dataset at the post-publication phase. Some trusted repositories have adopted tools to track data use. For instance, DANS provides data users with a validation template to rank data set available in EASY: users can provide the rating (up to five stars) to data quality, quality of documentation, completeness of the data, consistency, structure and usefulness of the file format [25].","refs":[{"start":130,"end":134,"marker":"bibr","target":"#b22"},{"start":351,"end":355,"marker":"bibr","target":"#b23"}]}]},{"title":"Interviews with the Human Mortality Database Management Team","paragraphs":[{"text":"The scientific community that manages the Human Mortality Database (HMD) was selected for the pilot as it fulfilled the requirements set up in the OpenUP methodology, and not least, for their willingness to an active collaboration on the analysis. The Human Mortality Database documents the longevity revolution of the modern era and facilitates research into its causes and consequences providing open data on human mortality. HMD was launched in 2002 as a result of a collaborative project that involved researchers of the Department of Demography at the University of California Berkeley (UCB) and the Max Planck Institute for Demographic Research (MPIDR).","refs":[]},{"text":"The following paragraphs summarise the key points of the interviews carried out with the two directors and two country specialists (CSs). As previously mentioned, its content was revised and approved by the interviewees.","refs":[]}]},{"title":"Origin, Motivations and Organisational Features","paragraphs":[{"text":"Two previous relevant experiences guided the development of the database: the Kannisto-Thatcher Database on Old Age Mortality (KTD) at the MPIDR and the Berkeley Mortality Database (BMD), founded by John Wilmoth at UCB. Both experiences were concerned with what was at that time an emerging phenomenon of low mortality at young and adult ages, falling mortality at old ages, and greater survival to an advanced age, leading to a potential increase in the number of people exposed to degenerative diseases, which are difficult to treat or prevent. To understand this phenomenon, it was necessary to analyse and model longevity and survival of humans with a special emphasis on advanced (frontier) age over a long period of time. This research needed reliable data at international level providing long-term and continuous series without gaps, running up to the highest ages, providing fine details according to age, time, and cohort dimensions, ensuring sufficient quality and comparability across time and populations. HMD was therefore developed to answer this scientific question providing a methodology based on the previous mentioned experiences as well as freely available high-quality data [26].","refs":[{"start":1196,"end":1200,"marker":"bibr","target":"#b24"}]},{"text":"The two HMD directors explained that \"the collaboration was originally, (and still is), based on a small, very well-established group of internationally based demographers who were willing to serve the scientific community interested in demographic studies\". The workload is equally distributed among the team that comprises CSs who have high-level competences on demographic development of a set of specific countries and are responsible for collecting and analysing data from the related national statistical offices. Other tasks comprise the development of computer codes, which are also made freely available to the end user who wants to reproduce the analysis, as well as the management of the website. Strong collaboration pertains the data quality process performed before data are publicly available, which constitutes a form of internal pre-publishing peer review process. During the interview the two directors agreed that \"trust among the team and scientific curiosity are the drivers of this successful cooperation\", that only recently was formalized by a Memorandum of understanding.","refs":[]}]},{"title":"Goals and Main Features of the Database","paragraphs":[{"text":"The main goal of HMD is to support research on human mortality and longevity providing open data on 39 countries and some sub-areas and sub-populations with series starting as early as 1751 (i.e. Sweden) and covering more than 100 years for 16 populations. Birth and death counts are generally based on data from national vital registration systems, while data on population are based on the national census and estimates between censuses. However, differences may exist among countries in the periodicity of census, methods and definition used as well as in data format. Moreover, some countries have experienced changes in their territorial boundaries, have suffered substantial loss during war periods and/or faced substantial consistent migration over the period covered by HMD. For these reasons, as underlined by the two directors, HMD has developed a methodology to produce detailed death counts and population estimates, to correct mortality estimates at old ages, and to build high quality life tables (as described in detail in the Methods protocol). \"All HMD data are prepared using this standard methodology. This assures comparability in time and across countries\". The two County specialists explained, that \"when special methods are needed to accommodate issues in data availability, this is documented in the country-specific documentation as well as reported in summary tables\" [27]. Country-specific details related to the data quality and statistical system in each country are therefore documented in the country-specific Background and Documentation file accessible from each country webpage. The application of these thorough procedures, \"the punctual explanation of the estimations and refinements of data sources make this database different from other sources providing mortality rates\". These procedures guarantee a uniform analysis of raw data, facilitating the comparability across time and space, while the detailed documentation and the availability of source data allow end user to reproduce the analysis. The HMD team has also developed software code that guide them in the evaluation of data quality as well as software packages that facilitate end user to import and working with HMD data. These tools are freely available to end users along with technical reports explaining how to use these scripts [28]. This is another value-added feature of HMD.","refs":[{"start":1395,"end":1399,"marker":"bibr","target":"#b25"},{"start":2335,"end":2339,"marker":"bibr","target":null}]}]},{"title":"The HMD Data Quality Assessment Process","paragraphs":[{"text":"The HMD team has developed a set of procedural steps to ensure data quality. This important topic was addressed in the interviews with the two directors and particularly explored in the interviews with the CSs. An activity diagram that reconstructed the workflow of the activities performed before data publication was presented to the CSs Fig. 2. Data quality assessment process and discussed to have further insights on the procedures adopted to assess data quality. This intended to explore whether collaborative activities resembling a peer review process could be tracked in HMD data quality assessment. A high-level description resulting from the interviews is provided in Fig. 2.","refs":[{"start":345,"end":346,"marker":"figure","target":null},{"start":684,"end":685,"marker":"figure","target":null}]},{"text":"During the interviews the CSs explained that each country or area is assigned to an individual researcher, a CS, who maintains a close relationship with a local expert generally at national statistical offices, and has an extensive knowledge of the population dynamics as well as how data are collected at national level. A CS is responsible for the first quality checks that evaluate consistency and plausibility of input data, prepares pre-calculation file (Lexis files) and analyses the results on the basis of a predefined data quality checklist and diagnostic charts that help him/her to explore unusual fluctuation and/or any other issues in data sources. The results of this analysis are shared within the HMD community via an internal report and are the basis for the application of the six-step procedure to produce the complete data series (exposures to risk, death rates, life expectancy and other life tables). Before data are published, the HMD team perform an additional phase of validation. These activities are crucial especially when a new country has to be included in HMD. However, they constitute a routine procedure every time data are updated. \"In cases of unexpected changes in national statistical systems or in regimes of national statistical registration, the updating procedures are non-trivial\".","refs":[]},{"text":"All steps in the computing of data analysis are documented in detail and made available to end users in the different files (Background and documentation, Data source and Explanatory Notes). According to the CSs interviewed, this is the distinctive feature of HMD: \"Data refinements and harmonization that allows comparison across countries are documented in detail so that researchers in this field are aware of possible problems in the data and know how these issues have been solved\". When asked about long preservation of data, it emerged that the two HMD directors are dependent on funds. At the moment MPIDR support their activities (\"MPIDR researchers are allowed to spend half of their work time on HMD\"), while the UCB team has to provide its own funds. A clear commitment of the organisation would therefore be very important and would also mean a clear recognition of their activities. Between the lines, it emerged that publication of scientific papers are generally considered more important than managing a database. In their opinion, \"the analysis of data, their quality check is not only a service for the community of reference but is a researcher activity in itself.\" The majority of the interviewees has heard about open review of journals but has little knowledge on all its traits. If they see a similarity with peer review of data, this is associated in particular with transparency as a means of reconstructing the methods and procedures used for the data analysis.","refs":[]}]},{"title":"Lessons Learned","paragraphs":[{"text":"Some important indications emerged from the analysis of the interviews that can drive the adoption of data quality assessment, and hence peer review, as well as some principles that can incentivize other scientific communities to share their research data. As stated by the HMD interviewees \"the guiding principles to create an open access database were: comparability, flexibility, accessibility and reproducibility\". Comparability was reached using a uniform, scientific methodology to calculate the various statistics of the 39 countries included in the database. Flexibility was achieved in the analysis of results using a uniform set of procedures for each population, but at the same time giving significant attention to each population in terms of its history and socio-political development. This is also reflected in the available formats of output data series. This is achieved thanks to the experiences and knowledge CSs, that is persons in charge of collecting data from a specific number of countries, who interact with statistical offices, check data consistency and provide population statistics together with a country report that explains specificity and motivation of analysis. Accessibility was guaranteed from the beginning by free of charge access of data, as well as by the provision of data in an open, no-proprietary format. Reproducibility is provided by the reconstruction of the data lifecycle that includes the availability of raw data, the method applied, the related results as well as the explanatory documentation. One of the main successful features of HMD is its transparent way of data managing and sharing that has two central phases of data validation. The first one is carried out by the CSs, who analyse the raw data according to a common predefined checklist that verifies consistency and plausibility of data. The second one is carried out in a collaborative way within the HMD team that validate the statistics before their publication, each time the database is updated.","refs":[]},{"text":"Moreover, another successful component of HMD was its collaborative approach that is based on a strong scientific interest in the field as well as on the trust among the involved community that only recently has formally signed a Memorandum of understanding. The interviews also highlighted some indications that confirm some concerns already mentioned by other surveys. Interviewees stressed the importance of having a strong commitment of the organization in supporting the development of data infrastructures. This pertains different aspects: a long-term financial support (beyond the project duration), a policy endorsement on open data as well as a formal recognition of scientists for the efforts in data curation and quality assurance.","refs":[]},{"text":"Implementation of other pilots should be further promoted in different subdisciplines of Social sciences to get a deeper insight into sharing and evaluation practices of research data, focusing in particular on the procedures that document data validation and scientific assessment. It is necessary to promote transparency in the process of data evaluation, similar to the one adopted by HMD, so to facilitate the reproducibility of the research.","refs":[]}]}],"tables":{},"abstract":{"title":"Abstract","paragraphs":[{"text":"The paper presents the results of a pilot carried out within the European project OpenUp (Opening up new methods, indicators and tools for peer review, dissemination of research results and impact measurement). Aim of the pilot is to investigate the applicability of peer review and/or Open Peer Review (OPR) to datasets in disciplines related to Social sciences. Main emphasis is given to the characteristic and features of data sharing and validation in this heterogeneous scientific field, thus providing the basis for the selection of the community chosen for the pilot. Indications emerging from the analysis of the interviews carried out in the pilot can drive the adoption of data quality assessment, and hence peer review, as well as provide some principles that can incentivize other scientific communities to share their research data.","refs":[]}]}}