{"bibliography":{"title":"Learning to Recognize Critical Cells in Document Tables","authors":[{"person_name":{"surname":"Mauro","first_name":"Nicola"},"affiliations":[{"department":"Dipartimento di Informatica","institution":"LACAM Laboratory Università degli Studi di Bari \"Aldo Moro\"","laboratory":null},{"department":"Centro Interdipartimentale per la Logica e sue Applicazioni","institution":"Università degli Studi di Bari \"Aldo Moro\"","laboratory":null}],"email":null},{"person_name":{"surname":"Ferilli","first_name":"Stefano"},"affiliations":[{"department":"Dipartimento di Informatica","institution":"LACAM Laboratory Università degli Studi di Bari \"Aldo Moro\"","laboratory":null},{"department":"Centro Interdipartimentale per la Logica e sue Applicazioni","institution":"Università degli Studi di Bari \"Aldo Moro\"","laboratory":null}],"email":"ferilli@di.uniba.it"},{"person_name":{"surname":"Esposito","first_name":"Floriana"},"affiliations":[{"department":"Dipartimento di Informatica","institution":"LACAM Laboratory Università degli Studi di Bari \"Aldo Moro\"","laboratory":null},{"department":"Centro Interdipartimentale per la Logica e sue Applicazioni","institution":"Università degli Studi di Bari \"Aldo Moro\"","laboratory":null}],"email":"esposito@di.uniba.it"}],"date":null,"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"keywords":[],"citations":{"b0":{"title":"label(c10076_2_1, single_stub)","authors":[],"date":null,"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b1":{"title":"","authors":[],"date":null,"ids":null,"target":null,"publisher":null,"journal":"next_row","series":null,"scope":{"volume":10076,"pages":{"from_page":10076,"to_page":10076}}},"b2":{"title":"","authors":[],"date":null,"ids":null,"target":null,"publisher":null,"journal":"cell","series":null,"scope":null},"b3":{"title":"References","authors":[],"date":null,"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":10076,"to_page":10079}}},"b4":{"title":"Mining sequential patterns","authors":[{"person_name":{"surname":"Agrawal","first_name":"R"},"affiliations":[],"email":null},{"person_name":{"surname":"Srikant","first_name":"R"},"affiliations":[],"email":null}],"date":{"year":"1995","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":3,"to_page":14}}},"b5":{"title":"Webtables: Exploring the power of tables on the web","authors":[{"person_name":{"surname":"Cafarella","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Halevy","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Wang","first_name":"Z"},"affiliations":[],"email":null},{"person_name":{"surname":"Wu","first_name":"E"},"affiliations":[],"email":null},{"person_name":{"surname":"Zhang","first_name":"Y"},"affiliations":[],"email":null}],"date":{"year":"2008","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b6":{"title":"Optimizing Probabilistic Models for Relational Sequence Learning","authors":[{"person_name":{"surname":"Mauro","first_name":"Di"},"affiliations":[],"email":null},{"person_name":{"surname":"Basile","first_name":"N"},"affiliations":[],"email":null},{"person_name":{"surname":"Ferilli","first_name":"T"},"affiliations":[],"email":null},{"person_name":{"surname":"Esposito","first_name":"S"},"affiliations":[],"email":null}],"date":{"year":"2011","month":null,"day":null},"ids":null,"target":null,"publisher":"Springer","journal":null,"series":null,"scope":{"volume":6804,"pages":{"from_page":240,"to_page":249}}},"b7":{"title":"Multi-dimensional relational sequence mining","authors":[{"person_name":{"surname":"Esposito","first_name":"F"},"affiliations":[],"email":null},{"person_name":{"surname":"Di Mauro","first_name":"N"},"affiliations":[],"email":null},{"person_name":{"surname":"Basile","first_name":"T"},"affiliations":[],"email":null},{"person_name":{"surname":"Ferilli","first_name":"S"},"affiliations":[],"email":null}],"date":{"year":"2008","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"Fundamenta Informaticae","series":null,"scope":{"volume":89,"pages":{"from_page":23,"to_page":43}}},"b8":{"title":"Machine learning for digital document processing: From layout analysis to metadata extraction","authors":[{"person_name":{"surname":"Esposito","first_name":"F"},"affiliations":[],"email":null},{"person_name":{"surname":"Ferilli","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Basile","first_name":"T"},"affiliations":[],"email":null},{"person_name":{"surname":"Di Mauro","first_name":"N"},"affiliations":[],"email":null}],"date":{"year":"2008","month":null,"day":null},"ids":null,"target":null,"publisher":"Springer","journal":null,"series":null,"scope":{"volume":90,"pages":{"from_page":105,"to_page":138}}},"b9":{"title":"Greedy randomized adaptive search procedures","authors":[{"person_name":{"surname":"Feo","first_name":"T"},"affiliations":[],"email":null},{"person_name":{"surname":"Resende","first_name":"M"},"affiliations":[],"email":null}],"date":{"year":"1995","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"Journal of Global Optimization","series":null,"scope":{"volume":6,"pages":{"from_page":109,"to_page":133}}},"b10":{"title":"θ-Subsumption and Resolution: A New Algorithm","authors":[{"person_name":{"surname":"Ferilli","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Di Mauro","first_name":"N"},"affiliations":[],"email":null},{"person_name":{"surname":"Basile","first_name":"T"},"affiliations":[],"email":null},{"person_name":{"surname":"Esposito","first_name":"F"},"affiliations":[],"email":null}],"date":{"year":"2003","month":null,"day":null},"ids":null,"target":null,"publisher":"Springer","journal":null,"series":null,"scope":{"volume":2871,"pages":{"from_page":384,"to_page":391}}},"b11":{"title":"Stochastic Local Search: Foundations & Applications","authors":[{"person_name":{"surname":"Hoos","first_name":"H"},"affiliations":[],"email":null},{"person_name":{"surname":"Stützle","first_name":"T"},"affiliations":[],"email":null}],"date":{"year":"2004","month":null,"day":null},"ids":null,"target":null,"publisher":"Morgan Kaufmann Publishers Inc","journal":null,"series":null,"scope":null},"b12":{"title":"Table structure recognition based on robust block segmentation","authors":[{"person_name":{"surname":"Kieninger","first_name":"T"},"affiliations":[],"email":null}],"date":{"year":"1998","month":null,"day":null},"ids":null,"target":null,"publisher":"SPIE","journal":null,"series":null,"scope":{"volume":3305,"pages":{"from_page":22,"to_page":32}}},"b13":{"title":"Functional-based table category identification in digital library","authors":[{"person_name":{"surname":"Kim","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Liu","first_name":"Y"},"affiliations":[],"email":null}],"date":{"year":"2011","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":1364,"to_page":1368}}},"b14":{"title":"Feature construction with version spaces for biochemical applications","authors":[{"person_name":{"surname":"Kramer","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"De Raedt","first_name":"L"},"affiliations":[],"email":null}],"date":null,"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":258,"to_page":265}}},"b15":{"title":"Tableseer: Automatic table metadata extraction and searching in digital libraries categories and subject descriptors","authors":[{"person_name":{"surname":"Liu","first_name":"Y"},"affiliations":[],"email":null},{"person_name":{"surname":"Bai","first_name":"K"},"affiliations":[],"email":null},{"person_name":{"surname":"Mitra","first_name":"P"},"affiliations":[],"email":null},{"person_name":{"surname":"Giles","first_name":"C"},"affiliations":[],"email":null}],"date":{"year":"2007","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":91,"to_page":100}}},"b16":{"title":"Identifying table boundaries in digital documents via sparse line detection","authors":[{"person_name":{"surname":"Liu","first_name":"Y"},"affiliations":[],"email":null},{"person_name":{"surname":"Mitra","first_name":"P"},"affiliations":[],"email":null},{"person_name":{"surname":"Giles","first_name":"C"},"affiliations":[],"email":null}],"date":{"year":"2008","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b17":{"title":"Table metadata: Headers, augmentations and aggregates","authors":[{"person_name":{"surname":"Nagy","first_name":"G"},"affiliations":[],"email":null},{"person_name":{"surname":"Padmanabhan","first_name":"R"},"affiliations":[],"email":null},{"person_name":{"surname":"Jandhyala","first_name":"R"},"affiliations":[],"email":null},{"person_name":{"surname":"Silversmith","first_name":"W"},"affiliations":[],"email":null},{"person_name":{"surname":"Krishnamoorthy","first_name":"M"},"affiliations":[],"email":null}],"date":{"year":"2010","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b18":{"title":"Data extraction from web tables: The devil is in the details","authors":[{"person_name":{"surname":"Nagy","first_name":"G"},"affiliations":[],"email":null},{"person_name":{"surname":"Seth","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Jin","first_name":"D"},"affiliations":[],"email":null},{"person_name":{"surname":"Embley","first_name":"D"},"affiliations":[],"email":null},{"person_name":{"surname":"Machado","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Krishnamoorthy","first_name":"M"},"affiliations":[],"email":null}],"date":{"year":"2011","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":242,"to_page":246}}},"b19":{"title":"A machine learning based approach for table detection on the web","authors":[{"person_name":{"surname":"Wang","first_name":"Y"},"affiliations":[],"email":null},{"person_name":{"surname":"Hu","first_name":"J"},"affiliations":[],"email":null}],"date":{"year":"2002","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":242,"to_page":250}}}},"sections":[{"title":"Introduction","paragraphs":[{"text":"The main motivation underlying the birth and spread of libraries consisted in collecting large quantities of documents, usually in paper format, with preservation and access objectives. Each library was typically characterized by a specific focus-of-interest, that established the direction and limits according to which the collections were developed, thus helping users to have in one place a complete landscape of the information they were interested in. As a technological counterpart, digital libraries have the same aims and objectives, but dealing with documents in digital format. This change of medium has a dramatic impact on the management of the collections, and on their exploitation by end-users. Huge quantities of documents can be easily collected and spread all over the world using the Internet, however, the users may experience difficulties in properly retrieving the data they are interested in. Information Retrieval (IR) and Machine Learning (ML) technologies can provide automatic tools to support such activities.","refs":[]},{"text":"The identification of relevant documents that can satisfy the users' query is the subject of the IR research field. Of course, to provide more effective results the automatic techniques should move from the purely lexical aspects of document to those concerning their semantics, which is still an open research issue. But having a thorough understanding of the document content is not only useful to support search and retrieval. It is also a fundamental requirement to be able to correlate the documents, and in particular the data and information they carry. In particular, a component of documents that is usually very informative and information-dense are tables. Authors use tables to compactly represent many important data in a small space, to draw more attention from readers, or for information comparison [10]. Thus, the availability of automatic components that can identify tables in documents, and that are able to understand the table structure, would be a precious support to extract the knowledge they contain, represent it formally (e.g., using a relational Database) and make it available to people and/or other software (e.g., using semantic technologies that are being developed nowadays). This paper proposes a set of intelligent techniques that cover the steps going from a document in digital format up to the identification of tables and the understanding of their structure, and particularly focuses on the exploitation of Machine Learning methodologies and systems for understanding the table structure.","refs":[{"start":815,"end":819,"marker":"bibr","target":"#b13"}]}]},{"title":"Preliminaries","paragraphs":[{"text":"Many works are present in the literature concerning tables and their analysis, focusing on different objectives, aspects and problems. Some concern theoretical contributions, such as the distinction between genuine tables (tables aimed at representing and organizing meaningful information) and non-genuine ones (tables just aimed at obtaining a spatial partition of the page, as in most Web documents) [16]. This is a relevant issue, since, according to [2], only 1% Web tables are genuine. Others face more practical problems, such as table boundary identification [13] and table structure decomposition [9], or the classification of tables according to their type of content and intended exploitation. In addition to table data extraction, table functionality analysis (aimed at understanding table types, functions, and purposes) is another crucial task for table understanding, table data sharing and reuse [10]. Yet other researchers focus on applications such as table search [12] or table classification [16]. Indeed, accurately extracting tables from document repositories is a challenging problem, but also selecting interesting tables from the set of collected tables is an open issue.","refs":[{"start":403,"end":407,"marker":"bibr","target":"#b19"},{"start":455,"end":458,"marker":"bibr","target":"#b5"},{"start":567,"end":571,"marker":"bibr","target":"#b16"},{"start":606,"end":609,"marker":"bibr","target":"#b12"},{"start":912,"end":916,"marker":"bibr","target":"#b13"},{"start":983,"end":987,"marker":"bibr","target":"#b15"},{"start":1012,"end":1016,"marker":"bibr","target":"#b19"}]},{"text":"As concerns the table identification step, we considered the DOMINUS framework [5] for document processing and management, and extended it with suitable techniques for table recognition. DOMINUS provides an integrated and general framework to manage digital libraries in which most knowledge-intensive tasks are carried out using intelligent techniques, among which Expert System and symbolic Machine Learning technologies play a predominant role. After submission, documents in different digital formats are processed to identify their layout structure, then to identify the kind of document and its relevant components, to extract the content from selected components and to exploit such a content for indexing and information extraction purposes. Hence, while the layout analysis phase is involved in table recognition, the information extraction step is concerned with table structure identification and subsequent content extraction.","refs":[{"start":79,"end":82,"marker":"bibr","target":"#b8"}]},{"text":"As to table recognition, DOMINUS deals with two kinds of digital documents: born-digital ones and digitized ones (typically obtained by scanning legacy paper sources). This distinction is relevant because the basic document components (text, images, geometric shapes-and specifically lines) are explicitly represented in born-digital documents (such as PDF ones), but not in digitized ones (usually coming in the form of raster images). Thus, in the latter case, suitable image processing techniques must be applied to identify them. In particular, horizontal and vertical lines are fundamental components for table recognition, although not sufficient (some tables do not show a perfect grid for visually highlighting their cell organization). Thus, in the case of document images, a variation of the Hough transform, specifically focused on horizontal/vertical lines, and on the identification of line segments, was developed and integrated in the DOMINUS framework. Then, the set of lines and other content blocks in a page were fed as an input to an expert module in charge of identifying and collecting the subsets of elements that together make up a table. Expert Systems technology was exploited because there is no standard representation for tables, and the many different styles used by different authors can vary significantly as regards the alignment of the content of rows and columns, the use of horizontal/vertical lines to separate portions of the table, and the position of the table in the page. Moreover, some tables are particularly tricky due to the presence of blank cells, or of cells that span several rows and/or columns. The expert component, whose detailed description is outside the scope of this paper, was able to identify and extract most tables in different kinds of documents, with some difficulties on very small tables and on multi-column documents.  As to table structure identification, our work specifically stems from a research stream carried on by Nagy et al. [15], specifically concerned with the extraction of the table structure and with its formal manipulation aimed at transposing the content into a relational representation that can be integrated in a typical database. They presented [15] a method based on header paths for efficient and complete extraction of labeled data from tables meant for humans. Header paths are a purely syntactic representation of visual tables that can be transformed (factored ) into existing representations of structured data such as category trees, relational tables, and RDF triples.","refs":[{"start":2001,"end":2005,"marker":"bibr","target":"#b18"},{"start":2233,"end":2237,"marker":"bibr","target":"#b18"}]},{"text":"A table contains a rectangular configuration of data cells, each of which can be uniquely referred by a row and a column index. As reported in [15], a table contains a set of content-cells that can be identified by a column-header path and a row-header-path. The table segmentation process aims at identifying four critical cells useful to partition the table into stub, row header, column header, and delta regions. In particular, this setting is concerned with six kinds of tablerelated elements, as shown in Figure 1: explaining the remaining part of the dimensions according to which the data are organized; Stub. One or more cells that correspond to the intersection between the horizontal projection of the row heading and the vertical projection of the column heading; Notes. One or more text lines following the table, aimed at explaining portions of its content.","refs":[{"start":143,"end":147,"marker":"bibr","target":"#b18"},{"start":518,"end":519,"marker":"figure","target":null}]},{"text":"Some details should be pointed out. First of all, the notes are optional, and hence might be missing in some tables. The row and column headings may be quite complex, when the table is intended to represent data that are conceptually distributed along more than two dimensions (as a side effect, this event typically causes the presence of cell content that spans over many rows or columns). The stub can be made up of just one cell (if both the row headings consist of a single column, and the column headings consist of a single row) or of many cells (in case of composite row and/or column headings); it may be empty, but it often contains a meta-header aimed at explaining the row and/or column headings. Thus, although the mutual position of these elements is known and fixed, identifying the specific boundaries of each of them may become very complex. To do this Nagy et al. [15] adopt an algorithmic approach, leveraging typical patterns. High accuracy should be required if the table data in the available documents are to be extensively and precisely extracted. Due to the many different kinds of tables that can be found in documents, and to the many different ways in which information can be organized in tables, we believe that a significant high accuracy cannot be reached by hand-written rules, but the characterizing essence of the above elements can be captured only using automatic techniques provided by Machine Learning.","refs":[{"start":882,"end":886,"marker":"bibr","target":"#b18"}]}]},{"title":"Proposed Approach","paragraphs":[{"text":"The first question to answer for applying Machine Learning to table structure recognition is the type of approach to be used. To answer this question, several aspects must be considered. A fundamental one concerns the kind of representation to be exploited. In this respect, it is quite clear that the most important feature to understand a table lies in its spacial structure, which in turn is made up of several relationships among the cells (both spacial and content ones). Indeed, it is self-evident that, when trying to understand a table, and specifically its components as described above, these are the parameters that any human considers. As a consequence, propositional techniques don't have a sufficient representational power to handle this kind of complexity, and first-order logic approaches must be considered. In particular, the following features/predicates were deemed as profitable for table description:","refs":[]},{"text":"-Table boundaries -Columns and Rows, and adjacency between them -Cells and their belonging to a given row and column -Cell content type It should be noted that, in the proposed setting, the whole set of elements (stub, table cells and headings, caption, notes) associated to a table is represented in a Comma Separated Values (CSV) file, and hence in this file not only the actual table elements, but also caption and notes (if any) are represented as cells. Thus, there is no structural hint in the CSV file to distinguish different kinds of elements. In particular, caption and notes are considered as belonging to a single cell (typically in the first column), and the content of multi-row or multi-column cells is assumed to be placed in the (top-left)-most cell.","refs":[]},{"text":"Another issue is the choice of classes to be learned. A straightforward possibility would be learning, for each cell, the type of table component to which it belongs. However, this would cause a significant growth in the number of examples, which would affect computational costs, and would be more difficult to handle in the subsequent classification phase (because each cell would be classified independently of the others (so that, for example, a data cell might be identified in the heading section). To solve the former problem, and to reduce the impact of the latter, a different solution was adopted. Four classes were defined as shown in Figure 2, that are non-redundant and are sufficient, alone, to univoquely determine the whole table structure:   In fact, either end stub or home data is redundant, because the home data cell is always assumed to be placed just one column to the right, and one row below, the end stub. Conversely, if classes are to be considered mutually exclusive, an additional class must be included, to specifically identify the case of a stub in which home stub and end stub coincide: single stub. The stub cell, in the case of a single-cell stub. Indeed, the captions can be identified as the content cell above the home stub row, and the notes as the content cells below the end data row; the column heading cells are those in the columns to the right of the end stub column and in the rows between the home stub row and the end stub row; the row heading cells are those in the rows below the end stub row and in the columns between the home stub column and the end stub column.","refs":[{"start":653,"end":654,"marker":"figure","target":null}]},{"text":"The last question concerns how rigid the learned models should be. Due to the problem being very multi-faceted, and to the lack of stable criteria to identify the table components, it is desirable that the learned models are quite flexible, with a preference for statistical approaches over purely logical ones.","refs":[]}]},{"title":"Lynx: A Statistical Relational Learning Approach","paragraphs":[{"text":"The SRL approach Lynx [3] was used here to tackle the specific problem of critical cells identification in tables. Lynx implements a probabilistic query-based classifier, using first-order logic as a representation language. A first-order alphabet consists of a set of constants, a set of variables, a set of function symbols, and a non-empty set of predicate symbols. Both function symbols and predicate symbols have a natural number (its arity) assigned. A term is a constant symbol, a variable symbol, or an n-ary function symbol f applied to n terms t 1 , t 2 , . . . , t n . An atom p(t 1 , . . . , t n ) is a predicate symbol p of arity n applied to n terms t i . An atom l and its negation l are said to be (resp., positive and negative) literals. Lynx adopts the relational framework, and the corresponding query mining algorithm, reported in [4].","refs":[{"start":22,"end":25,"marker":"bibr","target":"#b6"},{"start":851,"end":854,"marker":"bibr","target":"#b7"}]},{"text":"Feature Construction via Query Mining. The first step of Lynx carries out a feature construction process by mining frequent queries with an approach similar to that reported in [11]. The algorithm for frequent relational query mining is based on the same idea as the generic level-wise search method, known in data mining from the Apriori algorithm [1]. The algorithm starts with the most general queries. Then, at each step it tries to specialize all the candidate frequent queries, discarding the non-frequent queries and storing those whose length is equal to the user specified input parameter maxsize. For each new refined query, semantically equivalent queries are detected (using the θ OI -subsumption relation [7]) and discarded. The algorithm uses a background knowledge B containing the examples and a set of constraints that must be satisfied by the generated queries, among which: maxsize(M), maximal query length; type(p) and mode(p), denote, respectively, the type and the input/output mode of the predicate's arguments p, used to specify a language bias; key([p 1 , p 2 , . . . , p n ]) specifies that each query must have one of the predicates p 1 , p 2 , . . . ","refs":[{"start":177,"end":181,"marker":"bibr","target":"#b14"},{"start":349,"end":352,"marker":"bibr","target":"#b4"},{"start":718,"end":721,"marker":"bibr","target":"#b10"}]},{"text":"where X i ∈ X is a single relational example and Y i ∈ Y is the label associated to X i , the goal is to learn a function h : X → Y from D that predicts the label for each unseen instance. Let P, with |P| = d, be the set of constructed features obtained in the first step of the Lynx system (the queries mined from D), as previously reported. For each example X k ∈ X we can build a d-component vector-valued random variable x = (x 1 , x 2 , . . . , x d ) where each x i ∈ x is 1 if the query p i ∈ P subsumes example X k , and 0 otherwise, for each 1 ≤ i ≤ d.","refs":[]},{"text":"Using Bayes' theorem, if p(Y j ) describes the prior probability of class Y j , then the posterior probability p(Y j |x) can be computed from p(x|Y j ) as","refs":[]},{"text":". Given a set of discriminant functions g i (x), i = 1, . . . , Q, a classifier is said to assign vector x to class Y j if g j (x) > g i (x) for all j = i. Taking g i (x) = P (Y i |x), the maximum discriminant function corresponds to the maximum a posteriori (MAP) probability. For minimum error rate classification, the following discriminant function will be used","refs":[]},{"text":"We are considering a multi-class classification problem involving discrete features. In this problem the components of vector x are binary-valued and conditionally independent. In particular, let the component of vector x = (x 1 , . . . , x d ) be binary valued (0 or 1). We define","refs":[]},{"text":"with the components of x being statistically independent for all x i ∈ x. The factors p ij can be estimated by frequency counts on the training examples, as","refs":[]},{"text":"By assuming conditional independence we can write P (x|Y i ) as a product of the probabilities of the components of x. Given this assumption, a particularly convenient way of writing the class-conditional probabilities is as follows:","refs":[]},{"text":"Hence, Eq. 1 yields the discriminant function","refs":[]},{"text":"The factor corresponding to the prior probability for class Y j can be estimated from the training set as p","refs":[]},{"text":"The minimum probability of error is achieved by the following decision rule: decide","refs":[]},{"text":", where g i (•) is defined as in Eq. 2.","refs":[]},{"text":"Feature Selection with Stochastic Local Search. After constructing a set of features, and presenting a method to use those features to classify unseen examples, the problem is how to find a subset of these features that optimizes prediction accuracy. The optimization problem of selecting a subset of features with a superior classification performance may be formulated as follows. Let P be the constructed original set of queries, and let f : 2 |P| → R be a function scoring a selected subset X ⊆ P. The problem of feature selection is to find a subset X ⊆ P such that f ( X) = max Z⊆P f (Z). An exhaustive approach to this problem would require examining all 2 |P| possible subsets of the feature set P, making it impractical for even small values of |P|. The use of a stochastic local search procedure [8] allows to obtain good solutions without having to explore the whole solution space.","refs":[{"start":806,"end":809,"marker":"bibr","target":"#b11"}]},{"text":"Given a subset P ⊆ P, for each example X j ∈ X we let the classifier find the MAP hypothesis h P (X j ) = arg max i g i (x j ) by adopting the discriminant function reported in Eq. 1, where x j is the feature based representation of example X j obtained using queries in P . Hence the initial optimization problem corresponds to minimize the expectation E[1 hP (Xj ) =Yj ] where 1 hP (Xj ) =Yj is the characteristic function of training example X j returning 1 if h P (X j ) = Y j , and 0 otherwise. Finally, given D the training set with |D| = m and P a set of features, the number of classification errors made by the Bayesian model is err","refs":[]},{"text":"Consider a combinatorial optimization problem, where one is given a discrete set X of solutions and an objective function f : X → R to be minimized, and seek a solution x * ∈ X such that ∀x ∈ X : f (x * ) ≤ f (x). A method to find high-quality solutions for a combinatorial problem consists of a two-step approach made up of a greedy construction phase followed by a perturbative local search [8]. GRASP [6] solves the problem of the limited number of different candidate solutions generated by a greedy construction search method by randomizing the construction method. GRASP is an iterative process combining at each iteration a construction and a local search phase. In the construction phase a feasible solution is built, and then its neighborhood is explored by the local search. The Lynx system includes an implementation of the GRASP procedure in order to perform the feature selection task, as reported in [3].","refs":[{"start":393,"end":396,"marker":"bibr","target":"#b11"},{"start":404,"end":407,"marker":"bibr","target":"#b9"},{"start":914,"end":917,"marker":"bibr","target":"#b6"}]}]},{"title":"Problem Characterization and Validation","paragraphs":[{"text":"Lynx has been applied to a dataset consisting of 100 table descriptions. The dataset1 is a collection of tables randomly selected from ten large statistical web sites [14]. HTML tables are represented in Comma Separated Value (CSV) files. Information about each table cell (its contained value and its absolute position in terms of row and column index) are used to provide its relational representation to be exploited by Lynx. The goal is to correctly predict the label of the critical cells belonging to each table. In particular, each table cell has been labeled to belong to one of the following classes: caption, note, home data, end data, home stub, end stub, and single stub.","refs":[{"start":84,"end":85,"marker":null,"target":"#foot_0"},{"start":167,"end":171,"marker":"bibr","target":"#b17"}]},{"text":"Figure 3 reports a sample table description adopting the relational language we used. In particular, predicate label/2 indicates the corresponding class label of a cell; row/3 (resp., col/3) define the position and the identifier of a row (resp., column) belonging to the table; next row/2 (resp., next col/2) denote the spatial relationship between two adjacent rows (resp., columns); and finally, cell/4 specifies the type of a cell. Given a table (also including caption and notes, if any), a row/3 (resp., col/3) atom is introduced for each row (resp., column) of the CSV file, reporting as arguments the table identifier, the row (resp., column) identifier, and its index. Then, suitable next row/2 (resp., next col/2) atoms are introduced to link adjacent rows (resp., columns) to each other in the proper sequence. Finally, for each cell a cell/4 atom is added, reporting as arguments the corresponding identifier, the associated row and column identifiers, and the type of content.","refs":[{"start":7,"end":8,"marker":"figure","target":null}]},{"text":"Given a training set made up of the relational descriptions of critical cells belonging to each table, Lynx is applied in order to construct the relevant relational features maximizing the likelihood on the training data, as reported in Fig. 3. An example of a relational table description Section 3.1. After this first step the system build a model composed of probabilistic query such as label(A),cell(B,A,C,D),next row(C,E),cell(B, ,E,D), whose corresponding class probabilities are p(q|note) = 0.507, p(q|home data) = 0.944, p(q|caption) = 0.497, p(q|single stub) = 0.628, p(q|end data) = 0.000, p(q|end stub) = 0.714, and p(q|home stub) = 0.548. These probabilistic queries are then used to predict critical cells belonging to testing tables.","refs":[{"start":242,"end":243,"marker":"figure","target":null}]},{"text":"Table 1 reports the results obtained with Lynx with a 10-fold cross validation in terms of accuracy, Conditional Log Likelihood (CLL), and areas under the Receiver operating characteristic (ROC) and Precision Recall (PR) curve. As we can see from the table, the results are very promising. The two labels on which the system obtains best results are those regarding the data region.","refs":[{"start":6,"end":7,"marker":"table","target":"#tab_3"}]},{"text":"While, it has some difficulties in correctly classifying the labels single stub and end stub. The next step towards improving these results is to use some collective classification techniques.","refs":[]}]},{"title":"Conclusions","paragraphs":[{"text":"Tables are very informative components of documents, that compactly represent many inter-related data. It would be desirable to extract these data in order to make them available also outside the document. This requires to understand a table structure. Machine learning solutions may help to deal with the extreme variability in table styles and structures. We propose to exploit a first-order logic representation to capture the complex spatial relationships involved in a table structure, and a learning system that can mix the power of this representation with the flexibility of statistical approaches. On a dataset including different kinds of tables, encouraging results were obtained.","refs":[]},{"text":"As a future work we are trying to combine the prediction of single critical cell labels in order to improve the accuracy of the segmentation process. We will adopt a collective classification procedure whose aim should be to improve the likelihood of a prediction for a given label knowing the probability of the prediction made on the neighbor labeled cells with an iterative approach. The iterative procedure will combine expectation steps, predicting labels on the known distribution, and maximization steps, improving the probability distribution.","refs":[]}]}],"tables":{"tab_3":{"heading":"Table 1 .","description":"Accuracy, CLL, AUC of ROC and PR with a 10-fold cross validation","rows":[["","","AUC-ROC AUC-PR"],["","caption","0,984","0,951"],["","note","0,986","0,978"],["Accuracy CLL 90,69 -4,89 Dev.St. 0,017 2,38 Mean","home stub end stub single stub home data","0,987 0,983 0,989 0,991","0,925 0,825 0,810 0,968"],["","end data","1,000","0,998"],["","Mean","0,989","0,922"],["","Dev.St.","0,006","0,075"]]}},"abstract":{"title":"Abstract","paragraphs":[{"text":"Tables are among the most informative components of documents, because they are exploited to compactly and intuitively represent data, typically for understandability purposes. The needs are to identify and extract tables from documents, and, on the other hand, to be able to extract the data they contain. The latter task involves the understanding of a table structure. Due to the variability in style, size, and aims of tables, algorithmic approaches to this task can be insufficient, and the exploitation of machine learning systems may represent an effective solution. This paper proposes the exploitation of a first-order logic representation, that is able to capture the complex spatial relationships involved in a table structure, and of a learning system that can mix the power of this representation with the flexibility of statistical approaches. The obtained encouraging results suggest further investigation and refinement of the proposal.","refs":[]}]}}