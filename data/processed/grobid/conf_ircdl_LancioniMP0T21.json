{"bibliography":{"title":"Efficient Keyphrase Generation with GANs","authors":[{"person_name":{"surname":"Lancioni","first_name":"Giuseppe"},"affiliations":[{"department":null,"institution":"Università degli Studi di Udineailab","laboratory":null}],"email":null},{"person_name":{"surname":"Mohamed","first_name":"Saida"},"affiliations":[{"department":null,"institution":"Università degli Studi di Udineailab","laboratory":null}],"email":null},{"person_name":{"surname":"Portelli","first_name":"Beatrice"},"affiliations":[{"department":null,"institution":"Università degli Studi di Udineailab","laboratory":null}],"email":"portelli.beatrice@spes.uniud.itgiuseppe.serra"},{"person_name":{"surname":"Tasso","first_name":"Carlo"},"affiliations":[{"department":null,"institution":"Università degli Studi di Udineailab","laboratory":null}],"email":"carlo.tasso@uniud.it"}],"date":null,"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"keywords":["Reinforcement learning","Keyphrase generation","Gan"],"citations":{"b0":{"title":"Opinion Expression Mining by Exploiting Keyphrase Extraction","authors":[{"person_name":{"surname":"Berend","first_name":"G"},"affiliations":[],"email":null}],"date":{"year":"2011","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"IJCNLP","series":null,"scope":null},"b1":{"title":"Neural Keyphrase Generation via Reinforcement Learning with Adaptive Rewards","authors":[{"person_name":{"surname":"Chan","first_name":"H"},"affiliations":[],"email":null},{"person_name":{"surname":"Chen","first_name":"W"},"affiliations":[],"email":null},{"person_name":{"surname":"Wang","first_name":"L"},"affiliations":[],"email":null},{"person_name":{"surname":"King","first_name":"I"},"affiliations":[],"email":null}],"date":{"year":"2019","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b2":{"title":"Keyphrase Generation with Correlation Constraints","authors":[{"person_name":{"surname":"Chen","first_name":"J"},"affiliations":[],"email":null},{"person_name":{"surname":"Zhang","first_name":"X"},"affiliations":[],"email":null},{"person_name":{"surname":"Wu","first_name":"Y"},"affiliations":[],"email":null},{"person_name":{"surname":"Yan","first_name":"Z"},"affiliations":[],"email":null},{"person_name":{"surname":"Li","first_name":"Z"},"affiliations":[],"email":null}],"date":{"year":"2018","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b3":{"title":"An Integrated Approach for Keyphrase Generation via Exploring the Power of Retrieval and Extraction","authors":[{"person_name":{"surname":"Chen","first_name":"W"},"affiliations":[],"email":null},{"person_name":{"surname":"Chan","first_name":"H"},"affiliations":[],"email":null},{"person_name":{"surname":"Li","first_name":"P"},"affiliations":[],"email":null},{"person_name":{"surname":"Bing","first_name":"L"},"affiliations":[],"email":null},{"person_name":{"surname":"King","first_name":"I"},"affiliations":[],"email":null}],"date":{"year":"2019","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b4":{"title":"Title-Guided Encoding for Keyphrase Generation","authors":[{"person_name":{"surname":"Chen","first_name":"W"},"affiliations":[],"email":null},{"person_name":{"surname":"Gao","first_name":"Y"},"affiliations":[],"email":null},{"person_name":{"surname":"Zhang","first_name":"J"},"affiliations":[],"email":null},{"person_name":{"surname":"King","first_name":"I"},"affiliations":[],"email":null},{"person_name":{"surname":"Lyu","first_name":"M"},"affiliations":[],"email":null}],"date":{"year":"2019","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b5":{"title":"Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation","authors":[{"person_name":{"surname":"Cho","first_name":"K"},"affiliations":[],"email":null},{"person_name":{"surname":"Van Merriënboer","first_name":"B"},"affiliations":[],"email":null},{"person_name":{"surname":"Gulcehre","first_name":"C"},"affiliations":[],"email":null},{"person_name":{"surname":"Bahdanau","first_name":"D"},"affiliations":[],"email":null},{"person_name":{"surname":"Bougares","first_name":"F"},"affiliations":[],"email":null},{"person_name":{"surname":"Schwenk","first_name":"H"},"affiliations":[],"email":null},{"person_name":{"surname":"Bengio","first_name":"Y"},"affiliations":[],"email":null}],"date":{"year":"2014","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b6":{"title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","authors":[{"person_name":{"surname":"Devlin","first_name":"J"},"affiliations":[],"email":null},{"person_name":{"surname":"Chang","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Lee","first_name":"K"},"affiliations":[],"email":null},{"person_name":{"surname":"Toutanova","first_name":"K"},"affiliations":[],"email":null}],"date":{"year":"2018","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b7":{"title":"Incorporating Copying Mechanism in Sequenceto-Sequence Learning","authors":[{"person_name":{"surname":"Gu","first_name":"J"},"affiliations":[],"email":null},{"person_name":{"surname":"Lu","first_name":"Z"},"affiliations":[],"email":null},{"person_name":{"surname":"Li","first_name":"H"},"affiliations":[],"email":null},{"person_name":{"surname":"Li","first_name":"V"},"affiliations":[],"email":null}],"date":{"year":"2016","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b8":{"title":"CorePhrase: Keyphrase Extraction for Document Clustering","authors":[{"person_name":{"surname":"Hammouda","first_name":"K"},"affiliations":[],"email":null},{"person_name":{"surname":"Matute","first_name":"D"},"affiliations":[],"email":null},{"person_name":{"surname":"Kamel","first_name":"M"},"affiliations":[],"email":null}],"date":{"year":"2005","month":null,"day":null},"ids":null,"target":null,"publisher":"MLDM","journal":null,"series":null,"scope":null},"b9":{"title":"Improved Automatic Keyword Extraction Given More Linguistic Knowledge","authors":[{"person_name":{"surname":"Hulth","first_name":"A"},"affiliations":[],"email":null}],"date":{"year":"2003","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b10":{"title":"A Study on Automatically Extracted Keywords in Text Categorization","authors":[{"person_name":{"surname":"Hulth","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Megyesi","first_name":"B"},"affiliations":[],"email":null}],"date":{"year":"2006","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b11":{"title":"Phrasier: A System for Interactive Document Retrieval Using Keyphrases","authors":[{"person_name":{"surname":"Jones","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Staveley","first_name":"M"},"affiliations":[],"email":null}],"date":{"year":"1999","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b12":{"title":"SemEval-2010 Task 5 : Automatic Keyphrase Extraction from Scientific Articles","authors":[{"person_name":{"surname":"Kim","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Medelyan","first_name":"O"},"affiliations":[],"email":null},{"person_name":{"surname":"Kan","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Baldwin","first_name":"T"},"affiliations":[],"email":null}],"date":{"year":"2010","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b13":{"title":"Adam: A Method for Stochastic Optimization","authors":[{"person_name":{"surname":"Kingma","first_name":"D"},"affiliations":[],"email":null},{"person_name":{"surname":"Ba","first_name":"J"},"affiliations":[],"email":null}],"date":{"year":"2015","month":null,"day":null},"ids":null,"target":null,"publisher":"ICLR","journal":null,"series":null,"scope":null},"b14":{"title":"Large Dataset for Keyphrases Extraction","authors":[{"person_name":{"surname":"Krapivin","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Autaeu","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Marchese","first_name":"M"},"affiliations":[],"email":null}],"date":{"year":"2009","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b15":{"title":"Clustering to Find Exemplar Terms for Keyphrase Extraction","authors":[{"person_name":{"surname":"Liu","first_name":"Z"},"affiliations":[],"email":null},{"person_name":{"surname":"Li","first_name":"P"},"affiliations":[],"email":null},{"person_name":{"surname":"Zheng","first_name":"Y"},"affiliations":[],"email":null},{"person_name":{"surname":"Sun","first_name":"M"},"affiliations":[],"email":null}],"date":{"year":"2009","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b16":{"title":"Fixing Weight Decay Regularization in Adam","authors":[{"person_name":{"surname":"Loshchilov","first_name":"I"},"affiliations":[],"email":null},{"person_name":{"surname":"Hutter","first_name":"F"},"affiliations":[],"email":null}],"date":{"year":"2017","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b17":{"title":"Scientific Information Extraction with Semi-supervised Neural Tagging","authors":[{"person_name":{"surname":"Luan","first_name":"Y"},"affiliations":[],"email":null},{"person_name":{"surname":"Ostendorf","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Hajishirzi","first_name":"H"},"affiliations":[],"email":null}],"date":{"year":"2017","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b18":{"title":"Deep Keyphrase Generation","authors":[{"person_name":{"surname":"Meng","first_name":"R"},"affiliations":[],"email":null},{"person_name":{"surname":"Zhao","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Han","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"He","first_name":"D"},"affiliations":[],"email":null},{"person_name":{"surname":"Brusilovsky","first_name":"P"},"affiliations":[],"email":null},{"person_name":{"surname":"Chi","first_name":"Y"},"affiliations":[],"email":null}],"date":{"year":"2017","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b19":{"title":"TextRank: Bringing Order into Text","authors":[{"person_name":{"surname":"Mihalcea","first_name":"R"},"affiliations":[],"email":null},{"person_name":{"surname":"Tarau","first_name":"P"},"affiliations":[],"email":null}],"date":{"year":"2004","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b20":{"title":"Keyphrase Extraction in Scientific Publications","authors":[{"person_name":{"surname":"Nguyen","first_name":"T"},"affiliations":[],"email":null},{"person_name":{"surname":"Kan","first_name":"M"},"affiliations":[],"email":null}],"date":{"year":"2007","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"ICADL","series":null,"scope":null},"b21":{"title":"Sequence Level Training with Recurrent Neural Networks","authors":[{"person_name":{"surname":"Ranzato","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Chopra","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Auli","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Zaremba","first_name":"W"},"affiliations":[],"email":null}],"date":{"year":"2016","month":null,"day":null},"ids":null,"target":null,"publisher":"ICLR","journal":null,"series":null,"scope":null},"b22":{"title":"Self-Critical Sequence Training for Image Captioning","authors":[{"person_name":{"surname":"Rennie","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Marcheret","first_name":"E"},"affiliations":[],"email":null},{"person_name":{"surname":"Mroueh","first_name":"Y"},"affiliations":[],"email":null},{"person_name":{"surname":"Ross","first_name":"J"},"affiliations":[],"email":null},{"person_name":{"surname":"Goel","first_name":"V"},"affiliations":[],"email":null}],"date":{"year":"2017","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b23":{"title":"Keyphrase Generation for Scientific Articles using GANs","authors":[{"person_name":{"surname":"Swaminathan","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Gupta","first_name":"R"},"affiliations":[],"email":null},{"person_name":{"surname":"Zhang","first_name":"H"},"affiliations":[],"email":null},{"person_name":{"surname":"Mahata","first_name":"D"},"affiliations":[],"email":null},{"person_name":{"surname":"Gosangi","first_name":"R"},"affiliations":[],"email":null},{"person_name":{"surname":"Shah","first_name":"R"},"affiliations":[],"email":null}],"date":{"year":"2019","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b24":{"title":"Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning","authors":[{"person_name":{"surname":"Williams","first_name":"R"},"affiliations":[],"email":null}],"date":{"year":"1992","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b25":{"title":"KEA: Practical Automatic Keyphrase Extraction","authors":[{"person_name":{"surname":"Witten","first_name":"I"},"affiliations":[],"email":null},{"person_name":{"surname":"Paynter","first_name":"G"},"affiliations":[],"email":null},{"person_name":{"surname":"Frank","first_name":"E"},"affiliations":[],"email":null},{"person_name":{"surname":"Gutwin","first_name":"C"},"affiliations":[],"email":null},{"person_name":{"surname":"Nevill-Manning","first_name":"C"},"affiliations":[],"email":null}],"date":{"year":"1999","month":null,"day":null},"ids":null,"target":null,"publisher":"ACM","journal":null,"series":null,"scope":null},"b26":{"title":"HuggingFace's Transformers: Stateof-the-art Natural Language Processing","authors":[{"person_name":{"surname":"Wolf","first_name":"T"},"affiliations":[],"email":null},{"person_name":{"surname":"Debut","first_name":"L"},"affiliations":[],"email":null},{"person_name":{"surname":"Sanh","first_name":"V"},"affiliations":[],"email":null},{"person_name":{"surname":"Chaumond","first_name":"J"},"affiliations":[],"email":null},{"person_name":{"surname":"Delangue","first_name":"C"},"affiliations":[],"email":null},{"person_name":{"surname":"Moi","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Cistac","first_name":"P"},"affiliations":[],"email":null},{"person_name":{"surname":"Rault","first_name":"T"},"affiliations":[],"email":null},{"person_name":{"surname":"Louf","first_name":"R"},"affiliations":[],"email":null},{"person_name":{"surname":"Funtowicz","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Brew","first_name":"J"},"affiliations":[],"email":null}],"date":{"year":"2019","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b27":{"title":"Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation","authors":[{"person_name":{"surname":"Wu","first_name":"Y"},"affiliations":[],"email":null},{"person_name":{"surname":"Schuster","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Chen","first_name":"Z"},"affiliations":[],"email":null},{"person_name":{"surname":"Le","first_name":"Q"},"affiliations":[],"email":null},{"person_name":{"surname":"Norouzi","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Macherey","first_name":"W"},"affiliations":[],"email":null},{"person_name":{"surname":"Krikun","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Cao","first_name":"Y"},"affiliations":[],"email":null},{"person_name":{"surname":"Gao","first_name":"Q"},"affiliations":[],"email":null},{"person_name":{"surname":"Macherey","first_name":"K"},"affiliations":[],"email":null},{"person_name":{"surname":"Klingner","first_name":"J"},"affiliations":[],"email":null},{"person_name":{"surname":"Shah","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Johnson","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Liu","first_name":"X"},"affiliations":[],"email":null},{"person_name":{"surname":"Kaiser","first_name":"L"},"affiliations":[],"email":null},{"person_name":{"surname":"Gouws","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Kato","first_name":"Y"},"affiliations":[],"email":null},{"person_name":{"surname":"Kudo","first_name":"T"},"affiliations":[],"email":null},{"person_name":{"surname":"Kazawa","first_name":"H"},"affiliations":[],"email":null},{"person_name":{"surname":"Stevens","first_name":"K"},"affiliations":[],"email":null},{"person_name":{"surname":"Kurian","first_name":"G"},"affiliations":[],"email":null},{"person_name":{"surname":"Patil","first_name":"N"},"affiliations":[],"email":null},{"person_name":{"surname":"Wang","first_name":"W"},"affiliations":[],"email":null},{"person_name":{"surname":"Young","first_name":"C"},"affiliations":[],"email":null},{"person_name":{"surname":"Smith","first_name":"J"},"affiliations":[],"email":null},{"person_name":{"surname":"Riesa","first_name":"J"},"affiliations":[],"email":null},{"person_name":{"surname":"Rudnick","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Vinyals","first_name":"O"},"affiliations":[],"email":null},{"person_name":{"surname":"Corrado","first_name":"G"},"affiliations":[],"email":null},{"person_name":{"surname":"Hughes","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Dean","first_name":"J"},"affiliations":[],"email":null}],"date":{"year":"2016","month":null,"day":null},"ids":null,"target":null,"publisher":"CoRR","journal":null,"series":null,"scope":null},"b28":{"title":"Semi-Supervised Learning for Neural Keyphrase Generation","authors":[{"person_name":{"surname":"Ye","first_name":"H"},"affiliations":[],"email":null},{"person_name":{"surname":"Wang","first_name":"L"},"affiliations":[],"email":null}],"date":{"year":"2018","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b29":{"title":"Semi-Supervised Learning for Neural Keyphrase Generation","authors":[{"person_name":{"surname":"Ye","first_name":"H"},"affiliations":[],"email":null},{"person_name":{"surname":"Wang","first_name":"L"},"affiliations":[],"email":null}],"date":{"year":"2018","month":null,"day":null},"ids":{"DOI":null,"arXiv":"arXiv:1808.06773"},"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b30":{"title":"SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient","authors":[{"person_name":{"surname":"Yu","first_name":"L"},"affiliations":[],"email":null},{"person_name":{"surname":"Zhang","first_name":"W"},"affiliations":[],"email":null},{"person_name":{"surname":"Wang","first_name":"J"},"affiliations":[],"email":null},{"person_name":{"surname":"Yu","first_name":"Y"},"affiliations":[],"email":null}],"date":{"year":"2016","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b31":{"title":"Generating Diverse Numbers of Diverse Keyphrases","authors":[{"person_name":{"surname":"Yuan","first_name":"X"},"affiliations":[],"email":null},{"person_name":{"surname":"Wang","first_name":"T"},"affiliations":[],"email":null},{"person_name":{"surname":"Meng","first_name":"R"},"affiliations":[],"email":null},{"person_name":{"surname":"Thaker","first_name":"K"},"affiliations":[],"email":null},{"person_name":{"surname":"He","first_name":"D"},"affiliations":[],"email":null},{"person_name":{"surname":"Trischler","first_name":"A"},"affiliations":[],"email":null}],"date":{"year":"2018","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b32":{"title":"Keyphrase Extraction Using Deep Recurrent Neural Networks on Twitter","authors":[{"person_name":{"surname":"Zhang","first_name":"Q"},"affiliations":[],"email":null},{"person_name":{"surname":"Wang","first_name":"Y"},"affiliations":[],"email":null},{"person_name":{"surname":"Gong","first_name":"Y"},"affiliations":[],"email":null},{"person_name":{"surname":"Huang","first_name":"X"},"affiliations":[],"email":null}],"date":{"year":"2016","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b33":{"title":"World Wide Web site summarization","authors":[{"person_name":{"surname":"Zhang","first_name":"Y"},"affiliations":[],"email":null},{"person_name":{"surname":"Zincir-Heywood","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Milios","first_name":"E"},"affiliations":[],"email":null}],"date":{"year":"2004","month":null,"day":null},"ids":null,"target":null,"publisher":"Web Intelligence and Agent Systems","journal":null,"series":null,"scope":null}},"sections":[{"title":"Introduction","paragraphs":[{"text":"A keyphrase is a sequence of words that summarizes the content of a whole document and expresses its core concepts. High quality keyphrases (KPs) can facilitate the understanding of a document and they are used to provide and retrieve information regarding the whole document on a high level. The world wide growth of digital libraries has made the task of automatic KP prediction both useful and necessary. There are many topics in which such ability could be positively applied, such as text summarization [34], opinion mining [1], document clustering [9], information retrieval [12] and text categorization [11].","refs":[{"start":508,"end":512,"marker":"bibr","target":"#b33"},{"start":529,"end":532,"marker":"bibr","target":"#b0"},{"start":554,"end":557,"marker":"bibr","target":"#b8"},{"start":581,"end":585,"marker":"bibr","target":"#b11"},{"start":610,"end":614,"marker":"bibr","target":"#b10"}]},{"text":"KPs can be either present or absent. Present KPs are exact substrings of the document and can be extracted from its text, while absent KPs are sequences of words which do not exist in the text, but can be abstracted from its contents. They are also referred to as extractive and abstractive KPs, respectively. The research community has made great efforts in the task of predicting KPs so far, and the proposed solutions all rely on the following two main approaches: 1. extraction of sequences of words from the document (automatic extractive methods); 2. generation of words and phrases related to the document (automatic abstractive methods).","refs":[]},{"text":"Extractive approaches are only able to deal with present KPs [29,18,33]: the greatest drawback in this case is that predicted KPs are related to the written content of the source document, and not to its semantic meaning.","refs":[{"start":61,"end":65,"marker":"bibr","target":"#b28"},{"start":65,"end":68,"marker":"bibr","target":"#b17"},{"start":68,"end":71,"marker":"bibr","target":"#b32"}]},{"text":"The other main approach is the abstractive one, which has been introduced to address the limitations of the extractive approaches and to better mimic the way humans assign KPs to a given document. Despite it being a more recent line of research, there are a good number of studies tackling this problem [19,3,4]. Abstractive methods are designed to produce sets of KPs which are not strictly related to the words of source text. In principle this method could be used to predict both absent and present KPs from a given source text. Generative models are best suited for abstractive approaches. A lot of examples can be found in literature in which such kind of models are used, mainly leveraging the Encode-Decoder framework [19,3]. This architecture works by compressing the contents of the input (e.g. the text document) into a hidden representation using an Encoder module. The same representation is then decompressed using the Decoder module, which returns the desired output (e.g. a sequence of KPs). Recently, Generative Adversarial Networks (GANs) have been introduced in text generation task [31], and in particular in keyphrase generation [24]. GANs are based on an architecture that simultaneously trains two models: a generative model that captures the data distribution, and a discriminative model that estimates the probability that a sample came from the (real) training data rather than from the generator. The aforementioned approaches rely on the use of large datasets to perform training, with a high consumption in terms of computational resources.","refs":[{"start":303,"end":307,"marker":"bibr","target":"#b18"},{"start":307,"end":309,"marker":"bibr","target":"#b2"},{"start":309,"end":311,"marker":"bibr","target":"#b3"},{"start":726,"end":730,"marker":"bibr","target":"#b18"},{"start":730,"end":732,"marker":"bibr","target":"#b2"},{"start":1102,"end":1106,"marker":"bibr","target":"#b30"},{"start":1150,"end":1154,"marker":"bibr","target":"#b23"}]},{"text":"In this paper we introduce a new GAN architecture for keyphrase generation with a focus on data efficiency: the aim is to only use a small subset of the training data and still achieve reasonably good results. The main contribution of our approach is the introduction of a novel Discriminator model based on BERT that is able to distinguish between human and machine-generated keyphrases leveraging on the powerful language model of BERT. A Reinforcement Learning strategy has been used in our architecture to overcome the problems given by the direct application of GAN in text generation. Our architecture achieved competitive results using only 1% of the available training samples compared to previous approaches.","refs":[]}]},{"title":"Related Work","paragraphs":[]},{"title":"Automatic Keyphrase Extraction","paragraphs":[{"text":"Many different extractive approaches have been proposed in the literature, but most of them consist of the following two steps. Firstly, a reasonable number of KP candidates are extracted. The number of candidates usually exceeds the number of correct candidates and it is selected using heuristic methods. Secondly, a ranking algorithm is used to give a score to each candidate based on the source text. This whole process can be performed either in a supervised or unsupervised fashion. For supervised methods, this task is treated as a binary classification task [26,21], and gives positive scores to the correct candidates in the list. Unsupervised methods aim to find central nodes of the text graph [20], or detect phrases from topical clusters [16].","refs":[{"start":566,"end":570,"marker":"bibr","target":"#b25"},{"start":570,"end":573,"marker":"bibr","target":"#b20"},{"start":705,"end":709,"marker":"bibr","target":"#b19"},{"start":751,"end":755,"marker":"bibr","target":"#b15"}]},{"text":"There are also other studies that differ from the previously described pipeline. For example the authors in [33] applied an alignment model to learn the conversion from the source text to target KPs. Also, recurrent neural network have been used to build sequence labeling models to extract KPs from tweets [33].","refs":[{"start":108,"end":112,"marker":"bibr","target":"#b32"},{"start":307,"end":311,"marker":"bibr","target":"#b32"}]}]},{"title":"Automatic Keyphrase Generation","paragraphs":[{"text":"Abstractive methods represent an important approach which is gaining a growing attention, as they allow to generate results which are more in the line of human expectations. Sequence-to-sequence (Seq2seq) models showed great success in keyphrase generation and can generate human-like results. They are based on the Encoder-Decoder framework, where the Encoder generates a semantic representation of the source text and the Decoder is responsible for generating target texts based on such semantic representation. CopyRNN [19] was the first specific Encoder-Decoder model to be used in the topic of keyphrase generation; it incorporates an attention mechanism. CorrRNN [3] was introduced later and focused on capturing the correlation between KPs. TG-Net [5] exploits the information given by the title to learn a better representation for the input documents. Chen et al. [4] leveraged extractive models to improve the performance of the (abstractive) keyphrase generation one. Ye et al. [30] proposed a semi-supervised approach considering a limited training dataset to improve the performance. All the previous approaches used the beam search algorithm to generate large number of KPs from which to choose the k-best ones as final predictions. CatSeq and CatSeqD [32] were the first two recurrent generative models with the ability to predict the appropriate number of predicted KPs for each document (instead of predicting a fixed number of KPs for each sample). CatSeq proposed several novelties. Firstly, an orthogonal regularization module to prevent the model from predicting the same word after generating the KP separator token. Secondly, semantic coverage, a self-supervised technique with the aim of enhancing the semantic content of the predictions.","refs":[{"start":522,"end":526,"marker":"bibr","target":"#b18"},{"start":669,"end":672,"marker":"bibr","target":"#b2"},{"start":755,"end":758,"marker":"bibr","target":"#b4"},{"start":873,"end":876,"marker":"bibr","target":"#b3"},{"start":989,"end":993,"marker":"bibr","target":"#b29"},{"start":1266,"end":1270,"marker":"bibr","target":"#b31"}]},{"text":"Reinforcement Learning has been used in a wide range of text generation tasks [28,22]. The generative models CatSeq, CatSeqD, CorrRNN and TG-Net have been improved by applying a Reinforcement Learning (RL) approach with adaptive reward to produce their improved versions catSeq-2RF1, catSeqD-2RF1, catSeqCorr-2RF1 and catSeqTG-2RF1 [2]. In [24], the authors propose a keyphrase generation approach using Generative Adversarial Networks (GAN) conditioned on scientific documents. The architecture is composed of a CatSeq model as Generator and a hierarchical attention-based model as Discriminator. This was the first attempt to apply GAN in the Keyphrase generation task. This approach was able to show improvements in the generation of abstractive KPs, but no significant improvements in extractive KPs.","refs":[{"start":78,"end":82,"marker":"bibr","target":"#b27"},{"start":82,"end":85,"marker":"bibr","target":"#b21"},{"start":332,"end":335,"marker":"bibr","target":"#b1"},{"start":340,"end":344,"marker":"bibr","target":"#b23"}]}]},{"title":"The proposed approach","paragraphs":[{"text":"The novelty of the approach presented in this paper is two-fold: first, we introduce a BERT Discriminator as part of a Generative Adversarial Networks (GAN) architecture for the keyphrase generation task; and second, we train our system with only a small amount of the available data to pursue data efficiency.","refs":[]},{"text":"A general overview of the implemented system is given in Figure 1. It is based on two main components: a state-of-the-art Generator that relies on the Encoder-Decoder model and is able to generate a list of KPs for a given input text, and the new BERT-based Discriminator that is trained to separate the true KPs from the fake ones by giving them a score: the higher the score, the more likely is the keyphrase list to be real.","refs":[{"start":64,"end":65,"marker":"figure","target":"#fig_1"}]},{"text":"To overcome the well known problems of differentiability that arise when employing GAN architectures for text generation, Reinforcement Learning (RL) paradigm is adopted for training the system [31].","refs":[{"start":194,"end":198,"marker":"bibr","target":"#b30"}]}]},{"title":"Formal Problem Definition","paragraphs":[{"text":"A source document x and the related list of M ground-truth keyphrases y = (y 1 , y 2 , . . . , y M ) (True KPs) are represented by the pair (x, y). Both x and y i are sequences of words:","refs":[]},{"text":"where L and K i are the number of words of x and of its i-th KP respectively. A keyphrase generation model will predict a set of keyphrases ŷ = (ŷ 1 , ŷ2 , . . . , ŷN ) (Fake KPs) with the aim to reproduce the true ones, so that ŷ ≡ y.","refs":[]}]},{"title":"Details of the System","paragraphs":[{"text":"Generator The task of the Generator G is to take a source document x and generate a sequence of predicted KPs ŷ. For our system we chose catSeq [32] that is based on the CopyRNN [19], a generative model optimized for KP generation. It introduces the ability to predict a sequence of KPs that is obtained by concatenating together target KPs separated by a special token. In this way the training schema moves from one-to-many to one-to-seq, and the system can be trained to generate a variable number of KPs. It also employs the Copy Mechanism [8] to deal with long-tail words, which are the less frequent words in the vocabulary of the input samples. They are removed to gain efficiency during training, but being frequently very specific for the topic of the document, they could be part of KPs. The Copy Mechanism employs a positional attention to give a score to the words surroundings the ones which were removed, recovering the best scoring ones. Implementation relies on a bidirectional Gated Recurrent Unit (GRU) [6] for the encoder, and a forward GRU for the decoder.   Discriminator The Discriminator D is basically a binary classifier whose aim is to separate the true samples (x, y) from the fake ones (x, ŷ). It performs this task by computing a regression score for each sample, giving a high value to the reputedly real samples and a low value to the others. We introduce a novel BERT-based model for our Discriminator. BERT [7] is part of the Transformer architecture, and since its introduction has achieved state-of-the-art results in many Natural Language Processing tasks. Our implementation is based on a BERT pretrained model, fine-tuned for Sequence Classification. The input samples are processed in four steps as (see Figure 1):","refs":[{"start":144,"end":148,"marker":"bibr","target":"#b31"},{"start":178,"end":182,"marker":"bibr","target":"#b18"},{"start":544,"end":547,"marker":"bibr","target":"#b7"},{"start":1021,"end":1024,"marker":"bibr","target":"#b5"},{"start":1440,"end":1443,"marker":"bibr","target":"#b6"},{"start":1750,"end":1751,"marker":"figure","target":"#fig_1"}]},{"text":"1. Input Preparation. Input pairs (x, y) are first lower-cased and tokenized, then the tokens are concatenated together in the form","refs":[]},{"text":"where <x> and <yi> are the sequences of tokens of the input document and of the i-th KP; [CLS] is the BERT special token marking the start of the sequence;","refs":[]},{"text":"[SEP] is the BERT special token for marking the end of the sequence, also used to separate the input document from the list of related KPs; and the semicolon <;> is the KP separator.","refs":[]},{"text":"2. BERT Modelling. The prepared input sequence is passed through the 12 consecutive Encoder blocks of the pretrained BERT model. Pretrained weights act as initialization, and are optimized during training. Since BERT processing is positional, each input token is mapped to its corresponding output.","refs":[]},{"text":"3. Output Aggregation. Output tokens are averaged together to obtain an Embedding of the whole input sequence. Note that generally when using a BERTbased model, the output of the [CLS] token is usually considered as a sentence embedding. Nevertheless, we averaged all the output tokens, as this aggregated value has proven to be a better estimate of the semantic content of the input (see also [7]).","refs":[{"start":394,"end":397,"marker":"bibr","target":"#b6"}]},{"text":"4. Regression Layer. The sentence embedding is passed through a dense layer that evaluates a Regression score. This score is used to perform the classification of the input samples: the higher the score is, the more probable that the sample is a real one. The same score is also used as the Reward given by the Discriminator to the Generator in the Reinforcement Learning Reinforcement Learning with Policy Gradient We follow the Reinforcement Learning paradigm to train the system, as proposed in [31,24].","refs":[{"start":498,"end":502,"marker":"bibr","target":"#b30"},{"start":502,"end":505,"marker":"bibr","target":"#b23"}]},{"text":"In detail, we consider the Generator G as an agent whose action a at step t is to generate a word ŷt which is part of the set ŷ of predicted KPs for the document x. Action a is performed following the policy π(ŷ t |s t , x, θ) that represents the probability of sampling ŷt given the state s t = (ŷ 1 , . . . , ŷt-1 ), the sequence of words generated up to step t -1. The policy is differentiable with respect to the parameters θ of G. As the agent G generates the predicted list of KPs, the Discriminator D, that plays the role of the environment, evaluates them and gives back a reward:","refs":[]},{"text":"where r(ŷ t |s t ) is the expected accumulative reward at step t and T denotes the steps needed to generate the whole prediction ŷ. The aim of the agent G is to maximize the function J(θ) defined as the expected value of the final reward under the distribution of probability given by the policy π:","refs":[]},{"text":"The gradient of J(θ) is evaluated by means of the policy gradient theorem and the REINFORCE algorithm [25]:","refs":[{"start":102,"end":106,"marker":"bibr","target":"#b24"}]},{"text":"Expectation E π in Equation 3 can be approximated by sampling with ŷ ∼ π(•|x, θ). Then, defining the loss function of G as L(θ) = -J(θ), an estimator of its gradient is:","refs":[]},{"text":"A regularization term b t has been introduced as an expected accumulative reward evaluated on a greedy decoded sequence of predictions, as suggested in [23]. Its aim is two-fold: to lower the variance of the process, and to support the predictions with a higher reward with respect to the greedy decoded sequence.  ","refs":[{"start":152,"end":156,"marker":"bibr","target":"#b22"}]}]},{"title":"GAN Training","paragraphs":[]},{"title":"Datasets","paragraphs":[{"text":"Five well known datasets largely used in literature have been considered in this work:","refs":[]},{"text":"KP20k [19] 567,830 titles and abstracts from papers about computer science; of them, 20,000 samples are usually employed for testing, another 20,000 for validation, and the remaining 527,830 samples for training. This is the only dataset used for training duties. In our data-efficient training approach we only use 2,000 out of the >500,000 training samples.","refs":[{"start":6,"end":10,"marker":"bibr","target":"#b18"}]},{"text":"Inspec [10] 2,000 abstracts from disciplines: Computers and Control, and Information Technology. Only 500 samples are used for testing.","refs":[{"start":7,"end":11,"marker":"bibr","target":"#b9"}]},{"text":"Krapivin [15] 500 articles about computer science. Since no hint is given by the authors on how to split testing data, the first 400 samples in alphabetical order are taken for testing.","refs":[{"start":9,"end":13,"marker":"bibr","target":"#b14"}]},{"text":"NUS [21] 211 papers selected from scientific publications; used for testing. Semeval2010 [13] 288 articles from the ACL Computer Library, of whose 100 are used for testing. Some statistics about test samples are given in Table 1. Procedures that are standard protocol in KP generation are applied to data (see for example [2]): all duplicate documents are removed from training set; for each document the sequence of KPs is given in order of appearance; digits are replaced with the <digit> token; out of vocabulary words are replaced with the <unk> token.","refs":[{"start":4,"end":8,"marker":"bibr","target":"#b20"},{"start":89,"end":93,"marker":"bibr","target":"#b12"},{"start":227,"end":228,"marker":"table","target":"#tab_1"},{"start":322,"end":325,"marker":"bibr","target":"#b1"}]},{"text":"The vocabulary of the generator V G consists of the 50,000 most frequent words in the training dataset. The vocabulary of the discriminator V D is the one of the pretrained BERT base uncased (english version): it contains 30,522 words and chunks of words (called wordpieces) eventually used to compose all the possible flections (e.g.: 'hexahedral' is tokenized as 'he', '##xa', '##hedral'). ","refs":[]}]},{"title":"Comparative Results","paragraphs":[{"text":"The system has been trained with 2,000 samples randomly extracted from KP20k training dataset, and then evaluated using the five baseline datasets described in Section 4.1. A comparison has been carried out with four state-ofthe-art approaches, namely catSeqD [32]; catSeqCorr-2RF1 and catSeqTG-2RF1 [2], and GAN [24]. Results are shown in terms of F 1 score: F 1@5 is evaluated over the top 5 high scoring KPs, while F 1@M takes into account all the predictions. Results are shown in Table 2.","refs":[{"start":260,"end":264,"marker":"bibr","target":"#b31"},{"start":300,"end":303,"marker":"bibr","target":"#b1"},{"start":313,"end":317,"marker":"bibr","target":"#b23"},{"start":491,"end":492,"marker":"table","target":"#tab_2"}]},{"text":"Our approach achieves competitive results with respect of the above mentioned models. In depth, it is by far the best on Inspec, both in F 1@M and F 1@5 scores. Also it performs very well on Semeval2010, where we match the best F 1@M score and are close to the best F 1@5. Note that Semeval2010 is the smallest of the five testing datasets and contains the smallest amount of target KPs. This makes it a very difficult test set to perform well on.","refs":[]},{"text":"We point out that our approach shows good performance in the F 1@5 score. Since the F 1@5 score is evaluated taking the best 5 predicted KPs, we claim that our approach is able to generate good quality KPs.","refs":[]},{"text":"A final consideration has to be made about Equation 3: the expectation of the policy function is evaluated using only complete sequences ŷ, and this determines relatively large oscillations in the ∇J, inducing instability in the training process [31]. Our system has shown a great efficiency in dealing with this problem, even in a training scenario characterized by the scarce availability of resources in terms of data. In fact, we observed a trend to a quick convergence of the training, obtaining the best results at second iteration of the Generator (G 2 ). We consider that this quick convergence was achieved due to the strength of the language model embedded in the architecture. ","refs":[{"start":52,"end":53,"marker":"formula","target":"#formula_4"},{"start":246,"end":250,"marker":"bibr","target":"#b30"}]}]},{"title":"Model","paragraphs":[{"text":"KP20k Inspec Krapivin NUS Semeval2010 F1@M F1@5 F1@M F1@5 F1@M F1@5 F1@M F1@5 F1@M F1@5 catSeqD [  MAP is defined as the mean of the average precision P scores evaluated for each set of predicted KPs. It is a measure of the proportion of relevant KPs among the predicted ones. nDCG is a measure of the usefulness, or gain, of a document based on its position, or rank, in the result list. It is widely used in information retrieval, specifically in web search and related tasks. For both the metrics the higher the scores, the better the accordance between the relevance of the predicted KPs with respect to the real ones.","refs":[{"start":96,"end":97,"marker":"bibr","target":null}]},{"text":"Results are shown in Table 3. For four out of five datasets and for both measures, our approach achieves better or nearly the same results than the baseline catSeq, clearly showing the strength of our method.","refs":[{"start":27,"end":28,"marker":"table","target":"#tab_4"}]}]},{"title":"Conclusion","paragraphs":[{"text":"In this paper we presented a system for Keyphrase Generation using a GAN architecture with Reinforcement Learning. Thanks to the characteristics of our approach, we have been able to train the system in a data-efficient way using only a small fraction of the available data. We tested it on five baseline datasets, achieving results that are competitive with some state-of-the-art generative models. To the best of our knowledge, this is the first attempt to train such a complex architecture for the demanding task of Keyphrase Generation in an scenario in which only a small amount of data is available.","refs":[]}]}],"tables":{"tab_1":{"heading":"Table 1 .","description":"Statistics on test samples for the five datasets.Optimization of generator G is performed with Adam[14]. G 0 is trained with MLE loss and a batch size of 12; following G i are trained with RL optimization and batch size of 32. Optimization of the discriminator D is performed with AdamW optimizer[17]. It is trained with MSE loss and a batch size of 3. For the Discriminator, we refer to the BERT implementation provided by huggingface[27] 1 . The model is a bert-base-uncased with 12 layers, 12 attention heads, and hidden size of 768. Input sequences are trimmed to 384 tokens. The model is fine-tuned for Sequence Classification with one label (regression). Training and testing run on a PC with a Titan RTX GPU, 24GiB.","rows":[["","KP20k","Inspec","Krapivin","NUS","Semeval2010"],["","#","%","#","%","#","%","#","%","#","%"],["Present KPs 66,267 62.91 3,602 73.59 1,297 55.57 1,191 52.26 612","42.41"],["Absent KPs 39,076 37.09 1,293 26.41 1,037 44.43 1,088 47.74 831","57.59"],["Total KPs","105,343 100.00 4,895 100.00 2,334 100.00 2,279 100.00 1,443 100.00"],["Test samples","20,000","","500","","400","","211","","100"],["4.2 Details of Implementation","","","","",""]]},"tab_2":{"heading":"Table 2 .","description":"Results of present keyphrases for five datasets.","rows":[]},"tab_4":{"heading":"Table 3 .","description":"Ranking measures for present KPs. Comparison of our approach (trained on 2,000 samples) and catSeq (trained on the whole dataset), on the five test datasets. nDCG MAP nDCG MAP nDCG MAP nDCG catSeq 0.305 0.585 0.164 0.570 0.303 0.576 0.285 0.740 0.189 0.663 Our approach 0.300 0.560 0.268 0.720 0.308 0.576 0.290 0.736 0.228 0.683","rows":[["Model","KP20k MAP nDCG MAP Inspec","Krapivin","NUS","Semeval2010"]]}},"abstract":{"title":"Abstract","paragraphs":[{"text":"Keyphrase Generation is the task of predicting keyphrases: short text sequences that convey the main semantic meaning of a document. In this paper, we introduce a keyphrase generation approach that makes use of a Generative Adversarial Networks (GANs) architecture. In our system, the Generator produces a sequence of keyphrases for an input document. The Discriminator, in turn, tries to distinguish between machine generated and human curated keyphrases. We propose a novel Discriminator architecture based on a BERT pretrained model fine-tuned for Sequence Classification. We train our proposed architecture using only a small subset of the standard available training dataset, amounting to less than 1% of the total, achieving a great level of data efficiency. The resulting model is evaluated on five public datasets, obtaining competitive and promising results with respect to four state-of-the-art generative models.","refs":[]}]}}