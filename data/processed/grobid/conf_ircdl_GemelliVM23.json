{"bibliography":{"title":"CTE: A Dataset for Contextualized Table Extraction","authors":[{"person_name":{"surname":"Gemelli","first_name":"Andrea"},"affiliations":[{"department":"Dipartimento di Ingegneria dell'Informazione (DINFO","institution":"Università degli studi di Firenze","laboratory":null}],"email":"andrea.gemelli@unifi.it"},{"person_name":{"surname":"Vivoli","first_name":"Emanuele"},"affiliations":[{"department":"Dipartimento di Ingegneria dell'Informazione (DINFO","institution":"Università degli studi di Firenze","laboratory":null}],"email":"emanuele.vivoli@unifi.it"},{"person_name":{"surname":"Marinai","first_name":"Simone"},"affiliations":[{"department":"Dipartimento di Ingegneria dell'Informazione (DINFO","institution":"Università degli studi di Firenze","laboratory":null}],"email":"simone.marinai@unifi.it"}],"date":null,"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"keywords":["Scientific paper analysis","Dataset","Benchmark","Document layout analysis","Table extraction"],"citations":{"b0":{"title":"Learning algorithms for document layout analysis","authors":[{"person_name":{"surname":"Marinai","first_name":"S"},"affiliations":[],"email":null}],"date":{"year":"2013","month":null,"day":null},"ids":{"DOI":"10.1016/B978-0-444-53859-8.00016-3","arXiv":null},"target":"https://doi.org/10.1016/B978-0-444-53859-8.00016-3","publisher":"Elsevier","journal":null,"series":"Handbook of Statistics","scope":{"volume":31,"pages":{"from_page":400,"to_page":419}}},"b1":{"title":"Current status and performance analysis of table recognition in document images with deep neural networks","authors":[{"person_name":{"surname":"Hashmi","first_name":"K"},"affiliations":[],"email":null},{"person_name":{"surname":"Liwicki","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Stricker","first_name":"D"},"affiliations":[],"email":null},{"person_name":{"surname":"Afzal","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Afzal","first_name":"M"},"affiliations":[],"email":null}],"date":{"year":"2021","month":null,"day":null},"ids":{"DOI":"10.1109/ACCESS.2021.3087865","arXiv":null},"target":"https://doi.org/10.1109/ACCESS.2021.3087865","publisher":null,"journal":"IEEE Access","series":null,"scope":{"volume":9,"pages":{"from_page":87663,"to_page":87685}}},"b2":{"title":"Axcell: Automatic extraction of results from machine learning papers","authors":[{"person_name":{"surname":"Kardas","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Czapla","first_name":"P"},"affiliations":[],"email":null},{"person_name":{"surname":"Stenetorp","first_name":"P"},"affiliations":[],"email":null},{"person_name":{"surname":"Ruder","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Riedel","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Taylor","first_name":"R"},"affiliations":[],"email":null},{"person_name":{"surname":"Stojnic","first_name":"R"},"affiliations":[],"email":null}],"date":{"year":"2020","month":null,"day":null},"ids":{"DOI":"10.18653/v1/2020.emnlp-main.692","arXiv":null},"target":"https://doi.org/10.18653/v1/2020.emnlp-main.692.doi:10.18653/v1/2020.emnlp-main.692","publisher":"Association for Computational Linguistics","journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":8580,"to_page":8594}}},"b3":{"title":"Publaynet: Largest dataset ever for document layout analysis","authors":[{"person_name":{"surname":"Zhong","first_name":"X"},"affiliations":[],"email":null},{"person_name":{"surname":"Tang","first_name":"J"},"affiliations":[],"email":null},{"person_name":{"surname":"Jimeno-Yepes","first_name":"A"},"affiliations":[],"email":null}],"date":{"year":"2019","month":null,"day":null},"ids":{"DOI":"10.1109/ICDAR.2019.00166","arXiv":null},"target":"https://doi.org/10.1109/ICDAR.2019.00166.doi:10.1109/ICDAR.2019.00166","publisher":"IEEE","journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":1015,"to_page":1022}}},"b4":{"title":"PubTables-1M: Towards a universal dataset and metrics for training and evaluating table extraction models","authors":[{"person_name":{"surname":"Smock","first_name":"B"},"affiliations":[],"email":null},{"person_name":{"surname":"Pesala","first_name":"R"},"affiliations":[],"email":null},{"person_name":{"surname":"Abraham","first_name":"R"},"affiliations":[],"email":null}],"date":{"year":"2021","month":null,"day":null},"ids":null,"target":"2110.00061","publisher":null,"journal":null,"series":null,"scope":null},"b5":{"title":"Figureseer: Parsing result-figures in research papers","authors":[{"person_name":{"surname":"Siegel","first_name":"N"},"affiliations":[],"email":null},{"person_name":{"surname":"Horvitz","first_name":"Z"},"affiliations":[],"email":null},{"person_name":{"surname":"Levin","first_name":"R"},"affiliations":[],"email":null},{"person_name":{"surname":"Divvala","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Farhadi","first_name":"A"},"affiliations":[],"email":null}],"date":{"year":"2016","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b6":{"title":"Doclaynet: A large humanannotated dataset for document-layout analysis","authors":[{"person_name":{"surname":"Pfitzmann","first_name":"B"},"affiliations":[],"email":null},{"person_name":{"surname":"Auer","first_name":"C"},"affiliations":[],"email":null},{"person_name":{"surname":"Dolfi","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Nassar","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Staar","first_name":"P"},"affiliations":[],"email":null}],"date":{"year":"2022","month":null,"day":null},"ids":{"DOI":"10.1145/3534678.353904","arXiv":null},"target":"https://arxiv.org/abs/2206.01062.doi:10.1145/3534678.353904","publisher":null,"journal":null,"series":null,"scope":null},"b7":{"title":"Docbank: A benchmark dataset for document layout analysis","authors":[{"person_name":{"surname":"Li","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Xu","first_name":"Y"},"affiliations":[],"email":null},{"person_name":{"surname":"Cui","first_name":"L"},"affiliations":[],"email":null},{"person_name":{"surname":"Huang","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Wei","first_name":"F"},"affiliations":[],"email":null},{"person_name":{"surname":"Li","first_name":"Z"},"affiliations":[],"email":null},{"person_name":{"surname":"Zhou","first_name":"M"},"affiliations":[],"email":null}],"date":{"year":"2020","month":null,"day":null},"ids":{"DOI":"10.18653/v1/2020.coling-main.82","arXiv":null},"target":"https://doi.org/10.18653/v1/2020.coling-main.82.doi:10.18653/v1/2020.coling-main.82","publisher":"International Committee on Computational Linguistics","journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":949,"to_page":960}}},"b8":{"title":"Faster R-CNN: towards real-time object detection with region proposal networks","authors":[{"person_name":{"surname":"Ren","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"He","first_name":"K"},"affiliations":[],"email":null},{"person_name":{"surname":"Girshick","first_name":"R"},"affiliations":[],"email":null},{"person_name":{"surname":"Sun","first_name":"J"},"affiliations":[],"email":null}],"date":{"year":"2015","month":null,"day":null},"ids":null,"target":"https://proceedings.neurips.cc/paper/2015/hash/14bfa6bb14875e45bba028a21ed38046-Abstract.html","publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":91,"to_page":99}}},"b9":{"title":"IEEE International Conference on Computer Vision, ICCV 2017","authors":[{"person_name":{"surname":"He","first_name":"K"},"affiliations":[],"email":null},{"person_name":{"surname":"Gkioxari","first_name":"G"},"affiliations":[],"email":null},{"person_name":{"surname":"Dollár","first_name":"P"},"affiliations":[],"email":null},{"person_name":{"surname":"Girshick","first_name":"R"},"affiliations":[],"email":null},{"person_name":{"surname":"Mask","first_name":"R-Cnn"},"affiliations":[],"email":null}],"date":{"year":"2017","month":null,"day":null},"ids":{"DOI":"10.1109/ICCV.2017.322","arXiv":null},"target":"https://doi.org/10.1109/ICCV.2017.322.doi:10.1109/ICCV.2017.322","publisher":"IEEE Computer Society","journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":2980,"to_page":2988}}},"b10":{"title":"Layoutlm: Pre-training of text and layout for document image understanding","authors":[{"person_name":{"surname":"Xu","first_name":"Y"},"affiliations":[],"email":null},{"person_name":{"surname":"Li","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Cui","first_name":"L"},"affiliations":[],"email":null},{"person_name":{"surname":"Huang","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Wei","first_name":"F"},"affiliations":[],"email":null},{"person_name":{"surname":"Zhou","first_name":"M"},"affiliations":[],"email":null}],"date":{"year":"2019","month":null,"day":null},"ids":null,"target":"1912.13318","publisher":null,"journal":null,"series":null,"scope":null},"b11":{"title":"Complicated table structure recognition","authors":[{"person_name":{"surname":"Chi","first_name":"Z"},"affiliations":[],"email":null},{"person_name":{"surname":"Huang","first_name":"H"},"affiliations":[],"email":null},{"person_name":{"surname":"Xu","first_name":"H"},"affiliations":[],"email":null},{"person_name":{"surname":"Yu","first_name":"H"},"affiliations":[],"email":null},{"person_name":{"surname":"Yin","first_name":"W"},"affiliations":[],"email":null},{"person_name":{"surname":"Mao","first_name":"X"},"affiliations":[],"email":null}],"date":{"year":"2019","month":null,"day":null},"ids":null,"target":"1908.04729","publisher":null,"journal":null,"series":null,"scope":null},"b12":{"title":"Doc2graph: A task agnostic document understanding framework based on graph neural networks","authors":[{"person_name":{"surname":"Gemelli","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Biswas","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Civitelli","first_name":"E"},"affiliations":[],"email":null},{"person_name":{"surname":"Lladós","first_name":"J"},"affiliations":[],"email":null},{"person_name":{"surname":"Marinai","first_name":"S"},"affiliations":[],"email":null}],"date":{"year":"2023","month":null,"day":null},"ids":null,"target":null,"publisher":"Springer Nature","journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":329,"to_page":344}}},"b13":{"title":"Graph neural networks and representation embedding for table extraction in PDF documents","authors":[{"person_name":{"surname":"Gemelli","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Vivoli","first_name":"E"},"affiliations":[],"email":null},{"person_name":{"surname":"Marinai","first_name":"S"},"affiliations":[],"email":null}],"date":{"year":"2022","month":null,"day":null},"ids":{"DOI":"10.1109/ICPR56361.2022.9956590","arXiv":null},"target":"https://doi.org/10.1109/ICPR56361.2022.9956590.doi:10.1109/ICPR56361.2022.9956590","publisher":"IEEE","journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":1719,"to_page":1726}}},"b14":{"title":"Pymupdf: Python bindings for mupdf's rendering library","authors":[{"person_name":{"surname":"Pymupdf","first_name":"J"},"affiliations":[],"email":null},{"person_name":{"surname":"Mckie","first_name":null},"affiliations":[],"email":null}],"date":{"year":"2012","month":null,"day":null},"ids":null,"target":"https://github.com/pymupdf/PyMuPDF","publisher":null,"journal":null,"series":null,"scope":null},"b15":{"title":"Automatic generation of scientific papers for data augmentation in document layout analysis","authors":[{"person_name":{"surname":"Pisaneschi","first_name":"L"},"affiliations":[],"email":null},{"person_name":{"surname":"Gemelli","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Marinai","first_name":"S"},"affiliations":[],"email":null}],"date":{"year":"2023","month":null,"day":null},"ids":{"DOI":"10.1016/j.patrec.2023.01.018","arXiv":null},"target":"https://doi.org/10.1016/j.patrec.2023.01.018","publisher":null,"journal":"Pattern Recognition Letters","series":null,"scope":{"volume":167,"pages":{"from_page":38,"to_page":44}}},"b16":{"title":"","authors":[{"person_name":{"surname":"Grobid","first_name":"Grobid"},"affiliations":[],"email":null}],"date":{"year":"2008","month":null,"day":null},"ids":null,"target":"https://github.com/kermitt2/grobid","publisher":null,"journal":null,"series":null,"scope":null},"b17":{"title":"Post-ocr paragraph recognition by graph convolutional networks","authors":[{"person_name":{"surname":"Wang","first_name":"R"},"affiliations":[],"email":null},{"person_name":{"surname":"Fujii","first_name":"Y"},"affiliations":[],"email":null},{"person_name":{"surname":"Popat","first_name":"A"},"affiliations":[],"email":null}],"date":{"year":"2022","month":null,"day":null},"ids":{"DOI":"10.1109/WACV51458.2022.00259","arXiv":null},"target":"https://doi.org/10.1109/WACV51458.2022.00259.doi:10.1109/WACV51458.2022.00259","publisher":"IEEE","journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":2533,"to_page":2542}}}},"sections":[{"title":"Introduction","paragraphs":[{"text":"Nowadays, large collections of documents require a huge amount of human work to annotate documents and extract important information. In the last thirty years, the community of Document Analysis and Recognition (DAR) tried to overcome this challenge, exploiting suitable algorithms and artificial intelligence techniques to automatize the analysis of documents and reduce its costs. Among others, Document Classification (DC), Layout Analysis (DLA), and Table Understanding (TU) more broadly attracted the interest of researchers and companies. DC is the first step of many DAR pipelines, since different kinds of documents require different strategies: given a document, either scanned or digital-born, the aim is to classify it into a specific category, e.g. invoice or magazine. DLA [1] aims at recognizing homogeneous regions within the document, grouping smaller components close to each other such as regions of text, and, if required, assigning it a category (e.g. a title or an image caption). Finally, TU [2] is an umbrella term for table detection and recognition: tables summarize important information within documents and their detection along with the recognition of their structure is crucial to automatically query collections of documents.","refs":[{"start":786,"end":789,"marker":"bibr","target":"#b0"},{"start":1014,"end":1017,"marker":"bibr","target":"#b1"}]},{"text":"During the past years, the interest in the detection and recognition of tables raised significantly, leading to the automation of important processes such as information extraction. In particular, for scientific literature, it is crucial to extract tabular data, e.g. to make the research comparable and help scholars to reconstruct the SOTA of the different fields of study [3]. Moreover, collections of scientific papers such as arXiv and PubMed opened to the possibility of accessing a large number of documents along with their structural information represented in standard formats such as L A T E X and XML. That is why scientific literature parsing and scientific table analysis rapidly became one of the most prominent areas of research in DAR: large datasets have been released [4,5], allowing the community to develop deep learning models. Unfortunately, as we will describe in the next sections, these datasets come with partial information that forces the experimentation of layout analysis and table extraction separately. From this identified lack, we define Contextualized Table Extraction, a broad task that comes along with novel annotations for a collection of 75k scientific pages containing more than 35k tables, encouraging the development of new systems capable of tackling a multitude of tasks at once.","refs":[{"start":375,"end":378,"marker":"bibr","target":"#b2"},{"start":787,"end":790,"marker":"bibr","target":"#b3"},{"start":790,"end":792,"marker":"bibr","target":"#b4"}]},{"text":"In this paper, we introduce a new task called Contextualized Table Extraction that is a framework, which involves detecting tables, recognizing their structure, and performing functional analysis in an end-to-end manner. CTE is formulated as a token and link classification task, which allows for multiple tasks to be addressed simultaneously overcoming common limitations such as being performed separately or lacking a comprehensive dataset. CTE is built on top of well known tasks in DAR. CTE is designed to be suitable for methods employing Graph Neural Networks, which are widely used in applications where the structure and layout in documents matter. We provide a new set of labels structured in a way that allows us to merge information of selected scientific publications from other well known benchmark datasets. In this way we obtain a comprehensive dataset for the task of CTE. We believe that the combination of methods applied to process the labeled documents and produce the merged information collected is a novel contribution to the field of document analysis as well.","refs":[]}]},{"title":"Related Work","paragraphs":[{"text":"Despite the advances in the field, several challenges strongly limited the generalization of methods developed until a few years ago. In particular, we can mention: (i) data quality (e.g. scanned documents or images captured in-the-wild); (ii) contents, due to different languages and/or scripts; (iii) document layouts (which differentiate in, e.g. magazines, scientific papers, and invoices). To address these challenges a large number of data need to be collected in order to fully exploit the power of Deep Learning models that achieve the state-of-the-art for the aforementioned tasks. Unfortunately, creating such datasets is nothing but trivial since accurate annotations come at a high cost in terms of time and human effort [6,7]. On the other hand, automatic annotation techniques are not always applicable since they require a large number of documents shared together with their source files in standard formats such as L A T E X, XML, or HTML [8,4]. Additionally, these techniques usually generate weakly labeled collections and are more error-prone than manually annotated ones. Since online archives of scientific papers are freely and publicly available along with the corresponding source information (e.g. arXiv and PubMed) several datasets have been proposed so far in the field of scientific literature parsing. Among others, we summarize in Table 1 some of the most important datasets proposed for layout analysis and table extraction. PubLayNet and DocBank have been widely used to train object detectors [9,10] and transformers [11] for DLA. Overall, these datasets contain around half a million pages labeled into five and twelve different classes, respectively. PubLayNet has been constructed merging the information extracted from PDFMiner (bounding box regions) and the XML files shared by the publishers (containing the region labels). DocBank is built gathering the L A T E Xsource files and assigning labels taking into account the section tags. For the Table Extraction task, a recent dataset has been released (PubTables-1M) which counts nearly one million tables, labeled to perform not only TD and TSR but also Table Functional Analysis (TFA) that provides additional information on table cells like table headers. Even if it is smaller, SciTSR [12] introduced a collection of 15k tables generated from L A T E X to perform TSR, mainly using a Graph Neural Network (GNN). Despite this contribution, GNNs also have the advantage of being lightweight compared to transformer-based architectures while still retaining good performance, as shown in the framework Doc2Graph [13] for document analysis.","refs":[{"start":733,"end":736,"marker":"bibr","target":"#b5"},{"start":736,"end":738,"marker":"bibr","target":"#b6"},{"start":956,"end":959,"marker":"bibr","target":"#b7"},{"start":959,"end":961,"marker":"bibr","target":"#b3"},{"start":1368,"end":1369,"marker":"table","target":"#tab_0"},{"start":1527,"end":1530,"marker":"bibr","target":"#b8"},{"start":1530,"end":1533,"marker":"bibr","target":"#b9"},{"start":1551,"end":1555,"marker":"bibr","target":"#b10"},{"start":2279,"end":2283,"marker":"bibr","target":"#b11"},{"start":2603,"end":2607,"marker":"bibr","target":"#b12"}]},{"text":"As it is possible to notice in Table 1, all these datasets lack a comprehensive and broader set of annotations, forcing the community to develop multiple systems that, in application scenarios, would lead to heavy and large pipelines.","refs":[{"start":37,"end":38,"marker":"table","target":"#tab_0"}]}]},{"title":"Contributions","paragraphs":[{"text":"Our ongoing work brings several novelties, that are discussed throughout the paper and are summarized as follows:","refs":[]}]},{"title":"• We define the task of Contextualized Table Extraction, an extended version of table","paragraphs":[{"text":"extraction as defined in [5] that adds layout information and encourages the development of end-to-end systems that can tackle multiple tasks at once;","refs":[{"start":25,"end":28,"marker":"bibr","target":"#b4"}]},{"text":"• Novel annotations are created by merging subset of [4,5] that can be found in our repo 1 .","refs":[{"start":53,"end":56,"marker":"bibr","target":"#b3"},{"start":56,"end":58,"marker":"bibr","target":"#b4"},{"start":89,"end":90,"marker":null,"target":"#foot_0"}]},{"text":"Our collection comprehends 75k scientific pages and more than 35k tables. Tokens at the basis of annotations correspond to words extracted from PDFs using PyMuPDF and labeled according to the region they belong to; table structure information is encoded as links between tokens; • The dataset encourages the use and development of graph methods on documents, providing to the community a new set of labeled data to experiment with GNN-based techniques. The annotations do not require any further processing (either in labels or data themselves) to construct a graph over the scientific pages.","refs":[]},{"text":"The paper is organized as follows: in Section 2 we describe in detail how the dataset has been created and how the annotations are presented, along with some limitations we aim to address in the near future. Section 3 formalizes the CTE task by means of token and link classification. Finally, in Section 4 and 5 we discuss future work and draw conclusions.","refs":[]}]},{"title":"Dataset Description","paragraphs":[{"text":"Contextualized Table Extraction (CTE), as we describe deeply in Section 3, involves not only detecting tables, recognizing their layout and functional structure, but also takes into consideration their surrounding information. We formalize CTE to be accomplished through token and link classification, allowing multiple tasks to be tackled at once. The F1 score for CTE is defined as the average of F1 scores for token and link classification.","refs":[]},{"text":"Although it is easy to freely access large collections of scientific papers (i.e. from arXiv or PubMed Central) it is difficult to find documents labeled with complete information. Most benchmark datasets support either DLA or TU. However, as our aim is encouraging the development of systems capable of tackling more tasks at once, a new dataset is needed. The proposed dataset for CTE is obtained by merging data and annotations given by PubLayNet and PubTables-1M datasets, both based on PubMed Central publications. As depicted in the next sections, firstly we identify the pages of scientific papers annotated in both datasets, then we merge the information and add two novel classes (captions and page information) and finally use PyMuPDF to extract text and position of tokens. We used a preliminary small version of this collection in [14], applying a GNN to tackle CTE. After the release of PubLayNet test set we updated the version of CTE dataset, now containing more annotated data.","refs":[{"start":843,"end":847,"marker":"bibr","target":"#b13"}]}]},{"title":"Subset of PubLayNet and PubTables-1M","paragraphs":[{"text":"PubLayNet is a collection of 358, 353 PDF pages with five types of regions annotated (title, text, list, table, image) [4]. PubTables-1M [5] is a collection of 947, 642 fully annotated tables, including information for table detection, recognition, and functional analysis (such as identifying column headers, projected rows, and table cells). The datasets are built to address different tasks, as summarized in Table 1.","refs":[{"start":119,"end":122,"marker":"bibr","target":"#b3"},{"start":137,"end":140,"marker":"bibr","target":"#b4"},{"start":418,"end":419,"marker":"table","target":"#tab_0"}]},{"text":"To merge the datasets, we first identify the papers belonging to both collections. From this subset, we keep pages with tables fully annotated in PubTables-1M and pages without tables: this filters out even more pages, since we found some PubTables-1M annotations to have only one annotated table in pages containing two or more tables. Following this step, we obtain approximately 75k pages. The resulting merged dataset contains objects labeled into 13 different classes, having in addition to the regions annotated in PubLayNet the table annotations described in PubTables-1M (row, column, table header, projected header, table cell, and grid cell). Moreover, we added two classes: caption and other. Captions are heuristically found taking into account the proximity with images and tables, while the other class contains all the remaining not-labeled text regions (e.g. page headers and page numbers).","refs":[]},{"text":"The GitHub repository of our dataset is at its second version, after adding the test-set released by PubLayNet2 . We followed PubLayNet for the train/val/test splits.","refs":[{"start":110,"end":111,"marker":null,"target":"#foot_1"}]}]},{"title":"Annotation procedure","paragraphs":[{"text":"Once a complete annotated list of pages is selected from the two datasets, we leverage an external tool to extract page tokens. After comparing several tools, we opted for PyMuPDF [15] which is a Python open-source library backed by a large community and constantly maintained. Each element, visible or not visible, present in the PDF page is extracted and annotated based on the annotation bounding-box it appears in, as depicted in Figure 1: tokens are labeled according to their enclosing labeled region (upper part); links, instead, are presented as groups of tokens for visualization purposes (bottom part), but encoded as couples as described in details in the next Section and in Table 2. By doing so, the resulting page is composed by extracting page tokens along with their position (bounding boxes coordinates) and their textual content (mostly single words). This process heavily depends on original versions of the PDF files: even if the document name is the same along the two datasets annotations (PubLayNet and PubTables-1M) the PDF version of PubLayNet documents could differ. This is due to the two years gap between the datasets release date. To obtain reliable information, in our approach we discard all the pages (and tables) in which the content of the two sources does not correspond anymore.","refs":[{"start":180,"end":184,"marker":"bibr","target":"#b14"},{"start":441,"end":442,"marker":"figure","target":"#fig_0"},{"start":693,"end":694,"marker":"table","target":null}]}]},{"title":"Dataset structure and format","paragraphs":[{"text":"After the merging procedure, we end up with three JSON files (subset of the original PubLayNet one) splitting the data into train, val, and test. Each one contains information regarding tokens extracted by PyMuPDF, their links and the regions that group them (larger objects). Tokens have these information: token id, bounding box coordinates, text, class id, and object id (larger region to which it belongs). Links between tokens (belonging to the same row, column or grid cell) have information such as link id, class id, and token id (list of tokens linked together). Finally, objects contain information such as object id, bounding box coordinates and class id. A representation of the aforementioned annotation format is represented in Tables 2. ","refs":[]}]},{"title":"Acronyms for table annotations are: THEAD (table headers), TSPAN (table sub-headers spanning along different columns), TGRID (table cells), TCOLS and TROWS (respectively columns and rows of the tables).","paragraphs":[]},{"title":"Limitations of the Dataset","paragraphs":[{"text":"We are aware that the proposed dataset, even if it is proposing a new benchmark to tackle CTE, has room for improvement. As such, in the following we list the limitations of the dataset:","refs":[]},{"text":"1. There is a small amount of data and tables compared to other datasets. Considering that adding more annotated data would be nothing but trivial, we believe this point could be addressed in two ways: i) as a starting pool of data to train generative models and getting new samples automatically labeled (e.g. using techniques similar to [16]); ii) using the CTE collection as a challenging benchmark to compare lightweight models, such as GNNs, along with state-of-the-art transformers (notably anger of huge amount of data).","refs":[{"start":339,"end":343,"marker":"bibr","target":"#b15"}]}]},{"title":"Table 2","paragraphs":[{"text":"Annotation Format: each line contains different information in case of Objects, Tokens, or Links. 2. The heuristics used for the the classes caption and other could affect the generalization of trained models, highly dependent on the paper format used in PubMed Central. On the other hand, we are enriching information about tables by recognizing captions, that contain valuable table descriptions and that otherwise would be discarded. 3. We still lack additional information such as author, keywords, and equations. We are going to add these additional labels in the near future, considering Grobid [17] in the annotation procedure, since it is a machine learning library for extracting technical information from scientific publications, from PDF to XML/TEI structured documents. 4. The first attempts to define a baselines are reported in [14], in which the task of TE and DLA are treated end-to-end. This paper aims at sharing the CTE dataset in a way that the scientific community can further propose baselines on this work.","refs":[{"start":601,"end":605,"marker":"bibr","target":"#b16"},{"start":843,"end":847,"marker":"bibr","target":"#b13"}]}]},{"title":"Object annotations","paragraphs":[]},{"title":"Contextualized Table Extraction","paragraphs":[{"text":"Contextualized Table Extraction (CTE) is the broader task of extracting tables (meaning their detection) recognizing their structure and performing functional analysis, along with other page layout information. To do so, CTE is formulated as a token and link classification tasks, similarly to [8], since fine-grained objects like tokens permit to tackle multiple tasks at once. For instance, recognizing the table headers and grid cells allows us to detect the tables (grouping tokens together through links) and add functional information. In addition, through token and link classification the need for more components would be reduced since a method capable of successfully solving CTE would require to train only one model, extracting more information at once. Given Precision and Recall for token and link classification, namely Token Precision (TP), Token Recall (TR), Link Precision (LP), and Link Recall (LR). We can define the 𝐹 1 𝐶𝑇 𝐸 metric as follows:","refs":[{"start":294,"end":297,"marker":"bibr","target":"#b7"}]}]},{"title":"Token classification","paragraphs":[{"text":"The first step required to tackle CTE is the classification of tokens, extracted from PDF pages using PyMuPDF. Tokens contain textual and positional information, along with class information inherited from the larger region they belong to (details in Table 2, tokens annotations). This subtask exposes these properties:","refs":[{"start":257,"end":258,"marker":"table","target":null}]},{"text":"1. Through token classification it is possible to achieve DLA, TD, and TFA at once. 2. If tackled along with link classification to achieve CTE the 𝐹 1 𝐶𝑇 𝐸 metric (Eq. 1) should be used. Instead, if tackled alone the metric proposed in [8] can be used as well.","refs":[{"start":237,"end":240,"marker":"bibr","target":"#b7"}]}]},{"title":"Link Classification","paragraphs":[{"text":"In order to group together tokens belonging to tables into columns, rows, or grid cells, additional information on links among pairs of tokens is added. This subtask exposes these properties:","refs":[]},{"text":"1. Through link classification it is possible to perform TSR. 2. Similarly to token classification, F1 is preferred to evaluate link classification if tackled alone. 3. Links connecting non-tables items should be considered as an additional class 'none'.","refs":[]}]},{"title":"Object Recognition","paragraphs":[{"text":"Even if not required to do CTE, the annotations include area information of different regions in the paper (as common for object detection). Grouping together tokens belonging to the same class via edges can be exploited to find such areas, e.g. extracting sub-graphs from the whole document. A recent paper [18] exploited GNN to perform post-OCR paragraph recognition by grouping together similar items in the pages.","refs":[{"start":308,"end":312,"marker":"bibr","target":"#b17"}]}]},{"title":"Limitation of the Task","paragraphs":[{"text":"While we acknowledge that CTE has some limitations, we believe that it represents a significant step towards a more comprehensive solution for table extraction in documents. In our previous work [14], we investigated different ways to achieve CTE through ablation studies, so as to analyze the impact of different components on the system's performances. In this paper, we define a metric, (𝐹 1 𝐶𝑇 𝐸 ), for the updated dataset regarding CTE. As the combination of two metrics, namely Token F1 and Link F1, they can be used to evaluate the performance of the system.","refs":[{"start":195,"end":199,"marker":"bibr","target":"#b13"}]}]},{"title":"Future work","paragraphs":[{"text":"In addition to providing a new dataset for contextualized table extraction, the CTE task can also serve as a basis for future research. One area of research is to investigate the effectiveness of using graph neural networks (GNNs) versus transformer architectures for the CTE task. The models might be pre-trained and fine-tuned on all the original data from [11] and [4]. Comparing a lightweight network, GNN-based, with a heavy network, such as transformer-based, can help determine which approach is best suited for the CTE task. Another potential avenue for future work is to investigate the use of the CTE dataset for information extraction tasks, specifically in the context of scientific papers. Many papers include tables with important information that can be challenging to extract automatically, and incorporating external knowledge bases could further improve performance. With the CTE dataset, it would be possible to explore how to effectively combine table structure information with external knowledge to answer questions based on scientific papers. Other open research questions that could be addressed using the CTE dataset include investigating cross-lingual performance, transfer learning, and developing techniques to handle different types of tables (e.g., nested tables, tables with merged cells).","refs":[{"start":359,"end":363,"marker":"bibr","target":"#b10"},{"start":368,"end":371,"marker":"bibr","target":"#b3"}]}]},{"title":"Conclusions","paragraphs":[{"text":"In this work we presented a new dataset to tackle the task of Contextualized Table Extraction. The dataset is obtained by merging two well-known benchmark datasets (PubTables-1M and PubLayNet). Usually, table extraction pipelines involve several components to perform different tasks on tables, without considering other important information present in the document such as captions. Based on these limitations, the proposed collection of data aims at developing models capable of tackling more tasks at once, resulting in CTE. Moreover, the annotations format encourages the development of systems based on GNN, that lack of a common benchmark within the DAR community for tasks different from TSR. We are looking to extend the dataset by adding more information such as authors, keywords, and equations.   ","refs":[{"start":83,"end":93,"marker":"table","target":null}]}]}],"tables":{"tab_0":{"heading":"Table 1","description":"Comparison of CTE with related datasets: ♣ denotes the datasets used to generate the new annotations. A dataset is 𝑆4𝐺 (suitable for graphs) if a graph can be constructed directly with no further preprocessing. DLA (Document Layout Analysis), TD (TableDetection), TSR (TableStructureRecognition), and TFA (Table Functional Analysis) show which tasks models can be trained for.","rows":[["Dataset","#pages #tables #classes DLA TD TSR TFA S4G"],["PubLayNet (♣)","358k","107k","5","✓","✓","✗","✗","✗"],["PubTables-1M (♣)","574k","948k","7","✗","✓","✓","✓","✗"],["DocBank","500k","417k*","12","✓","✓","✓*","✗","✓**"],["SciTSR","0","15k","-","✗","✗","✓","✗","✓"],["CTE","75k","35k","13","✓","✓","✓","✓","✓"],["*DocBank is an extension of TableBank, from which we gathered these information","",""],["**If tokens used as graph nodes, no information on edges","","","","",""]]}},"abstract":{"title":"Abstract","paragraphs":[{"text":"Relevant information in documents is often summarized in tables, helping the reader to identify useful facts. Most benchmark datasets support either document layout analysis or table understanding, but lack in providing data to apply both tasks in a unified way. We define the task of Contextualized Table Extraction (CTE), which aims to extract and define the structure of tables considering the textual context of the document. The dataset comprises 75k fully annotated pages of scientific papers, including more than 35k tables. Data are gathered from PubMed Central, merging the information provided by annotations in the PubTables-1M and PubLayNet datasets. The dataset can support CTE and adds new classes to the original ones. The generated annotations can be used to develop end-to-end pipelines for various tasks, including document layout analysis, table detection, structure recognition, and functional analysis. We formally define CTE and evaluation metrics, showing which subtasks can be tackled, describing advantages, limitations, and future works of this collection of data. Annotations and code will be accessible at https://github.com/AILab-UniFI/cte-dataset.","refs":[]}]}}