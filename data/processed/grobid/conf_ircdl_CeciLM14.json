{"bibliography":{"title":"Ranking Sentences for Keyphrase Extraction: A Relational Data Mining Approach","authors":[{"person_name":{"surname":"Ceci","first_name":"Michelangelo"},"affiliations":[{"department":"Dipartimento di Informatica","institution":"Università degli Studi di Bari \"Aldo Moro\"","laboratory":null}],"email":"michelangelo.ceci@uniba.it"},{"person_name":{"surname":"Loglisci","first_name":"Corrado"},"affiliations":[{"department":"Dipartimento di Informatica","institution":"Università degli Studi di Bari \"Aldo Moro\"","laboratory":null}],"email":null},{"person_name":{"surname":"Macchia","first_name":"Lucrezia"},"affiliations":[{"department":"Dipartimento di Informatica","institution":"Università degli Studi di Bari \"Aldo Moro\"","laboratory":null}],"email":null}],"date":null,"ids":{"DOI":"10.1016/j.procs.2014.10.011","arXiv":null},"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"keywords":["Ranking","Document summarization","Relational data mining"],"citations":{"b0":{"title":"Automatic extraction of document keyphrases for use in digital libraries: Evaluation and applications","authors":[{"person_name":{"surname":"Jones","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Paynter","first_name":"G"},"affiliations":[],"email":null}],"date":{"year":"2002","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"JASIST","series":null,"scope":{"volume":53,"pages":{"from_page":653,"to_page":677}}},"b1":{"title":"The oxford handbook of computational linguistics edited by ruslan mitkov","authors":[{"person_name":{"surname":"Jackson","first_name":"P"},"affiliations":[],"email":null}],"date":{"year":"2004","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"Computational Linguistics","series":null,"scope":{"volume":30,"pages":{"from_page":103,"to_page":106}}},"b2":{"title":"Automatic summarising: The state of the art","authors":[{"person_name":{"surname":"Jones","first_name":"Spärck"},"affiliations":[],"email":null}],"date":{"year":"2007","month":null,"day":null},"ids":{"DOI":"10.1016/j.ipm.2007.03.009","arXiv":null},"target":"http://dx.doi.org/10.1016/j.ipm.2007.03.009","publisher":null,"journal":"Inf Process Manage","series":null,"scope":{"volume":43,"pages":{"from_page":1449,"to_page":1481}}},"b3":{"title":"Constructing literature abstracts by computer: Techniques and prospects","authors":[{"person_name":{"surname":"Paice","first_name":"C"},"affiliations":[],"email":null}],"date":{"year":"1990","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"Inf Process Manage","series":null,"scope":{"volume":26,"pages":{"from_page":171,"to_page":186}}},"b4":{"title":"Relational Data Mining","authors":[{"person_name":{"surname":"Džeroski","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Lavrač","first_name":"N"},"affiliations":[],"email":null}],"date":{"year":"2001","month":null,"day":null},"ids":null,"target":null,"publisher":"Springer-Verlag","journal":null,"series":null,"scope":null},"b5":{"title":"Project d.a.m.a.: Document acquisition, management and archiving","authors":[{"person_name":{"surname":"Ceci","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Loglisci","first_name":"C"},"affiliations":[],"email":null},{"person_name":{"surname":"Ferilli","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Malerba","first_name":"D"},"affiliations":[],"email":null}],"date":{"year":"2011","month":null,"day":null},"ids":null,"target":null,"publisher":"Springer","journal":null,"series":"Communications in Computer and Information Science","scope":{"volume":249,"pages":{"from_page":115,"to_page":118}}},"b6":{"title":"Relational data mining and ILP for document image understanding","authors":[{"person_name":{"surname":"Ceci","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Berardi","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Malerba","first_name":"D"},"affiliations":[],"email":null}],"date":{"year":"2007","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"Applied Artificial Intelligence","series":null,"scope":{"volume":21,"pages":{"from_page":317,"to_page":342}}},"b7":{"title":"A data mining approach to reading order detection","authors":[{"person_name":{"surname":"Ceci","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Berardi","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Porcelli","first_name":"G"},"affiliations":[],"email":null},{"person_name":{"surname":"Malerba","first_name":"D"},"affiliations":[],"email":null}],"date":{"year":"2007","month":null,"day":null},"ids":null,"target":null,"publisher":"IEEE Computer Society","journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":924,"to_page":928}}},"b8":{"title":"Using gene expression programming to construct sentence ranking functions for text summarization","authors":[{"person_name":{"surname":"Zhuli","first_name":"X"},"affiliations":[],"email":null},{"person_name":{"surname":"Li","first_name":"X"},"affiliations":[],"email":null},{"person_name":{"surname":"Barbara","first_name":"D"},"affiliations":[],"email":null},{"person_name":{"surname":"Peter","first_name":"N"},"affiliations":[],"email":null},{"person_name":{"surname":"Weimin","first_name":"X"},"affiliations":[],"email":null},{"person_name":{"surname":"Thomas","first_name":"T"},"affiliations":[],"email":null}],"date":{"year":"2004","month":null,"day":null},"ids":{"DOI":"10.3115/1220355.1220557","arXiv":null},"target":"http://dx.doi.org/10.3115/1220355.1220557","publisher":"Association for Computational Linguistics","journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":1381,"to_page":1381}}},"b9":{"title":"Enhancing single-document summarization by combining ranknet and third-party sources","authors":[{"person_name":{"surname":"Svore","first_name":"K"},"affiliations":[],"email":null},{"person_name":{"surname":"Vanderwende","first_name":"L"},"affiliations":[],"email":null},{"person_name":{"surname":"Burges","first_name":"C"},"affiliations":[],"email":null}],"date":{"year":"2007","month":null,"day":null},"ids":null,"target":null,"publisher":"Association for Computational Linguistics","journal":null,"series":null,"scope":null},"b10":{"title":"Progress in Machine Learning; chap. Inductive generalization: a logical framework","authors":[{"person_name":{"surname":"Helft","first_name":"N"},"affiliations":[],"email":null}],"date":{"year":"1987","month":null,"day":null},"ids":null,"target":null,"publisher":"Sigma Press","journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":149,"to_page":157}}},"b11":{"title":"Emerging pattern based classification in relational data mining","authors":[{"person_name":{"surname":"Ceci","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Appice","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Malerba","first_name":"D"},"affiliations":[],"email":null}],"date":{"year":"2008","month":null,"day":null},"ids":null,"target":null,"publisher":"Springer","journal":null,"series":"Lecture Notes in Computer Science","scope":{"volume":5181,"pages":{"from_page":283,"to_page":296}}},"b12":{"title":"A machine oriented logic based on the resolution principle","authors":[{"person_name":{"surname":"Robinson","first_name":"J"},"affiliations":[],"email":null}],"date":{"year":"1965","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"Journal of the ACM","series":null,"scope":{"volume":12,"pages":{"from_page":23,"to_page":41}}},"b13":{"title":"Discovering emerging patterns in spatial databases: A multi-relational approach","authors":[{"person_name":{"surname":"Ceci","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Appice","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Malerba","first_name":"D"},"affiliations":[],"email":null}],"date":{"year":"2007","month":null,"day":null},"ids":null,"target":null,"publisher":"Springer","journal":null,"series":"Lecture Notes in Computer Science","scope":{"volume":4702,"pages":{"from_page":390,"to_page":397}}},"b14":{"title":"Levelwise search and borders of theories in knowledge discovery","authors":[{"person_name":{"surname":"Mannila","first_name":"H"},"affiliations":[],"email":null},{"person_name":{"surname":"Toivonen","first_name":"H"},"affiliations":[],"email":null}],"date":{"year":"1997","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"Data Mining and Knowledge Discovery","series":null,"scope":{"volume":1,"pages":{"from_page":241,"to_page":258}}},"b15":{"title":"A note on inductive generalization","authors":[{"person_name":{"surname":"Plotkin","first_name":"G"},"affiliations":[],"email":null}],"date":{"year":"1970","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"Machine Intelligence","series":null,"scope":{"volume":5,"pages":{"from_page":153,"to_page":163}}},"b16":{"title":"Mining association rules between sets of items in large databases","authors":[{"person_name":{"surname":"Agrawal","first_name":"R"},"affiliations":[],"email":null},{"person_name":{"surname":"Imielinski","first_name":"T"},"affiliations":[],"email":null},{"person_name":{"surname":"Swami","first_name":"A"},"affiliations":[],"email":null}],"date":{"year":"1993","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":207,"to_page":216}}},"b17":{"title":"Discovering relational emerging patterns","authors":[{"person_name":{"surname":"Appice","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Ceci","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Malgieri","first_name":"C"},"affiliations":[],"email":null},{"person_name":{"surname":"Malerba","first_name":"D"},"affiliations":[],"email":null}],"date":{"year":"2007","month":null,"day":null},"ids":null,"target":null,"publisher":"Springer","journal":null,"series":"Artificial Intelligence and Human-Oriented Computing","scope":{"volume":4733,"pages":{"from_page":206,"to_page":217}}},"b18":{"title":"Learning from order examples","authors":[{"person_name":{"surname":"Kamishima","first_name":"T"},"affiliations":[],"email":null},{"person_name":{"surname":"Akaho","first_name":"S"},"affiliations":[],"email":null}],"date":{"year":"2002","month":null,"day":null},"ids":null,"target":null,"publisher":"IEEE Computer Society","journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":645,"to_page":648}}},"b19":{"title":"Manyaspects: a system for highlighting diverse concepts in documents","authors":[{"person_name":{"surname":"Liu","first_name":"K"},"affiliations":[],"email":null},{"person_name":{"surname":"Terzi","first_name":"E"},"affiliations":[],"email":null},{"person_name":{"surname":"Grandison","first_name":"T"},"affiliations":[],"email":null}],"date":{"year":"2008","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"PVLDB","series":null,"scope":{"volume":1,"pages":{"from_page":1444,"to_page":1447}}}},"sections":[{"title":"Introduction","paragraphs":[{"text":"The growing amount of documents available in digital libraries makes difficult and arduous obtaining the desired information, and therefore demands for the development of technologies to effectively support the user in a rapid comprehension once interesting documents have been retrieved. Numerous studies have been carried out in Natural Language Processing and, in particular, in the subfield of Automatic Text Summarization in order to generate a summarizing text which conveys the most salient and important information of the original document(s) 1 . A summary can be defined as a text that is produced from one or more texts, that contains a significant portion of the information in the original text(s), and that is no longer than half of the original text(s). Summaries can be categorized in extracts, when they are created by selecting the keyphrases of the original text, and abstracts, when they are created by inferring the meaning of the source document or by re-generating the content of it 2 . The techniques oriented to the generation of abstracts require linguistic knowledge and sophisticated resources, and perform a deep analysis of the textual content by taking into account typical language constructs, such as discourse structure. The techniques based on the extracts rather perform a shallow analysis of the text and do not require linguistic knowledge. Although the extract-based techniques can produce summaries with evident problems of interpretation and cohesion among the selected portions of text, they have been proven to yield summaries whose informative level is satisfactory. This is particularly true when the extract is used as a component of another system and is not directly used by humans.","refs":[{"start":552,"end":553,"marker":"bibr","target":"#b0"},{"start":1006,"end":1007,"marker":"bibr","target":"#b1"}]},{"text":"A strategy widely investigated for extractive approaches, is that of selecting the more salient sentences through Machine Learning or Data Mining algorithms which aim at either recognizing and then classifying the sentences to be included in the summary or ranking the source sentences and then selecting those with highest rank 3 . Typically, sentences are described in terms of lexical and structural features (e.g., keywords frequency, title keywords, sentence location, indicator phrases, etc. 4 ) and represented as vectors of quantitative and categorical measures of those features (attribute-value representation). However, in some practical applications, it is also possible to exploit additional information conveyed by the structure of the original document. For example, in the case of document images obtained by scanning paper documents, sentences can be related to layout components or paragraphs. Another example is that of semistructured documents such as XML/HTML documents where sentences can be related to sections. In such situations, the classical attribute-value representation (based on the single-table assumption 5 ), according to which sentences would be represented in a single table of a relational database (each row represents a sentence and columns correspond to properties of the sentence) appears to be too restrictive for at least three reasons. First, sentences cannot be realistically considered independent observations, because their arrangement is mutually constrained. Second, relationships among sentences in the same paragraph cannot be properly represented by a fixed number of attributes in a table. Third, the representation of properties of objects related to sentences (such as layout components or sections) would lead to redundancy problems that cause changes in the underlying probability distribution of examples. Since the single-table assumption limits the representation of relationships between examples, it also prevents the discovery of this kind of patterns, which can be very useful in the context of document summarization.","refs":[{"start":329,"end":330,"marker":"bibr","target":"#b2"},{"start":498,"end":499,"marker":"bibr","target":"#b3"},{"start":1138,"end":1139,"marker":"bibr","target":"#b4"}]},{"text":"In this paper we propose an extractive approach aiming at learning to rank the source sentences extracted from document images. The proposed approach overcomes limitations posed by the single-table assumption by resorting to the relational data mining setting 5 according to which data are represented in several tables of a relational database possibly related according to foreign key constraints. This allows us to distinguish between the reference objects of analysis (sentences) and other task-relevant spatial objects (e.g. layout components), and to represent their interactions. This also allows us to represent different entities in different ways: sentences can be represented exploiting lexical and structural properties while layout components, for example, can be represented according to geometrical properties.","refs":[{"start":260,"end":261,"marker":"bibr","target":"#b4"}]}]},{"title":"Background and Related Works","paragraphs":[{"text":"Background of this work is the Document image analysis system WISDOM++1 that enables the transformation of document images into XML format by means of several complex steps 6 . Initial processing steps include binarization, skew detection, noise filtering, and segmentation. The document image is then decomposed into several constituent items which represent coherent components of the documents (e.g., text lines or halftone images), without any knowledge of the specific format. This layout analysis step precedes the interpretation or understanding of document images, whose aim is that of recognizing \"logic components\", that is, logically relevant layout components (e.g., title and section title of a scientific paper) 7 as well as extracting abstract relationships between layout components (e.g., reading order) 8 . By moving towards a higher level of abstraction, it is also possible to identify \"semantic components\" (e.g., motivations and experiments of a scientific paper) composed by several logic components (possibly belonging to different document pages) by exploiting their OCRed textual content. In this paper we add to WISDOM++ the keyphrase extraction step that is based on sentence ranking.","refs":[{"start":70,"end":71,"marker":null,"target":"#foot_0"},{"start":173,"end":174,"marker":"bibr","target":"#b5"},{"start":726,"end":727,"marker":"bibr","target":"#b6"},{"start":821,"end":822,"marker":"bibr","target":"#b7"}]},{"text":"Sentence ranking is an approach investigated mostly with extract-based techniques which implement supervised Data Mining algorithms. This assumes the availability of a set of textual documents where ranking of the sentences of each document is given. Data Mining algorithms are then used to learn a ranking model to be applied on new documents. As in our case, in the literature, learning to rank from previously ranked sentences has been also interpreted as the problem of learning a preference function.","refs":[]},{"text":"Several supervised learning techniques for sentence ranking have already been reported in the literature. Xie et al. 9 propose a evolutionary algorithm to generate ranking functions. After having preprocessed the training documents into a set of sentence feature vector, the algorithm first produces different populations of ranking functions, then, with each of them, generates an extract which is evaluated w.r.t. objective summary through the cosine measure. The best function is then used to produce the next population of functions and so on until the stop criterion is satisfied: the final ranking function is used to rank new sentences. Svore et al. 10 resort to a neural network pair to generate a ranking composed of three sentences. More precisely, given a training set consisting of the sentences (whose features include also properties derived by third-party sources) and a set of three human-generated sentences, ranking function is learned on pairs of sentences ranked w.r.t. to the similarity with three human-generated sentences. The three highest ranked sentences (namely those more similar to the human extract) are then selected for the summary. Although all cited approaches allow us to obtain summaries with high informative level, as stated before, they suffer from limitation imposed by single-table assumption that, among others, does not guarantee an adequate representation of additional information conveyed by the possible structure of the original document.","refs":[{"start":117,"end":118,"marker":"bibr","target":"#b8"},{"start":657,"end":659,"marker":"bibr","target":"#b9"}]}]},{"title":"Sentence Ranking","paragraphs":[{"text":"The problem of sentence ranking is solved by learning a preference model that leads to identify preference relations to be applied to new documents. In the proposed approach, the model is probabilistic and exploits relational patterns extracted from training data.","refs":[]}]},{"title":"Mining Preference Relations","paragraphs":[{"text":"The problem of mining preference relations between sentences can be formalized as follows. Given:","refs":[]},{"text":"• a database schema S with h relational tables S = {T 1 , T 2 , . . . , T h } • two sets PK and FK of primary and foreign key constrains on tables in S • a target relation T ∈ S representing sentences that play the role of reference objects • a precedence relation PT ∈ S with two attributes. Each tuple in this table represents an ordered pair of reference objects where the first reference object precedes the second one. It is noteworthy that in our approach, differently from other approaches, it is also possible to avoid to consider some sentences belonging to parts of the document that are not considered relevant for the task at hand (e.g. sentences in tables or sentences in references of a scientific paper). This means that the preference relation in training data does not necessarily express total ordering of sentences in training documents.","refs":[]},{"text":"By applying the Bayes theorem, P(a ≺ b|a, b) can be computed as:","refs":[]},{"text":"where:","refs":[]},{"text":"• P(a ≺ b) in (1) denotes the prior probability that a sentence precedes another. This probability might be different from 0.5, since, as stated before, training reference objects might not be totally ordered.","refs":[]},{"text":"In order to simplify the estimation of the likelihood P(a, b|a ≺ b), conditional independence is assumed (naïve Bayes assumption), according to which P(a, b|a ≺ b) can be factorized as follows:","refs":[]},{"text":"where a 1 , . . . , a m represent the set of attribute values of a and b 1 , . . . , b m represent the set of attribute values of b. However, the formulation reported above for naïve Bayesian classifiers is clearly limited to attribute-value representations. In the case of relational data, some extensions are necessary. The basic idea is that of using a set of relational patterns (a, b) to describe the considered objects, and then to define a suitable decomposition of the likelihood à la naive Bayesian classifier to simplify the probability estimation problem.","refs":[]},{"text":"Before describing how the likelihood is computed, it is necessary to provide a formal definition of relational pattern, which is a conjunction of unary and binary predicates2 of two different types: Definition 1 (Property predicate). A predicate p/2 is a property predicate associated to a table T i ∈ S -PT if the first argument of p represents the primary key of T i and the second argument represents another attribute in T i which is neither the primary key of T i nor a foreign key.","refs":[{"start":173,"end":174,"marker":null,"target":"#foot_1"}]}]},{"title":"Definition 2 (Structural predicate). A predicate p/2 is a structural predicate associated to a table T i ∈ S -PT if a","paragraphs":[{"text":"foreign key in S -PT exists that connects T i to a table T j ∈ S . The first argument of p represents the primary key of T j and the second argument represents the primary key of T i .","refs":[]}]},{"title":"Definition 3 (Relational Pattern). A Relational Pattern is in the form","paragraphs":[{"text":". * where attr/1 represents the predicate associated to the target relation T (the argument is the primary key of T ), rel/2 represents a generic structural predicate, attr/2 represents a generic property predicate and S is in the form of preference(A,B). A pattern P in this form is a relational pattern if the property of linkedness 11 ","refs":[{"start":335,"end":337,"marker":"bibr","target":"#b10"}]}]},{"title":"is satisfied (e.g. each variable C k or C j should be linked to the variables A or B by means of structural predicates).","paragraphs":[{"text":"The likelihood is then computed as follows: The application of the classical naïve Bayes independence assumption to all literals in","refs":[]},{"text":"R k is not correct, since it may lead to underestimate the probabilities for the case of precedence relations for which several patterns are represented (see 12 ). In fact, when working with redundant literals in F , P(a, b|a ≺ b) will approach zero. We solve this problem by exploiting the notion of factorization 13 that allows us to remove redundant literals. For this reason, we impose P(a, b|a ≺ b) = P(F|a ≺ b) for any minimal factor F of F and we compute this probability using the naïve Bayesian assumption on literals in F.","refs":[{"start":158,"end":160,"marker":"bibr","target":"#b11"},{"start":315,"end":317,"marker":"bibr","target":"#b12"}]}]},{"title":"Patterns construction","paragraphs":[{"text":"The relational pattern discovery is performed by exploring level-by-level the lattice of relational patterns ordered according to a generality relation ( ) between patterns. Formally, given two patterns P1 and P2, P1 P2 denotes that P1 (P2) is more general (specific) than P2 (P1). Hence, the search proceeds from the most general pattern and iteratively alternates the candidate generation and candidate evaluation phases (levelwise). In 14 , the authors propose an enhanced version of the level-wise method 15 to discover patterns from data in multiple tables of a relational database. Candidate patterns are searched in the space of linked relational patterns, which is structured according to the θ-subsumption generality order 16 .","refs":[{"start":439,"end":441,"marker":"bibr","target":"#b13"},{"start":509,"end":511,"marker":"bibr","target":"#b14"},{"start":732,"end":734,"marker":"bibr","target":"#b15"}]},{"text":"This makes possible to perform a levelwise exploration of the lattice of relational patterns ordered by θ-subsumption. In particular, patterns are discovered by generating the pattern space one level at a time starting from the most general pattern (the pattern that contains only the pre f erence/2 predicate) and then by applying a breadth-first evaluation in the lattice of relational patterns ordered according to θ . Indeed, we are not interested in all possible patterns, but only in those satisfying the following property: L.add arg max b i ∈V/L S U MPREF G (b i ) ; 5: end while (supp a≺b (P) > minS up ∨ supp b≺a (P) > minS up)∧(GR a≺b (P) > minGR ∨ GR b≺a (P) > minGR) where: minS up ∈ [0, 1) and minGR ∈ [1, +∞) are user defined thresholds; supp a≺b (P) represents the support of the pattern P with respect to a preference relation; GR a≺b (P) represents the growth rate computed as supp a≺b (P)/supp b≺a (P).","refs":[]},{"text":"This restriction of the search space permits us to apply different pruning criterion. The monotonicity property of the generality order θ with respect to the support value (i.e., a superset of an infrequent pattern cannot be frequent) 17 can be exploited to avoid generation of infrequent relational patterns. The monotonicity property does not hold for the growth rate: a refinement of a pattern whose growth rate is lower than the threshold minGR may or may not be a pattern with growth rate lower than minGR. However, the growth rate can be used for pruning as well. In particular, it is possible to stop the search when it is not possible to increase the growth rate with additional refinements 18 . Finally, as stopping criterion, the number of levels in the lattice to be explored can be limited by the user-defined parameter MAX L ≥ 1 which limits the maximum number of predicates in a candidate emerging pattern.","refs":[{"start":235,"end":237,"marker":"bibr","target":"#b16"},{"start":699,"end":701,"marker":"bibr","target":"#b17"}]}]},{"title":"Ranking Reconstruction","paragraphs":[{"text":"In this step, the goal is to build a ranking of sentences (reference objects). Formally, Given: A database with schema S -PT (the same schema used for training), Find: A total ordering of sentences in the target table T belonging to relevant semantic components.","refs":[]},{"text":"The algorithm follows the proposal reported in 19 and we aim at iteratively evaluating the most promising sentence to be appended to the resulting ranking. Let:","refs":[{"start":47,"end":49,"marker":"bibr","target":"#b18"}]},{"text":"is the set of weighted edges where weights are the probabilities P(a ≺ b|a, b) computed according to (1), • S U MPREF G : V → [0, #V] be a preference function defined as: S U MPREF G (a) = b∈V,b a w a,b , Algorithm 5 fully specifies the method for the ranking identification. The rationale is that at each step, a sentence is added to the final ranking. Such a sentence is that for which S U MPREF G ( ) is the highest. Higher values of S U MPREF G ( ) are given to sentences which have a high sum of probabilities to precede other sentences. Once the ranking of the sentences has been identified, the best m sentences are used to define the summary.","refs":[]}]},{"title":"Data extraction and representation","paragraphs":[{"text":"Reference objects correspond to descriptions of sentences extracted from document images. The representation of the sentences is obtained by means of a phase of natural language processing which extracts sentence features. Extraction includes tokenization, sentence splitting, part-of-speech (POS) tagging, stop-word removing and stemming.","refs":[]},{"text":"The execution in sequence of these techniques allows us to represent sentences in terms of the features:","refs":[]},{"text":"• ADJECTIVE POS FREQUENCY, VERBALFORM POS FREQUENCY, NOUN POS FREQUENCY express the normalized frequency of the words of some POS categories included in the analyzed sentence w.r.t. the total set of words in the same sentence. • TF IDF WORD1, . . . , TF IDF WORDn denote the presence in the sentence of the n words having highest t fid f values over the training corpus.","refs":[]},{"text":"• POSITION INSIDE DOCUMENT, POSITION INSIDE SECTION represent the normalized position of the sentence in the document and in the semantic component, respectively.","refs":[]},{"text":"In addition, we also consider the presence of indicator phrases, typically used in discourse analysis, that give important information about the structure of the discourse 4 . Indicator phrases are expressed as a set of CUE WORDs. We use semantic components (SEMANTIC COMPONENTS), logical components (BLOCKS) and the preference table (PREFERENCE). Logical components are described according to features that can be classified as: Locational x pos centre/y pos centre: position of the centroid of the logical component w.r.t. the x / y axis; Geometrical height/width: the height/width in pixels of a logical component; Logical: \"logical label\" associated to a logical component; Content type type o f : content type of a logical component (Possible values are: {image, text, horizontal line, vertical line, graphic, mixed}).","refs":[{"start":172,"end":173,"marker":"bibr","target":"#b3"}]}]},{"title":"Experiments","paragraphs":[{"text":"We explored the applicability the proposed method to the domain of document image understanding in order to generate summaries from key-phrases found in the semantically relevant layout components.","refs":[]},{"text":"Two datasets of document images were considered. The first dataset (denoted as TPAMI) is a set of twentythree scientific papers published as either regular or short in the IEEE Transactions on Pattern Analysis and Machine Intelligence in the January and February issues of 1996 in the multi-column document. We processed 210 document images in all, an average number of 9,13 images per document. The second dataset (denoted as ICML) is a set of thirty scientific papers published as either regular or short in the International Conference on Machine Learning of 2009 in the multi-column format. We processed 240 document images in all, an average number of 8 images per document. Papers are processed in order to segment them, perform layout analysis, identify logic type of logical components and identify semantic components. Admissible semantic components are abstract, method, motivation, experiment result, rejected among them, in this work, relevant semantic components considered for summarization are method and motivation. We consider only method and motivation because these components typically report the main contribution of the paper. Figure 1 reports an illustration. Three sentences (denoted as a, b, d) are selected from the section of Introduction, which is recognized as the component motivation. While one sentence (denoted as c) is selected from the section of Bayesian Networks and Probabilistic Inference (recognized as the component method). The summary includes these four sentences ranked as a, b, c, d.","refs":[{"start":1156,"end":1157,"marker":"figure","target":"#fig_2"}]},{"text":"The preference relation is constructed by ranking the sentences contained in relevant semantic components on the basis of the abstract. In particular, the ranking used for training exploits the cosine similarity between the sentences in the abstract of the document wa j and sentences in method and motivation semantic components w k :","refs":[]},{"text":"where each sentence (wa j or w k ) is represented in form of a t fid f vector of n elements. The score used for ranking (and, then defining the training preference relation) is:","refs":[]},{"text":"We evaluated the results with two evaluation measures. The first measure is ROUGE-N and it is determined with the created summary and reference summary (abstract). It is implemented as","refs":[]},{"text":"where Count match(n-gram) is the maximum number of n-grams that co-occur in the S ummary and Abstract, Count n-gram is the count of the n-grams in the Abstract. We used ROUGE-N as ROUGE-1, namely, the 1-grams were considered. The second measure is the cosine similarity (as defined above) but it is determined with the created summary and reference summary (abstract). Evaluation was performed by means of a six fold cross validation for TPAMI dataset and five fold cross validation for ICML dataset with the following setup: minS up = 0.05, minGR = 1.1, MAX L = 3, n = 10 and m = 10.  Comparisons was performed between the WISDOM++ and several unsupervised techniques 20 with respect to the two evaluation measures. The results in Table 1 show better performances of our solution, while the results in Table 2 reveal a behaviour comparable with the best competitors. It is worth of noting the difference between the two measures. When using the cosine similarity, we evaluate frequency-based quantities of the words representative of the documents. So, an high value of the similarity indicates that the summary contains sentences where representative words occur. Differently, Rouge-1, accounts the presence of the same 1-ngrams which could be even few representative for the content of the document. This denotes the capacity of the proposed solutions to produce summary with phrases which are really salient for the meaning of the document.","refs":[{"start":669,"end":671,"marker":"bibr","target":"#b19"},{"start":738,"end":739,"marker":"table","target":"#tab_0"},{"start":809,"end":810,"marker":"table","target":"#tab_1"}]}]},{"title":"Conclusions","paragraphs":[{"text":"In this paper we propose an extractive approach aiming at learning to rank the source sentences extracted from document images. The proposed approach resorts to the relational data mining setting in order to adequately exploit lexical properties of the text as well as structural, logical and semantic properties conveyed by the nature of the original documents. The method is based on a probabilistic learner that makes use of discovered relational patterns. Experiments on real-world datasets and comparisons with other techniques prove the effectiveness of the proposed approach. For future work, we plan two research directions. The first one is to apply the proposed method on a large corpus (not necessarily scientific documents). In the second one, we intend to consider the opportunity of automatically defining the optimal number of sentences to be included in the summary. To this purpose, automatic threshold determination algorithms can be used.","refs":[]}]}],"tables":{"tab_0":{"heading":"Table 1 .","description":"Cosine similarity computed between the abstracts and the generated summaries.","rows":[["Dataset WISDOM++ GREEDYEXP GREEDYUNIFORM","SVD","FURTHEST"],["TPAMI","82.19","79.52","79.63","80.61","82.26"],["ICML","61.25","59.39","60.54","53.53","54.68"],["Dataset WISDOM++ GREEDYEXP GREEDYUNIFORM SVD FURTHEST"],["TPAMI","0.96","0.98","0.98","0.90","0.61"],["ICML","0.82","0.87","0.87","0.80","0.44"]]},"tab_1":{"heading":"Table 2 .","description":"Rouge-1 computed between the abstracts and the generated summaries.","rows":[]}},"abstract":{"title":"Abstract","paragraphs":[{"text":"Document summarization involves reducing a text document into a short set of phrases or sentences that convey the main meaning of the text. In digital libraries, summaries can be used as concise descriptions which the user can read for a rapid comprehension of the retrieved documents. Most of the existing approaches rely on the classification algorithms which tend to generate \"crisp\" summaries, where the phrases are considered equally relevant and no information on their degree of importance or factor of significance is provided. Motivated by this, we present a probabilistic relational data mining method to model preference relations on sentences of document images. Preference relations are then used to rank the sentences which will form the final summary. We empirically evaluate the method on real document images.","refs":[]}]}}