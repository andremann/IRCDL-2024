{"bibliography":{"title":"Searching and Classifying Affinities in a Web Music Collection","authors":[{"person_name":{"surname":"Orio","first_name":"Nicola"},"affiliations":[{"department":"Department of Cultural Heritage","institution":"University of Padua","laboratory":null}],"email":"nicola.orio@unipd.it"}],"date":null,"ids":{"DOI":"10.1007/978-3-319-56300-8_6","arXiv":null},"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"keywords":[],"citations":{"b0":{"title":"Fast recognition of remixed music audio","authors":[{"person_name":{"surname":"Casey","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Slaney","first_name":"M"},"affiliations":[],"email":null}],"date":{"year":"2007","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":1428,"to_page":1428}}},"b1":{"title":"Image retrieval: Ideas, influences, and trends of the new age","authors":[{"person_name":{"surname":"Datta","first_name":"R"},"affiliations":[],"email":null},{"person_name":{"surname":"Joshi","first_name":"D"},"affiliations":[],"email":null},{"person_name":{"surname":"Li","first_name":"J"},"affiliations":[],"email":null},{"person_name":{"surname":"Wang","first_name":"J"},"affiliations":[],"email":null}],"date":{"year":"2008","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"ACM Comput. Surv","series":null,"scope":{"volume":40,"pages":{"from_page":60,"to_page":60}}},"b2":{"title":"On the evolution of clusters of near-duplicate web pages","authors":[{"person_name":{"surname":"Fetterly","first_name":"D"},"affiliations":[],"email":null},{"person_name":{"surname":"Manasse","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Najork","first_name":"M"},"affiliations":[],"email":null}],"date":{"year":"2003","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"J. Web Eng","series":null,"scope":{"volume":2,"pages":{"from_page":228,"to_page":246}}},"b3":{"title":"A highly robust audio fingerprinting system with an efficient search strategy","authors":[{"person_name":{"surname":"Haitsma","first_name":"J"},"affiliations":[],"email":null},{"person_name":{"surname":"Kalker","first_name":"T"},"affiliations":[],"email":null}],"date":{"year":"2003","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"J. New Music Res","series":null,"scope":{"volume":32,"pages":{"from_page":211,"to_page":221}}},"b4":{"title":"Electronic media, creativity and plagiarism","authors":[{"person_name":{"surname":"Imran","first_name":"N"},"affiliations":[],"email":null}],"date":{"year":"2010","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"ACM SIGCAS Comput. Soc","series":null,"scope":{"volume":40,"pages":{"from_page":25,"to_page":44}}},"b5":{"title":"Computer vision for music identification","authors":[{"person_name":{"surname":"Ke","first_name":"Y"},"affiliations":[],"email":null},{"person_name":{"surname":"Hoiem","first_name":"D"},"affiliations":[],"email":null},{"person_name":{"surname":"Sukthankar","first_name":"R"},"affiliations":[],"email":null}],"date":{"year":"2005","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":597,"to_page":604}}},"b6":{"title":"How does Chromaprint work?","authors":[{"person_name":{"surname":"Lalinsk√Ω","first_name":"L"},"affiliations":[],"email":null}],"date":{"year":"2011","month":"01","day":"31"},"ids":null,"target":"https://oxygene.sk/2011/01/how-does-chromaprint-work/","publisher":null,"journal":null,"series":null,"scope":null},"b7":{"title":"Near-duplicate video retrieval: Current research and future trends","authors":[{"person_name":{"surname":"Liu","first_name":"J"},"affiliations":[],"email":null},{"person_name":{"surname":"Huang","first_name":"Z"},"affiliations":[],"email":null},{"person_name":{"surname":"Cai","first_name":"H"},"affiliations":[],"email":null},{"person_name":{"surname":"Shen","first_name":"H"},"affiliations":[],"email":null},{"person_name":{"surname":"Ngo","first_name":"C"},"affiliations":[],"email":null},{"person_name":{"surname":"Wang","first_name":"W"},"affiliations":[],"email":null}],"date":{"year":"2013","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"ACM Comput. Surv","series":null,"scope":{"volume":45,"pages":{"from_page":1,"to_page":44}}},"b8":{"title":"Correlation-based retrieval for heavily changed nearduplicate videos","authors":[{"person_name":{"surname":"Liu","first_name":"J"},"affiliations":[],"email":null},{"person_name":{"surname":"Huang","first_name":"Z"},"affiliations":[],"email":null},{"person_name":{"surname":"Shen","first_name":"H"},"affiliations":[],"email":null},{"person_name":{"surname":"Cui","first_name":"B"},"affiliations":[],"email":null}],"date":{"year":"2011","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"ACM Trans. Inform. Syst","series":null,"scope":{"volume":29,"pages":{"from_page":25,"to_page":25}}},"b9":{"title":"An efficient identification methodology for improved access to music heritage collections","authors":[{"person_name":{"surname":"Montecchio","first_name":"N"},"affiliations":[],"email":null},{"person_name":{"surname":"Di Buccio","first_name":"E"},"affiliations":[],"email":null},{"person_name":{"surname":"Orio","first_name":"N"},"affiliations":[],"email":null}],"date":{"year":"2012","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"J. Multimedia","series":null,"scope":{"volume":7,"pages":{"from_page":145,"to_page":158}}},"b10":{"title":"A phylogenetic analysis of near-duplicate audio tracks","authors":[{"person_name":{"surname":"Nucci","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Tagliasacchi","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Tubaro","first_name":"S"},"affiliations":[],"email":null}],"date":{"year":"2013","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":99,"to_page":104}}},"b11":{"title":"Locality-sensitive hashing for finding nearest neighbors","authors":[{"person_name":{"surname":"Slaney","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Casey","first_name":"M"},"affiliations":[],"email":null}],"date":{"year":"2008","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"IEEE Signal Process. Mag","series":null,"scope":{"volume":25,"pages":{"from_page":128,"to_page":131}}},"b12":{"title":"The Shazam music recognition service","authors":[{"person_name":{"surname":"Wang","first_name":"A"},"affiliations":[],"email":null}],"date":{"year":"2006","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"Commun. ACM","series":null,"scope":{"volume":49,"pages":{"from_page":44,"to_page":48}}}},"sections":[{"title":"Introduction","paragraphs":[{"text":"The automatic detection of duplicates and near duplicates of textual documents has become an important research trend after the development of the Web [3]. In fact the same textual information may be contained, with minor modifications, in several web pages maintained by different organizations or individuals. One of the reasons why there exists a large number of near duplicate pages can be tracked back to a general tendency on the Web to underestimate the importante of copyright. And in fact, near duplicate identification has also important applications in patent analysis and in plagiarism detection [6].","refs":[{"start":151,"end":154,"marker":"bibr","target":"#b2"},{"start":608,"end":611,"marker":"bibr","target":"#b4"}]},{"text":"With the increasing availability of multimedia content on the Web and in cloud services duplicate detection is gaining relevance also for media other than text, in particular to help managing large video collections [9] and to improve image retrieval tasks [2]. Most of these approaches are based on the concept of fingerprinting as a way to reduce the very high dimensionality of the problem. The basic idea of fingerprinting is that multimedia objects can be represented by a compact array of features, with a size orders of magnitude smaller than the original object, allowing feature indexing and in general faster processing. Moreover, a robust fingerprinting algorithm is able to extract features that are mostly related to human perception, in order to identify duplicates of a given multimedia object even when some post-processing has been applied.","refs":[{"start":216,"end":219,"marker":"bibr","target":"#b7"},{"start":257,"end":260,"marker":"bibr","target":"#b1"}]},{"text":"The approach can be applied also to the music domain, and in fact acoustic fingerprinting is a well-known technique commercially exploited for music identification, which is at the basis of Shazam!, one of the most popular music services on the Internet [14], and of many others systems, such as the MusicID software patented by Gracenote [4] and the AudioID software used by MusicBrainz [8] based on an application of computer vision [7].","refs":[{"start":254,"end":258,"marker":"bibr","target":"#b12"},{"start":339,"end":342,"marker":"bibr","target":null},{"start":388,"end":391,"marker":"bibr","target":"#b6"},{"start":435,"end":438,"marker":"bibr","target":"#b5"}]},{"text":"However, detection of duplicates and near duplicates has been relatively less investigated in the case of music perhaps considering it a marginal problem in comparison with audio identification. A focus on remixing, which is one of the reasons why music near duplicates exist, has been given in [1] where Locality Sensitive Hashing has been applied as an alternative to audio fingerprinting. An interesting approach [12] proposes to model the processing operators that possibly create music duplicates and near duplicates.","refs":[{"start":295,"end":298,"marker":"bibr","target":"#b0"},{"start":416,"end":420,"marker":"bibr","target":"#b10"}]},{"text":"Although it does not apply to the test collection used in this work, in many cases near duplicates are created ad-hoc to dodge digital rights management software and publish copyrighted material on the web [10]. Yet, in the music domain most of duplicates and near duplicates exist as a natural process of artistic creation, which is intrinsically based on resemblance and differentiation with existing music, often using already published tracks as the basis to create new music. This paper focuses on this latter problem, the detection and classification of affinities between music tracks in a music digital library that is the basis for an online web service of music delivery.","refs":[{"start":206,"end":210,"marker":"bibr","target":"#b8"}]}]},{"title":"Affinities in a Music Collection","paragraphs":[{"text":"The goal of the project described in this paper is to improve the access of large music collections, as the one available from music web services, by detecting variants of the stored songs and by classifying the kind of variant. The results can be applied both to large web collections, where digital objects can be provided by the end users and thus there is basically no control on the inclusion of new files in the existing collection, and to audio digital libraries, where management can be improved by the detection of content similarities. The objectives, for both domains, can be summarized as follows:","refs":[]},{"text":"1. Duplicate removal helps saving storage space; although the increasing number of cloud services reduced its cost, storage is still a relevant cost for institutions. 2. Near duplicate detection can highlight inconsistencies in metadata information, which is the typical case when content is uploaded by the end users; moreover, detection can be carried out while new content is uploaded thus, in case near duplicates are already present, the use can be suggest with suitable metadata. 3. It has been shown that metadata insertion is an error prone process even in the case of digitization campaigns for music digital libraries [11], because usually digitization is carried out as a separate process in respect to metadata creation; the identification of near duplicates can be used to discover the presence of errors in the cataloguing process. 4. The content-based music search engine of the digital library should be aware of the presence of duplicate material; similarity matches tend to cluster around duplicates of a given track, possibly hiding additional relevant tracks. 5. The presence of subtle differences between tracks may be of interest for musicologists, musicians and eventually for the simple music fans; alternate takes of a given composition or different live versions of a studio recording are likely to be presents in the collection and be both relevant for the final user. 6. Music composition is increasingly a collaborative process, where the final product is often the result of manipulation of existing material that is remixed, looped, sampled, and so on; the possibility to track this process, which goes beyond the mere identification of the new track, can improve music enjoyment and partially guarantee correct attribution to different authors.","refs":[{"start":628,"end":632,"marker":"bibr","target":"#b9"}]},{"text":"This paper presents a research carried out in collaboration with the staff of a music digital library which is the basis of a web service for online music broadcast and delivery. The methods have been developed to address the real needs of the music experts who created and manage the music collection. Although it addresses the specific needs of a single web service, it is expected that the methodology can be extended also to other similar collections and, possibly, to social networks where content is directly provided by end users.","refs":[]}]},{"title":"The Test Collection","paragraphs":[{"text":"The music collection used to train the model and run the tests contains more than 350,000 audio tracks in MP3 format for an estimate global duration of about 20,000 h. The collection has been created in more than ten years by a group of music experts, starting from commercially available CDs that have been individually bought and converted in MP3 format. Descriptive metadata are managed by a DBMS while audio tracks are maintained by an external storage. For this experiment, the owner of the collection granted access to a limited amount of cataloguing information -basically title, authors and main performer -and full access to the MP3 content. The collection focuses on pop and rock genres, with less than 10% of the tracks belonging to classical, jazz and other repertoires. Clearly the used collection is orders of magnitude smaller that the one of popular web services, such as Spotify of Last.fm, but we considered it large enough to obtain significant results.","refs":[]},{"text":"Since popular songs are likely to be included in different CD editions -first release, remastering, best of, compilations -a certain redundancy was expected with a number of duplicates inside the collection. These can be, as it has been shown by the initial results, exact duplicates when the same audio source was present in different CDs, and near duplicates when different takes of the same song have been published or when remastering heavily affected the audio content. Because of the long time span required to create the collection, a number of different tools has been used for MP3 ripping, resulting in a different quality of the lossy compression and thus in audible differences between songs, that thus become near duplicates a well. It has also to be considered that a number of different persons was involved in the cataloguing process, with potential inconsistencies in the metadata creation that make metadata not completely reliable.","refs":[]},{"text":"Being used as the source material for the creation of the soundtrack of TV programs of a major Italian broadcaster, the collection contains also the result of post-processing of the original tracks. Hence the collection includes also what can be called far duplicates. In this context, far duplicates are considered two tracks that share a consistent part of audio content like in the case of remixing of song with additional instruments, loops used as the basis of new songs, mashups using more than one audio source and different montages of the same audio material. We define all the kind of duplicatesexact, near and far -with the general term affinities. The typology of affinity thus depends on a number of factors: the amount of audio material that is shared between two songs, the acoustic differences of the same source due to post-processing and remixing, the presence of different editing.","refs":[]},{"text":"All the tracks in the collection were already fingerprinted because an audio identification engine was already in place as the result of a previous project. The audio fingerprinting engine aims at identifying the usage of the audio tracks inside TV broadcasts in order to manage legal rights of authors, editors, performers and labels. The existing fingerprints, which are described in the next section, were computed in order to identify also very short music excerpts also in the presence of additional signals, mainly speech and environmental noise (e.g. clapping, car engines, crowd cheering, and so on). The computation of the 350,000 audio fingerprints required approximately two months on a octa-core machine with processors at 1.6 GHz. This relatively long computation time is comparable to the one required to compute grab music from an audio CD or to download/ upload the files.","refs":[]}]},{"title":"Detection of Affinities","paragraphs":[{"text":"Given the size of the audio collection, a pairwise comparison of all the tracks was impracticable. Even on the fast octa-core machine available for the experiments, the existing audio fingerprinting engine would have completed the identification of affinities within all the songs in an estimated time of about three months. For this reason we decided to divide the procedure in two steps.","refs":[]}]},{"title":"First Step: Pruning Candidate Affinities","paragraphs":[{"text":"A common approach to audio fingerprinting consists of summarizing with a sequence of integer numbers the audio content of short overlapping parts of the audio signal. A complete song is thus transformed in a sequence of integers, with the characteristic that similar audio excerpts are represented by the same integer. Thus, we can view this approach as audio hashing where collisions between buckets happen when the original audio excerpts are perceptually similar. A general approach exploits Locality Sensitive Hashing to create a set of hash function that guarantees at least a collision in case of similar audio content [13]. The fingerprints used in this work were computed following a simpler approach, proposed in [5], which uses a single hashing function computed from the frequency representation of the signal.","refs":[{"start":625,"end":629,"marker":"bibr","target":"#b11"},{"start":722,"end":725,"marker":"bibr","target":"#b3"}]},{"text":"Given an audio track t k sampled at the common CD rate of 44.1 kHz, we divide it in frames of about 0.1 s and compute their Fast Fourier Transform. Hash values are computed according to the distribution of the signal energy in a number of spectral bands. Thus the original track t k can be represented by a sequence of time ordered hash values","refs":[]},{"text":"where L depends on the length of the audio track and in an even more compact way as a set of unordered hash values","refs":[]},{"text":"where D is the number of distinct hash values.","refs":[]},{"text":"A first approximation of the affinity between two tracks t h and t k can thus be computed as the percentage of hash values they have in common, that is","refs":[]},{"text":"where the normalizing factor guarantees that the maximum affinity value is 1 when a track is completely contained into the other (or the two tracks are identical).","refs":[]},{"text":"The results of the first step are summarized in Table 1. The analysis showed that the collection contained 1057 exact duplicates (0.3% of the whole collection), at least from the point of view of the audio content because the actual size and content of the files may slightly differ. Although this has not been tested extensively, it is likely that almost all of these pairs can be identified with simple hashing techniques such as MD5. Another 104 pairs overlapped by more than 90% of their audio fingerprints. Since this high overlap is likely to be related to the use of different lossy compression software applied to the same CD track (according to the collection managers three different software were used along the years), this result seems to show that the fingerprinting technique is quite robust to lossy compression. These 1161 song pairs have been directly reported to the collection administrators in order to have one of the two files removed without additional manual checking. These files have not been used in subsequent analyses. It is interesting to note that in many cases the two songs of a pair were catalogued with different titles, which explains the double acquisition of the same material. Thus the analysis had a major impact on metadata correction, while the effect on MP3 cleanup was not particularly relevant in terms of storage reduction. The selection of the correct title in case of inconsistencies was carried out by a pool of experts. Yet, the first step aimed at pointing out near and far candidates, to be checked in the second step of the analysis. There were 712 song pairs that overlapped for more than a half of their audio content while the largest group of song pairs (2098) had an overlap between one quarter and a half of their content. Finally, a group of 1041 song pairs had an overlap between one tenth and one quarter. We decided not to consider in further analyses song pairs with an overlap smaller than one tenth. The choice of the thresholds was made according to the collection managers, in order to prioritize the process of manually investigating the identified affinities. The choice of ignoring overlaps smaller than 10% was another requirement in order to let the human intervention affordable and reduced the number of false positives basically to zero. A second experiment on affinity detection will be organized in the future in order to deal also with the remaining song pairs and to investigate in more details how false positives can impact the overall process of tracking affinities. False negatives were not measured with this collection. Yet, in a previous experiment carried out with a selection of 1000 songs the number of erroneous detection was about 6.3%.","refs":[{"start":54,"end":55,"marker":"table","target":"#tab_0"}]},{"text":"After the first step we obtained a total of 3851 song pairs to inspect in more detail during the second step of the analysis. Having reduced consistently the size of the problem, the second step can focus on effectiveness without having to deal with scalability issues.","refs":[]}]},{"title":"Second Step: Pairwise Match Between Affinities","paragraphs":[{"text":"The output of the first step is a list of song pairs annotated with their affinity value as an overall measure of the shared audio content. The second step aims at refining the computation of affinities with a more descriptive representation of the similarities between songs. For this reason we represent each track as a sequence of time ordered hash values and, for the sake of clarity, we assume that a generic song pair is always in","refs":[]},{"text":"where the length","refs":[]},{"text":", thus in all subsequent equations we assume N < M.","refs":[]},{"text":"According to [5] it is possible to compute hash values in order to define a similarity function between them. For instance, if hash values are in binary form the similarity","refs":[{"start":13,"end":16,"marker":"bibr","target":"#b3"}]},{"text":") can be set inversely proportional to the hamming distance between h 1 i and h 2 j (which can be easily normalized in the interval [0,1]). It is then possible to compute, for any short time interval in l 2 , the best matching time interval in l 1 , according to equations","refs":[]},{"text":"where m j is the similarity value of the best match between the two tracks around time position j of l 2 and p j is the corresponding time position in l 1 . The plot of these two functions can give interesting insight if paired with a manual inspection of the corresponding audio tracks. For instance, Fig. 1 shows an example on how exact duplicates are represented, in order to have a better understanding of the results presented in the subsequent figures. The top graphs depict the trend of m j and the bottom graphs depict p j . For all the graphs, the x-axis represents time of the longer track l 2 , in seconds. The y-axis in the m j (top)  graphs represents the value, in log scale, of the best match between the two tracks, while the y-axis in the p j (bottom) graphs represents the time position, in seconds, of the best match on the shorter track. Thus two identical tracks have the top graph consistently equal to zero and the bottom graph coincident to the bisect of the first quadrant (in order to save space on the page, the aspect ratio of the bottom graphs has been compressed along the y-axis). Two typologies of near duplicates are shown in Figs. 2 and3. In particular, Fig. 2(a) represents the effect of heavy audio remastering of the same original material. While the two tracks are perfectly aligned (bottom) the value of the best match m j (top) reveals the result of post-production which, in this particular case, is almost negligible at the beginning of the tracks becoming more relevant towards the end. Figure 2(b) represents the effect of a lighter remastering, which affects consistently the value of the best match m j (top). Another case of near duplicates is encountered when the two tracks slightly differ in the orchestration. For instance, in Fig. 3(a) the two songs are almost identical apart from three short excerpts towards the end of the song, where one of the takes has a choir doubling the main voice. Similarly, Fig. 3(b) shows differences in two longer parts, which correspond to two choruses where a clearly audible synthesizer has been added in the orchestration. In all these cases the linear monotonic trend of p j is a good evidence of a near-duplicate, while the trend of m j may help discriminate between remastering and alternative takes. Clearly, the choice of whether maintaining or not both tracks depends on the usages of the music collection. For the purpose of musicological analyses the two tracks, either remastered or different takes, are equally interesting and should be maintained, possibly with the indication of their differences. For the purpose of a TV broadcaster, the tracks are basically interchangeable since the average audience will never notice their differences.","refs":[{"start":307,"end":308,"marker":"figure","target":null},{"start":1165,"end":1166,"marker":"figure","target":null},{"start":1170,"end":1171,"marker":"figure","target":null},{"start":1193,"end":1194,"marker":"figure","target":null},{"start":1537,"end":1538,"marker":"figure","target":null},{"start":1783,"end":1784,"marker":"figure","target":null},{"start":1960,"end":1961,"marker":"figure","target":null}]},{"text":"Among the results of the first step we identified a number of far duplicates, that is song pairs that share a substantial part of the audio content but cannot be considered simple variants of the same audio source.","refs":[]},{"text":"We considered two main typologies: mashups and montages. Examples of mashups are shown in Fig. 4, where in both cases the audio material contained in track l 1 is used to create l 2 , possibly in combination with additional content taken from other tracks. This can easily be seen comparing the initial part of the top and bottom graphs. The best match m j is quite low and corresponds to random time correspondences of p j . When the mashup track starts using the audio material of the other track, the best match increases its value and the corresponding positions proceed aligned as in the case of near duplicates. Examples of different montages are shown in Fig. 5. Here the two tracks share part or even all of the audio content, which is organized in different ways along time. This operation can be surprising if we consider normal pop songs, but it is not uncommon in instrumental music -especially when sound samples are used instead of real music instruments -where the author composes and performs independent parts and then combines them in different ways to create variants final track. For instance, Fig. 5(a) compares the opening and closing tracks of a TV program. The two tracks have basically the same beginning and almost the same ending while from 38 to 55 s of l 2 the diagonal lines show that different parts of l 1 have been used. Figure 5(b) shows that there is a very high value of the best match m j along all l 2 but the corresponding elements of l 1 have been combined differently as shown by the discontinuities of p j .","refs":[{"start":95,"end":96,"marker":"figure","target":"#fig_1"},{"start":667,"end":668,"marker":"figure","target":"#fig_3"},{"start":1119,"end":1120,"marker":"figure","target":"#fig_3"},{"start":1361,"end":1362,"marker":"figure","target":"#fig_3"}]},{"text":"A particular case of far duplicates are loops. It may be argued that loops are more likely to be near duplicates, because one track uses and repeats many times the audio content of the other. This is shown in Fig. 6(a), where the longer track contains almost exact repetitions of the shorter one with the only instants with lower m j values at the joints between repetition. Yet, the use of loops is common practice for hip-hop artists, who compose new songs directly from already published recordings. Although this is probably not the case of the comparison shown in Fig. 6(b) -the two songs have the same name -the trends of the two graphs show that the longer track is based on the repetition of the shorter plus additional instrumentation, which may be the typical situation where part of a song is looped and combined with additional material to create a new song. Clearly, the simple approach of counting the common fingerprints of the first step can results also in false positives. Figure 7 shows two cases of false positives, where the percentage of common fingerprints may be explained by the use of the same audio samples probably taken from the same sound library. The trend of both bottom and top graphs are quite different from the ones shown in the previous figures, so the task of identifying false affinities does not present high complexity.  ","refs":[{"start":214,"end":218,"marker":"figure","target":"#fig_2"},{"start":574,"end":575,"marker":"figure","target":"#fig_2"},{"start":998,"end":999,"marker":"figure","target":"#fig_4"}]}]},{"title":"Discussion","paragraphs":[{"text":"Large multimedia digital collections are increasingly available on the Web, posing new challenges in organizing, storing and accessing the material. This paper focuses on a typical problem of music collections that is the presence of similar material with different levels of variations, which results in exact, near and far duplicates. We proposed the term affinities to refer to all these variations. From the results of our initial experiments, it seems to be possible to efficiently search for affinities even in a large music collection and, furthermore, to describe the typology of affinity between two audio files with the aid of a pair of graphs representing the level of match between two parallel audio excepts and the alignment curve. The trend of the two graphs that can be interpreted in order to identify the kind of affinity. It is expected that visual identification would be faster and, in case of long audio excerpts, even more reliable than identification based on pure listening. Yet, the next step in the approach will be the automatic classification of affinities, which can be based on the statistical properties of the graphs and on the parallel analysis of the matching and the alignment curves.","refs":[]}]}],"tables":{"tab_0":{"heading":"Table 1 .","description":"Amount of common fingerprints between song pairs in the collection.","rows":[["Overlap","# song pairs","% song pairs"],["Complete (af = 1)","1057","0.3%"],["High (af > 0.9)","104","0.03%"],["Partial (af > 0.5)","712","0.2%"],["Low (af > 0.25)","2098","0.6%"],["Minimal (af > 0.1)","1041","0.3%"],["Total","5012","1.43%"]]}},"abstract":{"title":"Abstract","paragraphs":[{"text":"Online music libraries available on the Web contain a large amount of audio content that is usually the result of digitization of analogue recordings or the direct acquisition of digital sources. The acquisition process is carried out by several persons and may last a number of years, thus it is likely that the same or similar audio content is present in different versions. This paper describes a number of possible similarities, which are called affinities, and presents a methodology to detect the kind of affinity from the automatic analysis and matching of the audio content.","refs":[]}]}}