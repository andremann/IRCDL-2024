{"bibliography":{"title":"Bidirectional LSTM Recurrent Neural Network for Keyphrase Extraction","authors":[{"person_name":{"surname":"Basaldella","first_name":"Marco"},"affiliations":[{"department":"Department of Mathematics, Computer Science, and Physics","institution":"University of Udine","laboratory":"Artificial Intelligence Laboratory"}],"email":null},{"person_name":{"surname":"Antolli","first_name":"Elisa"},"affiliations":[{"department":"Department of Mathematics, Computer Science, and Physics","institution":"University of Udine","laboratory":"Artificial Intelligence Laboratory"}],"email":"antolli.elisa@spes.uniud.it"},{"person_name":{"surname":"Serra","first_name":"Giuseppe"},"affiliations":[{"department":"Department of Mathematics, Computer Science, and Physics","institution":"University of Udine","laboratory":"Artificial Intelligence Laboratory"}],"email":null},{"person_name":{"surname":"Tasso","first_name":"Carlo"},"affiliations":[{"department":"Department of Mathematics, Computer Science, and Physics","institution":"University of Udine","laboratory":"Artificial Intelligence Laboratory"}],"email":"carlo.tasso@uniud.it"}],"date":null,"ids":{"DOI":"10.1007/978-3-319-73165-0_18","arXiv":null},"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"keywords":[],"citations":{"b0":{"title":"Theano: A Python framework for fast computation of mathematical expressions","authors":[{"person_name":{"surname":"Al-Rfou","first_name":"R"},"affiliations":[],"email":null}],"date":{"year":"2016","month":"05","day":null},"ids":null,"target":"http://arxiv.org/abs/1605.02688","publisher":null,"journal":null,"series":null,"scope":null},"b1":{"title":"Evaluating anaphora and coreference resolution to improve automatic keyphrase extraction","authors":[{"person_name":{"surname":"Basaldella","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Chiaradia","first_name":"G"},"affiliations":[],"email":null},{"person_name":{"surname":"Tasso","first_name":"C"},"affiliations":[],"email":null}],"date":{"year":"2016","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b2":{"title":"Natural Language Processing with Python, 1st edn","authors":[{"person_name":{"surname":"Bird","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Klein","first_name":"E"},"affiliations":[],"email":null},{"person_name":{"surname":"Loper","first_name":"E"},"affiliations":[],"email":null}],"date":{"year":"2009","month":null,"day":null},"ids":null,"target":null,"publisher":"O'Reilly Media Inc","journal":null,"series":null,"scope":null},"b3":{"title":"Topicrank: graph-based topic ranking for keyphrase extraction","authors":[{"person_name":{"surname":"Bougouin","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Boudin","first_name":"F"},"affiliations":[],"email":null},{"person_name":{"surname":"Daille","first_name":"B"},"affiliations":[],"email":null}],"date":{"year":"2013","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b4":{"title":"Natural language processing (almost) from scratch","authors":[{"person_name":{"surname":"Collobert","first_name":"R"},"affiliations":[],"email":null},{"person_name":{"surname":"Weston","first_name":"J"},"affiliations":[],"email":null},{"person_name":{"surname":"Bottou","first_name":"L"},"affiliations":[],"email":null},{"person_name":{"surname":"Karlen","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Kavukcuoglu","first_name":"K"},"affiliations":[],"email":null},{"person_name":{"surname":"Kuksa","first_name":"P"},"affiliations":[],"email":null}],"date":{"year":"2011","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"J. Mach. Learn. Res","series":null,"scope":{"volume":12,"pages":{"from_page":2493,"to_page":2537}}},"b5":{"title":"A new multi-lingual knowledge-base approach to keyphrase extraction for the Italian language","authors":[{"person_name":{"surname":"Degl'innocenti","first_name":"D"},"affiliations":[],"email":null},{"person_name":{"surname":"De Nart","first_name":"D"},"affiliations":[],"email":null},{"person_name":{"surname":"Tasso","first_name":"C"},"affiliations":[],"email":null}],"date":{"year":"2014","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b6":{"title":"Learning precise timing with LSTM recurrent networks","authors":[{"person_name":{"surname":"Gers","first_name":"F"},"affiliations":[],"email":null},{"person_name":{"surname":"Schraudolph","first_name":"N"},"affiliations":[],"email":null},{"person_name":{"surname":"Schmidhuber","first_name":"J"},"affiliations":[],"email":null}],"date":{"year":"2002","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"J. Mach. Learn. Res","series":null,"scope":{"volume":3,"pages":{"from_page":115,"to_page":143}}},"b7":{"title":"Framewise phoneme classification with bidirectional LSTM and other neural network architectures","authors":[{"person_name":{"surname":"Graves","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Schmidhuber","first_name":"J"},"affiliations":[],"email":null}],"date":{"year":"2005","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"Neural Netw","series":null,"scope":{"volume":18,"pages":{"from_page":602,"to_page":610}}},"b8":{"title":"Accurate keyphrase extraction by discriminating overlapping phrases","authors":[{"person_name":{"surname":"Haddoud","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Abdeddaim","first_name":"S"},"affiliations":[],"email":null}],"date":{"year":"2014","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"J. Inf. Sci","series":null,"scope":{"volume":40,"pages":{"from_page":488,"to_page":500}}},"b9":{"title":"CorePhrase: keyphrase extraction for document clustering","authors":[{"person_name":{"surname":"Hammouda","first_name":"K"},"affiliations":[],"email":null},{"person_name":{"surname":"Matute","first_name":"D"},"affiliations":[],"email":null},{"person_name":{"surname":"Kamel","first_name":"M"},"affiliations":[],"email":null}],"date":{"year":"2005","month":null,"day":null},"ids":{"DOI":"10.1007/11510888_26","arXiv":null},"target":"https://doi.org/10.1007/1151088826","publisher":"Springer","journal":null,"series":null,"scope":{"volume":3587,"pages":{"from_page":265,"to_page":274}}},"b10":{"title":"Automatic keyphrase extraction: a survey of the state of the art","authors":[{"person_name":{"surname":"Hasan","first_name":"K"},"affiliations":[],"email":null},{"person_name":{"surname":"Ng","first_name":"V"},"affiliations":[],"email":null}],"date":{"year":"2014","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b11":{"title":"Gradient flow in recurrent nets: the difficulty of learning long-term dependencies","authors":[{"person_name":{"surname":"Hochreiter","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Bengio","first_name":"Y"},"affiliations":[],"email":null},{"person_name":{"surname":"Frasconi","first_name":"P"},"affiliations":[],"email":null},{"person_name":{"surname":"Schmidhuber","first_name":"J"},"affiliations":[],"email":null}],"date":{"year":"2001","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b12":{"title":"Long short-term memory","authors":[{"person_name":{"surname":"Hochreiter","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Schmidhuber","first_name":"J"},"affiliations":[],"email":null}],"date":{"year":"1997","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"Neural Comput","series":null,"scope":{"volume":9,"pages":{"from_page":1735,"to_page":1780}}},"b13":{"title":"Improved automatic keyword extraction given more linguistic knowledge","authors":[{"person_name":{"surname":"Hulth","first_name":"A"},"affiliations":[],"email":null}],"date":{"year":"2003","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b14":{"title":"Phrasier: a system for interactive document retrieval using keyphrases","authors":[{"person_name":{"surname":"Jones","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Staveley","first_name":"M"},"affiliations":[],"email":null}],"date":{"year":"1999","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b15":{"title":"Semeval-2010 task 5: automatic keyphrase extraction from scientific articles","authors":[{"person_name":{"surname":"Kim","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Medelyan","first_name":"O"},"affiliations":[],"email":null},{"person_name":{"surname":"Kan","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Baldwin","first_name":"T"},"affiliations":[],"email":null}],"date":{"year":"2010","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b16":{"title":"Adam: a method for stochastic optimization","authors":[{"person_name":{"surname":"Kingma","first_name":"D"},"affiliations":[],"email":null},{"person_name":{"surname":"Ba","first_name":"J"},"affiliations":[],"email":null}],"date":{"year":"2014","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b17":{"title":"Neural architectures for named entity recognition","authors":[{"person_name":{"surname":"Lample","first_name":"G"},"affiliations":[],"email":null},{"person_name":{"surname":"Ballesteros","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Subramanian","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Kawakami","first_name":"K"},"affiliations":[],"email":null},{"person_name":{"surname":"Dyer","first_name":"C"},"affiliations":[],"email":null}],"date":{"year":"2016","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b18":{"title":"HUMB: automatic key term extraction from scientific articles in GROBID","authors":[{"person_name":{"surname":"Lopez","first_name":"P"},"affiliations":[],"email":null},{"person_name":{"surname":"Romary","first_name":"L"},"affiliations":[],"email":null}],"date":{"year":"2010","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b19":{"title":"Deep keyphrase generation","authors":[{"person_name":{"surname":"Meng","first_name":"R"},"affiliations":[],"email":null},{"person_name":{"surname":"Zhao","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Han","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"He","first_name":"D"},"affiliations":[],"email":null},{"person_name":{"surname":"Brusilovsky","first_name":"P"},"affiliations":[],"email":null},{"person_name":{"surname":"Chi","first_name":"Y"},"affiliations":[],"email":null}],"date":{"year":"2017","month":null,"day":null},"ids":null,"target":"http://aclanthology.coli.uni-saarland.de/pdf/P/P17/P17-1054.pdf","publisher":"Association for Computational Linguistics","journal":null,"series":null,"scope":{"volume":1,"pages":{"from_page":582,"to_page":592}}},"b20":{"title":"Textrank: bringing order into texts","authors":[{"person_name":{"surname":"Mihalcea","first_name":"R"},"affiliations":[],"email":null},{"person_name":{"surname":"Tarau","first_name":"P"},"affiliations":[],"email":null}],"date":{"year":"2004","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b21":{"title":"Distributed representations of words and phrases and their compositionality","authors":[{"person_name":{"surname":"Mikolov","first_name":"T"},"affiliations":[],"email":null},{"person_name":{"surname":"Sutskever","first_name":"I"},"affiliations":[],"email":null},{"person_name":{"surname":"Chen","first_name":"K"},"affiliations":[],"email":null},{"person_name":{"surname":"Corrado","first_name":"G"},"affiliations":[],"email":null},{"person_name":{"surname":"Dean","first_name":"J"},"affiliations":[],"email":null}],"date":{"year":"2013","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":3111,"to_page":3119}}},"b22":{"title":"A contentbased approach to social network analysis: a case study on research communities","authors":[{"person_name":{"surname":"De Nart","first_name":"D"},"affiliations":[],"email":null},{"person_name":{"surname":"Degl'innocenti","first_name":"D"},"affiliations":[],"email":null},{"person_name":{"surname":"Basaldella","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Agosti","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Tasso","first_name":"C"},"affiliations":[],"email":null}],"date":{"year":"2016","month":null,"day":null},"ids":{"DOI":"10.1007/978-3-319-41938-1_15","arXiv":null},"target":"https://doi.org/10.1007/978-3-319-41938-115","publisher":"Springer","journal":null,"series":null,"scope":{"volume":612,"pages":{"from_page":142,"to_page":154}}},"b23":{"title":"Modelling the User Modelling Community (and Other Communities as Well)","authors":[{"person_name":{"surname":"De Nart","first_name":"D"},"affiliations":[],"email":null},{"person_name":{"surname":"Degl'innocenti","first_name":"D"},"affiliations":[],"email":null},{"person_name":{"surname":"Pavan","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Basaldella","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Tasso","first_name":"C"},"affiliations":[],"email":null}],"date":{"year":"2015","month":null,"day":null},"ids":{"DOI":"10.1007/978-3-319-20267-9_31","arXiv":null},"target":"https://doi.org/10.1007/978-3-319-20267-931","publisher":"Springer","journal":null,"series":null,"scope":{"volume":9146,"pages":{"from_page":357,"to_page":363}}},"b24":{"title":"Deep sentence embedding using long short-term memory networks: analysis and application to information retrieval","authors":[{"person_name":{"surname":"Palangi","first_name":"H"},"affiliations":[],"email":null},{"person_name":{"surname":"Deng","first_name":"L"},"affiliations":[],"email":null},{"person_name":{"surname":"Shen","first_name":"Y"},"affiliations":[],"email":null},{"person_name":{"surname":"Gao","first_name":"J"},"affiliations":[],"email":null},{"person_name":{"surname":"He","first_name":"X"},"affiliations":[],"email":null},{"person_name":{"surname":"Chen","first_name":"J"},"affiliations":[],"email":null},{"person_name":{"surname":"Song","first_name":"X"},"affiliations":[],"email":null},{"person_name":{"surname":"Ward","first_name":"R"},"affiliations":[],"email":null}],"date":{"year":"2016","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"IEEE/ACM Trans. Audio Speech Lang. Process","series":null,"scope":{"volume":24,"pages":{"from_page":694,"to_page":707}}},"b25":{"title":"Glove: global vectors for word representation","authors":[{"person_name":{"surname":"Pennington","first_name":"J"},"affiliations":[],"email":null},{"person_name":{"surname":"Socher","first_name":"R"},"affiliations":[],"email":null},{"person_name":{"surname":"Manning","first_name":"C"},"affiliations":[],"email":null}],"date":{"year":"2014","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b26":{"title":"A neural attention model for abstractive sentence summarization","authors":[{"person_name":{"surname":"Rush","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Chopra","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Weston","first_name":"J"},"affiliations":[],"email":null}],"date":{"year":"2015","month":null,"day":null},"ids":{"DOI":null,"arXiv":"arXiv:1509.00685"},"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b27":{"title":"Dropout: a simple way to prevent neural networks from overfitting","authors":[{"person_name":{"surname":"Srivastava","first_name":"N"},"affiliations":[],"email":null},{"person_name":{"surname":"Hinton","first_name":"G"},"affiliations":[],"email":null},{"person_name":{"surname":"Krizhevsky","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Sutskever","first_name":"I"},"affiliations":[],"email":null},{"person_name":{"surname":"Salakhutdinov","first_name":"R"},"affiliations":[],"email":null}],"date":{"year":"2014","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"J. Mach. Learn. Res","series":null,"scope":{"volume":15,"pages":{"from_page":1929,"to_page":1958}}},"b28":{"title":"LSTM-based deep learning models for non-factoid answer selection","authors":[{"person_name":{"surname":"Tan","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Xiang","first_name":"B"},"affiliations":[],"email":null},{"person_name":{"surname":"Zhou","first_name":"B"},"affiliations":[],"email":null}],"date":{"year":"2015","month":null,"day":null},"ids":null,"target":"http://arxiv.org/abs/1511.04108","publisher":null,"journal":null,"series":null,"scope":null},"b29":{"title":"A language model approach to keyphrase extraction","authors":[{"person_name":{"surname":"Tomokiyo","first_name":"T"},"affiliations":[],"email":null},{"person_name":{"surname":"Hurst","first_name":"M"},"affiliations":[],"email":null}],"date":{"year":"2003","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b30":{"title":"Learning algorithms for keyphrase extraction","authors":[{"person_name":{"surname":"Turney","first_name":"P"},"affiliations":[],"email":null}],"date":{"year":"2000","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"Inf. Retriev","series":null,"scope":{"volume":2,"pages":{"from_page":303,"to_page":336}}},"b31":{"title":"KEA: practical automatic keyphrase extraction","authors":[{"person_name":{"surname":"Witten","first_name":"I"},"affiliations":[],"email":null},{"person_name":{"surname":"Paynter","first_name":"G"},"affiliations":[],"email":null},{"person_name":{"surname":"Frank","first_name":"E"},"affiliations":[],"email":null},{"person_name":{"surname":"Gutwin","first_name":"C"},"affiliations":[],"email":null},{"person_name":{"surname":"Nevill-Manning","first_name":"C"},"affiliations":[],"email":null}],"date":{"year":"1999","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":254,"to_page":255}}},"b32":{"title":"Keyphrase extraction using deep recurrent neural networks on Twitter","authors":[{"person_name":{"surname":"Zhang","first_name":"Q"},"affiliations":[],"email":null},{"person_name":{"surname":"Wang","first_name":"Y"},"affiliations":[],"email":null},{"person_name":{"surname":"Gong","first_name":"Y"},"affiliations":[],"email":null},{"person_name":{"surname":"Huang","first_name":"X"},"affiliations":[],"email":null}],"date":{"year":"2016","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b33":{"title":"World Wide Web site summarization","authors":[{"person_name":{"surname":"Zhang","first_name":"Y"},"affiliations":[],"email":null},{"person_name":{"surname":"Zincir-Heywood","first_name":"N"},"affiliations":[],"email":null},{"person_name":{"surname":"Milios","first_name":"E"},"affiliations":[],"email":null}],"date":{"year":"2004","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"Web Intell. Agent Syst","series":null,"scope":{"volume":2,"pages":{"from_page":39,"to_page":53}}}},"sections":[{"title":"Introduction","paragraphs":[{"text":"Keyphrases (herein KPs) are phrases that \"capture the main topic discussed on a given document\" [31]. More specifically, KPs are phrases typically one to five words long that appear verbatim in a document, and can be used to briefly summarize its content.","refs":[{"start":96,"end":100,"marker":"bibr","target":"#b30"}]},{"text":"The task of finding such KPs is called Automatic Keyphrase Extraction (herein AKE). It has received a lot of attention in the last two decades [11] and recently it has been successfully used in many Natural Language Processing (hence NLP) tasks, such as text summarization [34], document clustering [10], or non-NLP tasks such as social network analysis [23] or user modeling [24]. Automatic Keyphrase Extraction approaches have been also applied in Information Retrieval of relevant documents in digital document archives which can contain heterogeneous types of items, such as books articles, papers etc. [15].","refs":[{"start":143,"end":147,"marker":"bibr","target":"#b10"},{"start":273,"end":277,"marker":"bibr","target":"#b33"},{"start":299,"end":303,"marker":"bibr","target":"#b9"},{"start":354,"end":358,"marker":"bibr","target":"#b22"},{"start":376,"end":380,"marker":"bibr","target":"#b23"},{"start":607,"end":611,"marker":"bibr","target":"#b14"}]},{"text":"The first approaches to solve Automatic Keyphrase Extraction were based on supervised machine learning (herein ML) algorithms, like Naive Bayes [32] or C4.5 decision trees [31]. Since then, several researchers explored different ML techniques such as Multilayer Perceptrons [2,19], Support Vector Machines [19], Logistic Regression [2,9], and Bagging [14]. Since no algorithm stands out as the \"best\" ML algorithm, often authors test many techniques in a single experiment, and then they choose as best ML algorithm the best performing one [2,9] and/or even the least computationally expensive one [19].","refs":[{"start":144,"end":148,"marker":"bibr","target":"#b31"},{"start":172,"end":176,"marker":"bibr","target":"#b30"},{"start":274,"end":277,"marker":"bibr","target":"#b1"},{"start":277,"end":280,"marker":"bibr","target":"#b18"},{"start":306,"end":310,"marker":"bibr","target":"#b18"},{"start":332,"end":335,"marker":"bibr","target":"#b1"},{"start":335,"end":337,"marker":"bibr","target":"#b8"},{"start":351,"end":355,"marker":"bibr","target":"#b13"},{"start":540,"end":543,"marker":"bibr","target":"#b1"},{"start":543,"end":545,"marker":"bibr","target":"#b8"},{"start":598,"end":602,"marker":"bibr","target":"#b18"}]},{"text":"However, AKE algorithms based on unsupervised approaches have been developed over the years as well. For example, Tomokiyo et al. [30] proposed to use a language model approach to extract KPs, and Mihalcea et al. [21] presented a graph-based ranking algorithm to find keyphrases. Nevertheless, supervised approaches have been the best performing ones in challenges: for example, [19], a supervised approach, was the best performing algorithm in the SEMEVAL 2010 Keyphrase Extraction Task [16].","refs":[{"start":130,"end":134,"marker":"bibr","target":"#b29"},{"start":213,"end":217,"marker":"bibr","target":"#b20"},{"start":379,"end":383,"marker":"bibr","target":"#b18"},{"start":488,"end":492,"marker":"bibr","target":"#b15"}]},{"text":"In the last years, most attention is devoted to the features used in these supervised algorithms. The numbers of features used can range from just two [32] to more than 20 [9]. These features can be divided in categories identified with different kinds of knowledge they encode into the model:","refs":[{"start":151,"end":155,"marker":"bibr","target":"#b31"},{"start":172,"end":175,"marker":"bibr","target":"#b8"}]},{"text":"statistical knowledge: number of appearances of the KP in the document, TF-IDF, number of sentences containing the KP, etc.; -positional knowledge: position of the first occurrence of the KP in the document, position of the last occurrence, appearance in the title, appearance in specific sections (abstract, conclusions), etc.; -linguistic knowledge: part-of-speech tags of the KP [14], anaphoras pointing to the KP [2], etc.; -external knowledge: presence of the KP as a page on Wikipedia [6] or in specialized domain ontologies [19], etc.","refs":[{"start":382,"end":386,"marker":"bibr","target":"#b13"},{"start":417,"end":420,"marker":"bibr","target":"#b1"},{"start":491,"end":494,"marker":"bibr","target":"#b5"},{"start":531,"end":535,"marker":"bibr","target":"#b18"}]},{"text":"However, given the wide variety of lexical, linguistic and semantic aspects that can contribute to define a keyphrase, it difficult to design hand-crafted feature, and even the best performing algorithms hardly reach F1-Scores of 50% on the most common evaluation sets [14,16]. For this reason, AKE is still far from being a solved problem in the NLP community.","refs":[{"start":269,"end":273,"marker":"bibr","target":"#b13"},{"start":273,"end":276,"marker":"bibr","target":"#b15"}]},{"text":"In recent years, Deep Learning techniques have shown impressive results in many Natural Language Processing tasks, e.g., Named Entity Recognition, Automatic Summarization, Question Answering, and so on [18,25,27,29]. In Named Entity Recognition, for example, researchers have proposed several Neural Network Architectures","refs":[{"start":202,"end":206,"marker":"bibr","target":"#b17"},{"start":206,"end":209,"marker":"bibr","target":"#b24"},{"start":209,"end":212,"marker":"bibr","target":"#b26"},{"start":212,"end":215,"marker":"bibr","target":"#b28"}]},{"text":"To best of our knowledge, only recently some first attempts to address AKE task with Deep Learning techniques, has been presented [20,33]. In [33], the authors present an approach based on Recurrent Neural Networks, specifically designed for a particular domain, i.e., Twitter data. On the other hand, in [20] the authors use more datasets to evaluate their RNN for keyphrase extraction, and they propose a study of the keyphrases generated by their network as well.","refs":[{"start":130,"end":134,"marker":"bibr","target":"#b19"},{"start":134,"end":137,"marker":"bibr","target":"#b32"},{"start":142,"end":146,"marker":"bibr","target":"#b32"},{"start":305,"end":309,"marker":"bibr","target":"#b19"}]},{"text":"In this paper, we present a Deep Learning architecture for AKE. In particular, we investigate an approach based on based on Bidirectional Long Short-Term Memory RNN (hence Bi-LSTM), which is able to exploit previous and future context of a given word. Our system, since it does not require specific features carefully optimized for a specific domain, can be applied to a wide range of scenarios. To evaluate the proposed method, we conduct experiments on the well-known INSPEC dataset [14]. The experimental result showed the proposed solution performs significantly better than competitive methods.","refs":[{"start":485,"end":489,"marker":"bibr","target":"#b13"}]}]},{"title":"Proposed Approach","paragraphs":[{"text":"To extract KPs we implemented the following steps, as presented in Fig. 1. First, we split the document into sentences, and then we tokenize the sentences in words using NLTK [3]. Then, we associate a word embedding representation that maps each input word into a continuous vector representation. Finally, we feed our word embeddings into a Bi-LSTM units, which it can effectively deal with the variable lengths of sentences and it is able to analyze word features and their context (for example, distant relation between words). The Bi-LSTM is connected to a fully connected hidden layer, which in turn is connected to a softmax output layer with three neurons for each word. Between the Bi-LSTM layer and the hidden layer, and between the hidden layer and the output layer, we use dropout [28] to prevent overfitting.","refs":[{"start":72,"end":73,"marker":"figure","target":"#fig_0"},{"start":175,"end":178,"marker":"bibr","target":"#b2"},{"start":792,"end":796,"marker":"bibr","target":"#b27"}]},{"text":"As in the techniques used for Named Entity Recognition, the three neurons are mapped to three possible output classes: NO KP, BEGIN KP, INSIDE KP, which respectively mark tokens that are not keyphrases, the first token of a keyphrase, and the other tokens of a keyphrase.","refs":[]},{"text":"For example, if our input sentence is \"We train a neural network using Keras\", and the keyphrases in that sentence are \"neural network \" and \"Keras\", the tokens' classes will be We/NO KP train/NO KP a/NO KP neural/BEGIN KP network/INSIDE KP using/NO KP Keras/BEGIN KP'.","refs":[]}]},{"title":"Word Embeddings","paragraphs":[{"text":"The input layer of our model is a vector representation of the individual words contained in input document. Several recent studies [5,22] showed that such representations, called word embeddings, are able to represent the semantics of words better than an \"one hot\" encoding word representation, when trained on large corpus. However, the datasets for AKE are relatively small, therefore it is difficult to train word embeddings to capture the word semantics. Hence, we adopt Stanford's GloVe Embeddings, which are trained on 6 billion words extracted from Wikipedia and Web texts [26].","refs":[{"start":132,"end":135,"marker":"bibr","target":"#b4"},{"start":135,"end":138,"marker":"bibr","target":"#b21"},{"start":582,"end":586,"marker":"bibr","target":"#b25"}]}]},{"title":"Model Architecture","paragraphs":[{"text":"Let {x 1 , . . . , x n } the word embeddings representing the input tokens, a Recurrent Neural Network (hence RNN) computes the output vector y t of each token x t by iterating the following equations from t = 1 to n:","refs":[]},{"text":"where h t is the hidden vector sequence, W denotes weight matrices (for example W xh is the matrix of the weights connecting the input layer and the hidden layer), b denotes bias vectors, and H is activation function of the hidden layer. Equation 1 represents the connection between the previous and the current hidden states, thus RNNs can make use of previous context. In practice however, the RNN is not able to use effectively the all input history due to the vanishing gradient problem [12]. Hence, a better solution to exploit long range context is the Long Short-Term Memory (LSTM) architecture [13]. The LSTM is conceptually defined like an RNN, but hidden layer updates are replaced by specific units called memory cells. Specifically, a LSTM is implemented by the following functions [7]:","refs":[{"start":491,"end":495,"marker":"bibr","target":"#b11"},{"start":602,"end":606,"marker":"bibr","target":"#b12"},{"start":794,"end":797,"marker":"bibr","target":"#b6"}]},{"text":"where σ is the logistic sigmoid function, i, f , o, and c are the input gate, forget gate, output gate and cell activation vectors, and all b are learned biases. Another shortcoming of RNNs is that they consider only previous context, but in AKE we want to exploit future context as well. For example, consider the phrase \"John Doe is a lawyer; he likes fast cars\". When we first encounter \"John Doe\" in the phrase, we still don't know whether he's going to be an important entity; then, we find the word \"lawyer \" and the pronoun \"he\", which clearly refer to him, stressing his importance in the context. \"Lawyer \" and \"he\" are called anaphoras and the technique to find this contextual information is called anaphora resolution, which has been exploited to perform keyphrase extraction in [2].","refs":[{"start":791,"end":794,"marker":"bibr","target":"#b1"}]},{"text":"In order to use future context, in our approach we adopt a Bidirectional LSTM network [8]. In fact, with this architecture we are able to make use of both past context and future context of a specific word. It consists of two separate hidden layers; it first computes the forward hidden sequence -→ h t ; then, it computes the backward hidden sequence ←h t ; finally, it combines -→ h t and ←h t to generate the output y t . Let the hidden states h be LSTM blocks, a Bi-LSTM is implemented by the following functions:","refs":[{"start":86,"end":89,"marker":"bibr","target":"#b7"}]}]},{"title":"Experimental Results","paragraphs":[{"text":"We present experiments on a well-known keyphrase extraction dataset: the INSPEC dataset [14]. It is composed by 2000 abstract papers in English extracted from journal papers from the disciplines Computer and Control, Information Technology. It consists of 1000 documents for training, 500 for validation and the remaining 500 for testing. We choose this dataset since it's well known in the AKE community, so there are many other available results to compare with; moreover, is much bigger than the dataset used in the SEMEVAL 2010 [16] competition, which contains only 144 documents for training, 40 for validation, and 100 for testing.","refs":[{"start":88,"end":92,"marker":"bibr","target":"#b13"},{"start":532,"end":536,"marker":"bibr","target":"#b15"}]},{"text":"In order to implement our approach, we used Keras with Theano [1] as back end, which in turn allowed us to use CUDA to train our networks using a GPU. Experiments are run on a GeForce GTX Titan X Pascal GPU. The network is trained to minimize the Crossentropy loss. We train our network using the Root Mean Square Propagation optimization algorithm [17] and batch size 32. After trying different configurations for the network, we obtained the best results with a size of 150 neurons for the Bi-LSTM layer, 150 neurons for the hidden dense layer, and a value of 0.25 for the dropout layers in between.","refs":[{"start":62,"end":65,"marker":"bibr","target":"#b0"},{"start":349,"end":353,"marker":"bibr","target":"#b16"}]},{"text":"To test the impact of word embeddings, we perform experiments with the pre-trained Stanford's GloVe Embeddings using all the word embedding sizes available, i.e., 50, 100, 200 and 300. The training of the network takes about 30 s to perform a full epoch with all the GloVe Embeddings. To stop the training, we used Keras' own embedded early stopping rule, which halts training when the training loss does not decrease for two consecutive epochs. The number of epochs requested to converge in all the four settings is displayed in Table 1, along with precision, recall and F1-score obtained by our system when trained using different sizes of the word embeddings. We can note that the best results are obtained with embedding size of 100; however, the embedding sizes of 200 and 300 obtain a very close result in term of F1-Score. The scores seem to show an interesting pattern: in fact, looking at the results, we see that the precision increases with embedding size, while recall decreases from size 100 onwards.","refs":[{"start":536,"end":537,"marker":"table","target":"#tab_0"}]},{"text":"Table 2 compares the performances in term of precision, recall, and F-score our approach with other competitive systems, based both on supervised and unsupervised machine learning techniques. The first three systems are the ones presented in [14], with three different candidate keyphrase generation techniques: n-grams, Noun Phrase (NP) chunking, and patterns. The fourth system is Top-icRank [4], a graph-based keyphrase extraction method that relies on a topical representation of the document. Our proposed solution achieves best performance in term of F1-score and Recall. Although TopicRank obtains best performance in precision, its recall results are significantly worse than the ones obtained by us; moreover, we have to stress that we're able to obtain better precision when using an embedding size of 200 and 300, albeit with a slightly lower overall F1-Score. Finally, it's worth noting that we perform better than the results presented in [20], which is to the best of our knowledge the only one DL AKE algorithm evaluated on the INSPEC dataset. In fact, we obtain a F1@10 score of 0.422, while the best F1@10 score obtained by [20] is 0.342.","refs":[{"start":6,"end":7,"marker":"table","target":"#tab_1"},{"start":242,"end":246,"marker":"bibr","target":"#b13"},{"start":394,"end":397,"marker":"bibr","target":"#b3"},{"start":952,"end":956,"marker":"bibr","target":"#b19"},{"start":1141,"end":1145,"marker":"bibr","target":"#b19"}]}]},{"title":"Conclusion","paragraphs":[{"text":"In this work, we proposed a Deep Long-Short Term Memory Neural Network model to perform automatic keyphrase extraction, evaluating the proposed method on the INSPEC dataset. Since word representation is a crucial step for success, we perform experiments with different pre-trained word representations. We show that without requiring hand-crafted features, the proposed approach is highly effective and achieves better results with respect to other competitive methods. For the future, we plan to test additional network architectures and to evaluate our algorithms on more datasets, in order to demonstrate its robustness.","refs":[]}]}],"tables":{"tab_0":{"heading":"Table 1 .","description":"Performance with different vector sizes of the GloVe Word Embeddings: 50, 100, 200 and 300 (we called them GloVe-(SIZE), respectively).","rows":[["Embedding Size Precision Recall F1-score Epochs"],["GloVe-50","50 0.331","0.518 0.404","20"],["GloVe-100 100 0.340","0.578 0.428","14"],["GloVe-200 200 0.352","0.539 0.426","18"],["GloVe-300 300 0.364","0.500 0.421","8"]]},"tab_1":{"heading":"Table 2 .","description":"Comparison results on INSPEC dataset","rows":[["Method","Precision Recall F1-score"],["Proposed approach","0.340","0.578 0.428"],["n-grams with tag [14]","0.252","0.517 0.339"],["NP chunking with tag [14] 0.297","0.372 0.330"],["Pattern with tag [14]","0.217","0.399 0.281"],["TopicRank [4]","0.348","0.404 0.352"]]}},"abstract":{"title":"Abstract","paragraphs":[{"text":"To achieve state-of-the-art performance, keyphrase extraction systems rely on domain-specific knowledge and sophisticated features. In this paper, we propose a neural network architecture based on a Bidirectional Long Short-Term Memory Recurrent Neural Network that is able to detect the main topics on the input documents without the need of defining new hand-crafted features. A preliminary experimental evaluation on the well-known INSPEC dataset confirms the effectiveness of the proposed solution.","refs":[]}]}}