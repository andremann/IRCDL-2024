{"bibliography":{"title":"Automatic Image Cropping and Selection Using Saliency: An Application to Historical Manuscripts","authors":[{"person_name":{"surname":"Cornia","first_name":"Marcella"},"affiliations":[{"department":null,"institution":"University of Modena and Reggio Emilia","laboratory":null}],"email":"marcella.cornia@unimore.it"},{"person_name":{"surname":"Pini","first_name":"Stefano"},"affiliations":[{"department":null,"institution":"University of Modena and Reggio Emilia","laboratory":null}],"email":"stefano.pini@unimore.it"},{"person_name":{"surname":"Baraldi","first_name":"Lorenzo"},"affiliations":[{"department":null,"institution":"University of Modena and Reggio Emilia","laboratory":null}],"email":"lorenzo.baraldi@unimore.it"},{"person_name":{"surname":"Cucchiara","first_name":"Rita"},"affiliations":[{"department":null,"institution":"University of Modena and Reggio Emilia","laboratory":null}],"email":"rita.cucchiara@unimore.it"}],"date":null,"ids":{"DOI":"10.1007/978-3-319-73165-0_17","arXiv":null},"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"keywords":["Image selection","Image cropping","Saliency digital libraries"],"citations":{"b0":{"title":"Seam carving for content-aware image resizing","authors":[{"person_name":{"surname":"Avidan","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Shamir","first_name":"A"},"affiliations":[],"email":null}],"date":{"year":"2007","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"ACM Trans. Graph","series":null,"scope":{"volume":26,"pages":{"from_page":10,"to_page":10}}},"b1":{"title":"Affective classification of gaming activities coming from RPG gaming sessions","authors":[{"person_name":{"surname":"Balducci","first_name":"F"},"affiliations":[],"email":null},{"person_name":{"surname":"Grana","first_name":"C"},"affiliations":[],"email":null}],"date":{"year":"2017","month":null,"day":null},"ids":{"DOI":"10.1007/978-3-319-65849-0_11","arXiv":null},"target":"https://doi.org/10.1007/978-3-319-65849-0","publisher":"Springer","journal":null,"series":null,"scope":{"volume":10345,"pages":{"from_page":93,"to_page":100}}},"b2":{"title":"A framework for photo-quality assessment and enhancement based on visual aesthetics","authors":[{"person_name":{"surname":"Bhattacharya","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Sukthankar","first_name":"R"},"affiliations":[],"email":null},{"person_name":{"surname":"Shah","first_name":"M"},"affiliations":[],"email":null}],"date":{"year":"2010","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b3":{"title":"Indexing of historical document images: ad hoc dewarping technique for handwritten text","authors":[{"person_name":{"surname":"Bolelli","first_name":"F"},"affiliations":[],"email":null}],"date":{"year":"2017","month":null,"day":null},"ids":{"DOI":"10.1007/978-3-319-68130-6_4","arXiv":null},"target":"https://doi.org/10.1007/978-3-319-68130-6","publisher":"Springer","journal":null,"series":null,"scope":{"volume":733,"pages":{"from_page":45,"to_page":55}}},"b4":{"title":"Automatic image cropping: a computational complexity study","authors":[{"person_name":{"surname":"Chen","first_name":"J"},"affiliations":[],"email":null},{"person_name":{"surname":"Bai","first_name":"G"},"affiliations":[],"email":null},{"person_name":{"surname":"Liang","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Li","first_name":"Z"},"affiliations":[],"email":null}],"date":{"year":"2016","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b5":{"title":"Quantitative analysis of automatic image cropping algorithms: a dataset and comparative study","authors":[{"person_name":{"surname":"Chen","first_name":"Y"},"affiliations":[],"email":null},{"person_name":{"surname":"Huang","first_name":"T"},"affiliations":[],"email":null},{"person_name":{"surname":"Chang","first_name":"K"},"affiliations":[],"email":null},{"person_name":{"surname":"Tsai","first_name":"Y"},"affiliations":[],"email":null},{"person_name":{"surname":"Chen","first_name":"H"},"affiliations":[],"email":null},{"person_name":{"surname":"Chen","first_name":"B"},"affiliations":[],"email":null}],"date":{"year":"2017","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b6":{"title":"Learning to compose with professional photographs on the web","authors":[{"person_name":{"surname":"Chen","first_name":"Y"},"affiliations":[],"email":null},{"person_name":{"surname":"Klopp","first_name":"J"},"affiliations":[],"email":null},{"person_name":{"surname":"Sun","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Chien","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Ma","first_name":"K"},"affiliations":[],"email":null}],"date":{"year":"2017","month":null,"day":null},"ids":{"DOI":null,"arXiv":"arXiv:1702.00503"},"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b7":{"title":"Learning to photograph","authors":[{"person_name":{"surname":"Cheng","first_name":"B"},"affiliations":[],"email":null},{"person_name":{"surname":"Ni","first_name":"B"},"affiliations":[],"email":null},{"person_name":{"surname":"Yan","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Tian","first_name":"Q"},"affiliations":[],"email":null}],"date":{"year":"2010","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b8":{"title":"Self-adaptive image cropping for small displays","authors":[{"person_name":{"surname":"Ciocca","first_name":"G"},"affiliations":[],"email":null},{"person_name":{"surname":"Cusano","first_name":"C"},"affiliations":[],"email":null},{"person_name":{"surname":"Gasparini","first_name":"F"},"affiliations":[],"email":null},{"person_name":{"surname":"Schettini","first_name":"R"},"affiliations":[],"email":null}],"date":{"year":"2007","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"IEEE Trans. Consum. Electron","series":null,"scope":{"volume":53,"pages":{"from_page":1622,"to_page":1627}}},"b9":{"title":"A deep multi-level network for saliency prediction","authors":[{"person_name":{"surname":"Cornia","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Baraldi","first_name":"L"},"affiliations":[],"email":null},{"person_name":{"surname":"Serra","first_name":"G"},"affiliations":[],"email":null},{"person_name":{"surname":"Cucchiara","first_name":"R"},"affiliations":[],"email":null}],"date":{"year":"2016","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b10":{"title":"Multi-level net: a visual saliency prediction model","authors":[{"person_name":{"surname":"Cornia","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Baraldi","first_name":"L"},"affiliations":[],"email":null},{"person_name":{"surname":"Serra","first_name":"G"},"affiliations":[],"email":null},{"person_name":{"surname":"Cucchiara","first_name":"R"},"affiliations":[],"email":null}],"date":{"year":"2016","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b11":{"title":"Predicting human eye fixations via an LSTM-based saliency attentive model","authors":[{"person_name":{"surname":"Cornia","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Baraldi","first_name":"L"},"affiliations":[],"email":null},{"person_name":{"surname":"Serra","first_name":"G"},"affiliations":[],"email":null},{"person_name":{"surname":"Cucchiara","first_name":"R"},"affiliations":[],"email":null}],"date":{"year":"2017","month":null,"day":null},"ids":{"DOI":null,"arXiv":"arXiv:1611.09571"},"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b12":{"title":"Semantic transcoding for live video server","authors":[{"person_name":{"surname":"Cucchiara","first_name":"R"},"affiliations":[],"email":null},{"person_name":{"surname":"Grana","first_name":"C"},"affiliations":[],"email":null},{"person_name":{"surname":"Prati","first_name":"A"},"affiliations":[],"email":null}],"date":{"year":"2002","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b13":{"title":"To learn representativeness of video frames","authors":[{"person_name":{"surname":"Kang","first_name":"H"},"affiliations":[],"email":null},{"person_name":{"surname":"Hua","first_name":"X"},"affiliations":[],"email":null}],"date":{"year":"2005","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b14":{"title":"The design of high-level features for photo quality assessment","authors":[{"person_name":{"surname":"Ke","first_name":"Y"},"affiliations":[],"email":null},{"person_name":{"surname":"Tang","first_name":"X"},"affiliations":[],"email":null},{"person_name":{"surname":"Jing","first_name":"F"},"affiliations":[],"email":null}],"date":{"year":"2006","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b15":{"title":"Imagenet classification with deep convolutional neural networks","authors":[{"person_name":{"surname":"Krizhevsky","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Sutskever","first_name":"I"},"affiliations":[],"email":null},{"person_name":{"surname":"Hinton","first_name":"G"},"affiliations":[],"email":null}],"date":{"year":"2012","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":1097,"to_page":1105}}},"b16":{"title":"A2-RL: aesthetics aware reinforcement learning for automatic image cropping","authors":[{"person_name":{"surname":"Li","first_name":"D"},"affiliations":[],"email":null},{"person_name":{"surname":"Wu","first_name":"H"},"affiliations":[],"email":null},{"person_name":{"surname":"Zhang","first_name":"J"},"affiliations":[],"email":null},{"person_name":{"surname":"Huang","first_name":"K"},"affiliations":[],"email":null}],"date":{"year":"2017","month":null,"day":null},"ids":{"DOI":null,"arXiv":"arXiv:1709.04595"},"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b17":{"title":"Query sensitive dynamic web video thumbnail generation","authors":[{"person_name":{"surname":"Liu","first_name":"C"},"affiliations":[],"email":null},{"person_name":{"surname":"Huang","first_name":"Q"},"affiliations":[],"email":null},{"person_name":{"surname":"Jiang","first_name":"S"},"affiliations":[],"email":null}],"date":{"year":"2011","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b18":{"title":"Multi-task deep visual-semantic embedding for video thumbnail selection","authors":[{"person_name":{"surname":"Liu","first_name":"W"},"affiliations":[],"email":null},{"person_name":{"surname":"Mei","first_name":"T"},"affiliations":[],"email":null},{"person_name":{"surname":"Zhang","first_name":"Y"},"affiliations":[],"email":null},{"person_name":{"surname":"Che","first_name":"C"},"affiliations":[],"email":null},{"person_name":{"surname":"Luo","first_name":"J"},"affiliations":[],"email":null}],"date":{"year":"2015","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b19":{"title":"Towards extracting semantically meaningful key frames from personal video clips: from humans to computers","authors":[{"person_name":{"surname":"Luo","first_name":"J"},"affiliations":[],"email":null},{"person_name":{"surname":"Papin","first_name":"C"},"affiliations":[],"email":null},{"person_name":{"surname":"Costello","first_name":"K"},"affiliations":[],"email":null}],"date":{"year":"2009","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"IEEE Trans. Circ. Syst. Video Technol","series":null,"scope":{"volume":19,"pages":{"from_page":289,"to_page":301}}},"b20":{"title":"Automatic image cropping for mobile device with built-in camera","authors":[{"person_name":{"surname":"Ma","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Guo","first_name":"J"},"affiliations":[],"email":null}],"date":{"year":"2004","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b21":{"title":"Sensation-based photo cropping","authors":[{"person_name":{"surname":"Nishiyama","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Okabe","first_name":"T"},"affiliations":[],"email":null},{"person_name":{"surname":"Sato","first_name":"Y"},"affiliations":[],"email":null},{"person_name":{"surname":"Sato","first_name":"I"},"affiliations":[],"email":null}],"date":{"year":"2009","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b22":{"title":"Modeling photo composition and its application to photo re-arrangement","authors":[{"person_name":{"surname":"Park","first_name":"J"},"affiliations":[],"email":null},{"person_name":{"surname":"Lee","first_name":"J"},"affiliations":[],"email":null},{"person_name":{"surname":"Tai","first_name":"Y"},"affiliations":[],"email":null},{"person_name":{"surname":"Kweon","first_name":"I"},"affiliations":[],"email":null}],"date":{"year":"2012","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b23":{"title":"Gaze-based interaction for semi-automatic photo cropping","authors":[{"person_name":{"surname":"Santella","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Agrawala","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Decarlo","first_name":"D"},"affiliations":[],"email":null},{"person_name":{"surname":"Salesin","first_name":"D"},"affiliations":[],"email":null},{"person_name":{"surname":"Cohen","first_name":"M"},"affiliations":[],"email":null}],"date":{"year":"2006","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b24":{"title":"Very deep convolutional networks for large-scale image recognition","authors":[{"person_name":{"surname":"Simonyan","first_name":"K"},"affiliations":[],"email":null},{"person_name":{"surname":"Zisserman","first_name":"A"},"affiliations":[],"email":null}],"date":{"year":"2014","month":null,"day":null},"ids":{"DOI":null,"arXiv":"arXiv:1409.1556"},"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b25":{"title":"Attention based auto image cropping","authors":[{"person_name":{"surname":"Stentiford","first_name":"F"},"affiliations":[],"email":null}],"date":{"year":"2007","month":null,"day":null},"ids":null,"target":null,"publisher":"ICVS","journal":null,"series":null,"scope":null},"b26":{"title":"Automatic thumbnail cropping and its effectiveness","authors":[{"person_name":{"surname":"Suh","first_name":"B"},"affiliations":[],"email":null},{"person_name":{"surname":"Ling","first_name":"H"},"affiliations":[],"email":null},{"person_name":{"surname":"Bederson","first_name":"B"},"affiliations":[],"email":null},{"person_name":{"surname":"Jacobs","first_name":"D"},"affiliations":[],"email":null}],"date":{"year":"2003","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b27":{"title":"Content-based photo quality assessment","authors":[{"person_name":{"surname":"Tang","first_name":"X"},"affiliations":[],"email":null},{"person_name":{"surname":"Luo","first_name":"W"},"affiliations":[],"email":null},{"person_name":{"surname":"Wang","first_name":"X"},"affiliations":[],"email":null}],"date":{"year":"2013","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"IEEE Trans. Multimed","series":null,"scope":{"volume":15,"pages":{"from_page":1930,"to_page":1943}}},"b28":{"title":"Event driven web video summarization by tag localization and key-shot identification","authors":[{"person_name":{"surname":"Wang","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Hong","first_name":"R"},"affiliations":[],"email":null},{"person_name":{"surname":"Li","first_name":"G"},"affiliations":[],"email":null},{"person_name":{"surname":"Zha","first_name":"Z"},"affiliations":[],"email":null},{"person_name":{"surname":"Yan","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Chua","first_name":"T"},"affiliations":[],"email":null}],"date":{"year":"2012","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"IEEE Trans. Multimed","series":null,"scope":{"volume":14,"pages":{"from_page":975,"to_page":985}}},"b29":{"title":"Learning the change for automatic image cropping","authors":[{"person_name":{"surname":"Yan","first_name":"J"},"affiliations":[],"email":null},{"person_name":{"surname":"Lin","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Bing Kang","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Tang","first_name":"X"},"affiliations":[],"email":null}],"date":{"year":"2013","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b30":{"title":"Probabilistic graphlet transfer for photo cropping","authors":[{"person_name":{"surname":"Zhang","first_name":"L"},"affiliations":[],"email":null},{"person_name":{"surname":"Song","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Zhao","first_name":"Q"},"affiliations":[],"email":null},{"person_name":{"surname":"Liu","first_name":"X"},"affiliations":[],"email":null},{"person_name":{"surname":"Bu","first_name":"J"},"affiliations":[],"email":null},{"person_name":{"surname":"Chen","first_name":"C"},"affiliations":[],"email":null}],"date":{"year":"2013","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"IEEE Trans. Image Process","series":null,"scope":{"volume":22,"pages":{"from_page":802,"to_page":815}}},"b31":{"title":"Auto cropping for digital photographs","authors":[{"person_name":{"surname":"Zhang","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Zhang","first_name":"L"},"affiliations":[],"email":null},{"person_name":{"surname":"Sun","first_name":"Y"},"affiliations":[],"email":null},{"person_name":{"surname":"Feng","first_name":"L"},"affiliations":[],"email":null},{"person_name":{"surname":"Ma","first_name":"W"},"affiliations":[],"email":null}],"date":{"year":"2005","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"ICME","series":null,"scope":null}},"sections":[{"title":"Introduction","paragraphs":[{"text":"Image cropping aims at extracting rectangular subregions of a given image with the aim of preserving most of its visual content and enhancing the visual quality of the cropped image [5,6,30]. A good image cropping algorithm can have several applications, from helping professional editors in the advertisement and publishing industry, to increasing the presentation quality in search engines and social networks, where it is often the case that variable sized images need to be previewed with thumbnails of given size. In the case of collections of images, the combination of frame selection and image cropping techniques can be exploited to generate high quality thumbnails representing the entire collection. The same line of thinking can be extended, of course, to the case of selecting appropriate thumbnail for a video.","refs":[{"start":182,"end":185,"marker":"bibr","target":null},{"start":185,"end":187,"marker":"bibr","target":"#b5"},{"start":187,"end":190,"marker":"bibr","target":"#b29"}]},{"text":"Multimedia digital libraries, which contain collections of images and videos [2,4,13], are for sure a valuable application domain of image cropping and selection techniques. Motivated by these considerations, in this paper we devise a cropping technique based on saliency prediction. In fact, visual saliency prediction is the task of predicting the most important regions of an image by identifying those regions which most likely attract human gazes at the first glance [10][11][12]. By relying on this information, we propose a simple and effective image cropping solution which returns cropped regions with the most important visual content of their corresponding original images. To validate the effectiveness of the proposed cropping technique, we assess its performance on standard image cropping datasets by comparing to state of the art methods.","refs":[{"start":77,"end":80,"marker":"bibr","target":"#b1"},{"start":80,"end":82,"marker":"bibr","target":"#b3"},{"start":82,"end":85,"marker":"bibr","target":"#b12"},{"start":472,"end":476,"marker":"bibr","target":"#b9"},{"start":476,"end":480,"marker":"bibr","target":"#b10"},{"start":480,"end":484,"marker":"bibr","target":"#b11"}]},{"text":"Moreover, we propose an image selection method which exploits the ability of our cropping solution of finding the most important regions of images. In particular, to validate our solution in real-world scenarios, we apply it to the selection of the most representative pages of historical manuscripts. In this way, the selected pages can be used as an effective preview of each manuscript thus improving the navigation of historical digital libraries.","refs":[]},{"text":"Overall, the paper is organized as follows: Sect. 2 presents the main related image cropping methods and briefly reviews the thumbnail selection literature, Sect. 3 introduces the proposed saliency-based cropping technique, while the corresponding experimental results are reported in Sect. 4. Finally, the automatic page selection of historical manuscripts is presented in Sect. 5.","refs":[]}]},{"title":"Related Work","paragraphs":[{"text":"In this section, we start from reviewing the literature related to the automatic image cropping task. Also, we briefly describe some recent works addressing the thumbnail selection problem.","refs":[]}]},{"title":"Image Cropping","paragraphs":[{"text":"Existing image cropping methods can be categorized into two main categories: attention-based and aesthetics-based methods. The first ones aim at finding the most visually salient regions in the original images, while the second ones accomplish the cropping task mainly by analyzing the attractiveness of the cropped image with the help of a quality classifier.","refs":[]},{"text":"Attention-based approaches exploit visual saliency models or salient object detectors to identify the crop windows that more attract human attention [5, 24,26,27]. Some other hybrid methods employ a face detector to locate the regions of interest [32] or directly fit a saliency map from visually pleasurable photos taken by professional photographers [23]. Instead of using saliency, pixel importances can be also estimated using their objectness [9], or empirically defined energy functions [1,21].","refs":[{"start":153,"end":156,"marker":"bibr","target":"#b23"},{"start":156,"end":159,"marker":"bibr","target":"#b25"},{"start":159,"end":162,"marker":"bibr","target":"#b26"},{"start":247,"end":251,"marker":"bibr","target":"#b31"},{"start":352,"end":356,"marker":"bibr","target":"#b22"},{"start":448,"end":451,"marker":"bibr","target":"#b8"},{"start":493,"end":496,"marker":"bibr","target":"#b0"},{"start":496,"end":499,"marker":"bibr","target":"#b20"}]},{"text":"On the other hand, aesthetics-based methods leverage on photo quality assessment studies [3,15,28] using certain objective aspects of images, such as low level image features and empirical photographic composition rules. In particular, Nishiyama et al. [22] built a quality classifier using low level image features such as color histogram and Fourier coefficient from which they selected the cropped region with the highest quality score. Chen et al. [8] presented a method to learn the spatial correlation distributions of two arbitrary patches in an image for generating an omni-context prior which serve as rules to guide the composition of professional photos. Zhang et al. [31], instead, proposed a probabilistic model based on a region adjacency graph to transfer aesthetic features from the training photo onto the cropped ones.","refs":[{"start":253,"end":257,"marker":"bibr","target":"#b21"},{"start":452,"end":455,"marker":"bibr","target":"#b7"},{"start":679,"end":683,"marker":"bibr","target":"#b30"}]},{"text":"More recently, Yan et al. [30] proposed several features that accounts the removal of distracting content and the enhancement of overall composition. The influence of these features on crop solutions was learned from a training set of image pairs, before and after cropping by expert photographers. Other works, instead, exploit a RankSVM [6], working with features coming from the AlexNet model [16], or an aesthetics-aware deep ranking network [7] to classify each candidate window. Finally, Li et al. [6] formulated the automatic image cropping problem as a sequential decision-making process, and proposed an Aesthetics Aware Reinforcement Learning (A2-RL) model to solve this problem.","refs":[{"start":26,"end":30,"marker":"bibr","target":"#b29"},{"start":339,"end":342,"marker":"bibr","target":"#b5"},{"start":396,"end":400,"marker":"bibr","target":"#b15"},{"start":446,"end":449,"marker":"bibr","target":"#b6"},{"start":504,"end":507,"marker":"bibr","target":"#b5"}]}]},{"title":"Thumbnail Selection","paragraphs":[{"text":"The thumbnail selection problem has been widely addressed especially in the video domain, in which a frame that is visually representative of the video is selected and used as a representation of the video itself. In our case, instead, we want to find the most significant image from a collection of images (i.e. the pages of an historical manuscript), which somehow it can be considered as a related problem to the video thumbnail selection.","refs":[]},{"text":"Most conventional methods for video thumbnail selection have focused on learning visual representativeness purely from visual content [14,20], while more recent researches have addressed this problem as the selection of querydependent thumbnails to supply specific thumbnails for different queries.","refs":[{"start":134,"end":138,"marker":"bibr","target":"#b13"},{"start":138,"end":141,"marker":"bibr","target":"#b19"}]},{"text":"Liu et al. [18] proposed a reinforcement algorithm to rank the frames in each video, while a relevance model was employed to calculate the similarity between the video frames and the query keywords. Wang et al. [29] introduced a multiple instance learning approach to localize the tags into video shots and to select query-dependent thumbnail according to the tags.","refs":[{"start":11,"end":15,"marker":"bibr","target":"#b17"},{"start":211,"end":215,"marker":"bibr","target":"#b28"}]},{"text":"In [19], instead, a deep visual-semantic embedding was trained to retrieve query-dependent video thumbnails. In particular, this method employs a deeplylearned model to directly compute the similarity between the query and video thumbnails by mapping them into a common latent semantic space.","refs":[{"start":3,"end":7,"marker":"bibr","target":"#b18"}]}]},{"title":"Automatic Image Cropping","paragraphs":[{"text":"We tackle the image cropping task as that of finding a rectangular region R inside the given image I with maximum saliency. Comparing to previous methods which maximized a function of the saliency inside R, they all used other functions, such as the difference of saliency in R and outside R, or the difference between the mean saliency value in R and the mean saliency value outside R. We experimentally validated that when using state of the art saliency predictors, our choice, although simple, provides better results than more fancy objective functions.","refs":[]},{"text":"Formally, being x a pixel of the input image and S(x) its saliency value, predicted by a saliency model, we aim at finding:","refs":[]},{"text":"This objective boils down to finding the minimum bounding box of all salient pixels, and taking all regions R which contains the minimum bounding box.","refs":[]},{"text":"Since taking regions larger than the minimum bounding box would amount to having non salient pixels in R, we take R as the minimum bounding box of salient pixels.","refs":[]},{"text":"Regarding the saliency map, we compute it for every image by using the saliency method proposed in [12] which currently is the state of the art method in the saliency prediction task. In particular, starting from a classical convolutional neural network, it iteratively refines saliency predictions by incorporating an attentive mechanism. Also, it is able to reproduce the center bias present in human eye fixations by exploiting a set of prior maps directly learned from data. Overall, the performance achieved by the selected saliency method allows us to rely on saliency maps that effectively reproduce the human attention on natural images.","refs":[{"start":99,"end":103,"marker":"bibr","target":"#b11"}]}]},{"title":"Experimental Evaluation","paragraphs":[{"text":"In this section, we briefly describe datasets and metrics used to evaluate our solution and provide quantitative and qualitative comparisons with other image cropping methods.","refs":[]}]},{"title":"Datasets","paragraphs":[{"text":"To validate the effectiveness of visual saliency in the automatic image cropping task, we perform experiments on two different publicly available datasets.","refs":[]},{"text":"The Flickr-Cropping dataset [6] is composed of 1,743 images, each of them associated to ground-truth cropping parameters. Images are divided in training and test sets, respectively composed of 1,395 and 348 images. Our method is not trainable, but we perform experiments on test images only for a fair comparison with other methods.","refs":[{"start":28,"end":31,"marker":"bibr","target":"#b5"}]},{"text":"The CUHK Image Cropping dataset [30] contains the cropping parameters for 950 images that were manually cropped by an experienced photographer. Images are provided with cropping annotations of three different photographers.","refs":[{"start":32,"end":36,"marker":"bibr","target":"#b29"}]},{"text":"In our experiments, we evaluate the performance of our saliency-based cropping method with respect to all three different annotations.","refs":[]}]},{"title":"Metrics","paragraphs":[{"text":"Two different metrics are usually used to determine the accuracy of the automatic image cropping algorithms: the Intersection over Union (commonly abbreviated as IoU) and the Boundary Displacement Error (BDE).","refs":[]},{"text":"The Intersection over Union is an evaluation metric used to evaluate the overlapping between two bounding boxes. Technically, it is defined as","refs":[]},{"text":"where N is the number of samples, GT i is the area of the ith ground-truth bounding box and P i is the area of the ith predicted bounding box. The Boundary Displacement Error measures the distance between the sides of the ground-truth bounding box and the predicted one. For convenience, the values are normalized with respect to the size of the image. Mathematically, the metric is defined as","refs":[]},{"text":"where N is the number of samples, (x 1 , y 1 ) is the top left edge of the bounding box, (x 2 , y 2 ) is the bottom right edge of the bounding box, w i and h i are respectively width and height of the image, GT i is the ith ground-truth bounding box, and P i is the ith predicted bounding box.","refs":[]}]},{"title":"Results","paragraphs":[{"text":"We compare our solution with other automatic image cropping methods. For the Flickr-Cropping dataset, we perform comparisons with the most competitive saliency-based baseline presented in [6] (eDN), the RankSVM+DeCAF 7 model [6], the View Finding Network (VFN) proposed in [7] and the Aesthetics Aware Reinforcement Learning (A2-RL) model [17]. For the CUHK Image Cropping dataset, instead, the comparison methods are the change-based image cropping architecture presented in [30] (LearnChange) and the VFN and A2-RL models. Moreover, for both datasets, we compare our results with two variations of our model which we call Saliency Density and VGG Activations. The first one aims at maximizing the difference of the averaged saliency between the selected bounding box and the outer region of the image. For simplicity, we set the size of search window to each scale among [0.75, 0.80, . . . , 0.95] of the original image and slide search window over a 10 Ã— 10 uniform grid. The VGG Activations is, instead, the proposed image cropping method where the saliency maps are replaced with the activations of the last convolutional layer of the VGG-16 network [25]. In particular, since the last convolutional layer has 512 filters, we select for each image the activation map having the maximum sum.","refs":[{"start":188,"end":191,"marker":"bibr","target":"#b5"},{"start":225,"end":228,"marker":"bibr","target":"#b5"},{"start":273,"end":276,"marker":"bibr","target":"#b6"},{"start":339,"end":343,"marker":"bibr","target":"#b16"},{"start":476,"end":480,"marker":"bibr","target":"#b29"},{"start":1155,"end":1159,"marker":"bibr","target":"#b24"}]},{"text":"Table 1 shows the results on the Flickr-Cropping dataset. As it can be seen, our solution obtains the second best scores on both IoU and BDE metrics and achieves better results with respect to both our baselines. Table 2, instead, reports the results on the three different annotations of the CUHK Image Table 2. Experimental results on three different annotations of the CUHK Image Cropping [30] dataset. First, second and third best scores on each metric are respectively highlighted in red, green and blue colors.","refs":[{"start":6,"end":7,"marker":"table","target":"#tab_0"},{"start":219,"end":220,"marker":"table","target":null},{"start":310,"end":311,"marker":"table","target":null},{"start":392,"end":396,"marker":"bibr","target":"#b29"}]}]},{"title":"Annotation Method","paragraphs":[{"text":"Avg IoU Avg BDE 1 LearnChange [30] 0.7487 0.0667 VFN [7] 0.7847 0.0581 A2-RL [17] 0 Cropping dataset. In this case, our method achieves the best results on the first annotation on both metrics, while, on the other two annotations, it obtains the second or the third best scores. Despite the proposed solution is much simpler than the other comparison methods, the results achieved by our method on both considered datasets are very close to the best ones, thus confirming the effectiveness of the proposed strategy. Finally, some qualitative results with the corresponding saliency maps are presented in Fig. 1.","refs":[{"start":30,"end":34,"marker":"bibr","target":"#b29"},{"start":53,"end":56,"marker":"bibr","target":"#b6"},{"start":77,"end":81,"marker":"bibr","target":"#b16"},{"start":609,"end":610,"marker":"figure","target":null}]},{"text":"Fig. 1. results on sample images from the Flickr-Cropping dataset [6].","refs":[{"start":5,"end":6,"marker":"figure","target":null},{"start":66,"end":69,"marker":"bibr","target":"#b5"}]}]},{"title":"Automatic Page Selection of Historical Manuscripts","paragraphs":[{"text":"To validate our architecture in a real-world scenario, we apply it to find the best pages that represent historical manuscripts. This type of books usually have anonymous covers that does not represent its content, like plain colours or little artworks. Therefore, we develop a method to extract the most illustrative pages from every manuscript in order to use them as the preview of the book itself.","refs":[]},{"text":"Using this system, the navigation of historical digital libraries can be improved: users will be able to visually identify the content of a book watching its most representative images, without the need of opening it or read its summary.","refs":[]},{"text":"In this case, the proposed image cropping method is not the output of the system, but it is used to find the most interesting pages of every manuscript.","refs":[]},{"text":"In particular, the saliency map is calculated for every page of the book using the model reported in [12]. After extracting all saliency maps, Fig. 2. Example results of the page selection method on historical manuscripts. For each manuscript, the figure shows a list of some sample pages and the three pages selected by our method. As it can be seen, the selected pages contains representative visual contents and can be successfully used as a preview of the considered manuscript.","refs":[{"start":101,"end":105,"marker":"bibr","target":"#b11"},{"start":148,"end":149,"marker":"figure","target":null}]},{"text":"the method proposed in Sect. 3 is used to find the minimum crop that contains all the pixels with a saliency value higher than a threshold t (in our experiments t = 128). Then, a density score is calculated as the average value of saliency inside the bounding box divided by the average value of saliency outside the bounding box. In particular, it is formulated as","refs":[]},{"text":"where K is the number of pixels inside the bounding box, (i, j) and (l, m) are respectively the coordinates of the pixels inside and outside the bounding box, while w and h are width and height of the image.","refs":[]},{"text":"An high density score corresponds to an image where most of the saliency is restricted to a small area, therefore it contains a tiny region of high interest with respect to the rest of the image. On the contrary, a low density score corresponds to an image with a spread saliency map, therefore the image does not contain a valuable detail. Finally, the M images with the higher density score are selected as the most representative of the document.","refs":[]},{"text":"Note that the method does not require training and it is applicable to any type of book, but it performs better with illustrated books. In our experiments, we decide to select entire images in place of image crops since we consider the full pages more suitable to be a summary of the whole manuscript, but it would be also possible to extract some particular details.","refs":[]},{"text":"To validate our proposal, we apply the proposed automatic page selection method to a set of digitized historical manuscripts belonging to the Estense Library collection of Modena1 . Some notable results are shown in Fig. 2. As it can be seen, the selected pages contain representative visual contents of the corresponding manuscript and they can be used as a significant preview of the manuscript itself.","refs":[{"start":178,"end":179,"marker":null,"target":"#foot_0"},{"start":221,"end":222,"marker":"figure","target":null}]}]},{"title":"Conclusions","paragraphs":[{"text":"In this work, we presented a saliency-based image cropping method which, by selecting the minimum bounding box that contains all salient pixels, achieves promising results on different image cropping datasets. Moreover, we applied our solution to the image selection problem. In particular, to validate the effectiveness in real-world scenarios, we introduced a page selection method which identifies the most representative pages of an historical manuscript. Qualitative results demonstrated that our idea improves the navigation of historical digital libraries by automatic generating significant book previews.","refs":[]}]}],"tables":{"tab_0":{"heading":"Table 1 .","description":"Experimental results on the Flickr-Cropping[6] dataset. First, second and third best scores on each metric are respectively highlighted in red, green and blue colors.","rows":[["Method","Avg IoU Avg BDE"],["eDN [6]","0.4857","0.1372"],["RankSVM+DeCAF7 [6] 0.6019","0.1060"],["VFN [7]","0.6744 0.0872"],["A2-RL [17]","0.6564 0.0914"],["Saliency density","0.6193","0.0997"],["VGG activations","0.6004","0.1088"],["Ours","0.6589 0.0892"]]}},"abstract":{"title":"Abstract","paragraphs":[{"text":"Automatic image cropping techniques are particularly important to improve the visual quality of cropped images and can be applied to a wide range of applications such as photo-editing, image compression, and thumbnail selection. In this paper, we propose a saliencybased image cropping method which produces significant cropped images by only relying on the corresponding saliency maps. Experiments on standard image cropping datasets demonstrate the benefit of the proposed solution with respect to other cropping methods. Moreover, we present an image selection method that can be effectively applied to automatically select the most representative pages of historical manuscripts thus improving the navigation of historical digital libraries.","refs":[]}]}}