{"bibliography":{"title":"OpenAIRE's DOIBoost -Boosting Crossref for Research","authors":[{"person_name":{"surname":"Bruzzo","first_name":"Sandro"},"affiliations":[{"department":"Institute of Information Science and Technology -CNR","institution":null,"laboratory":null}],"email":"sandro.labruzzo@isti.cnr.it"},{"person_name":{"surname":"Manghi","first_name":"Paolo"},"affiliations":[{"department":"Institute of Information Science and Technology -CNR","institution":null,"laboratory":null}],"email":"paolo.manghi@isti.cnr.it"},{"person_name":{"surname":"Mannocci","first_name":"Andrea"},"affiliations":[{"department":"Knowledge Media Institute","institution":"The Open University","laboratory":null}],"email":"andrea.mannocci@open.ac.uk"}],"date":null,"ids":{"DOI":"10.1007/978-3-030-11226-4_11","arXiv":null},"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"keywords":["Orcid","Data science","Data integration","Open science","Scholarly communication","Crossref"],"citations":{"b0":{"title":"Unpaywall finds free versions of paywalled papers","authors":[{"person_name":{"surname":"Chawla","first_name":"D"},"affiliations":[],"email":null}],"date":{"year":"2017","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"Nature News","series":null,"scope":null},"b1":{"title":"An overview of Microsoft Academic Service (MAS) and applications","authors":[{"person_name":{"surname":"Sinha","first_name":"A"},"affiliations":[],"email":null}],"date":{"year":"2015","month":null,"day":null},"ids":null,"target":null,"publisher":"ACM","journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":243,"to_page":246}}},"b2":{"title":"ORCID: a system to uniquely identify researchers","authors":[{"person_name":{"surname":"Haak","first_name":"L"},"affiliations":[],"email":null},{"person_name":{"surname":"Fenner","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Paglione","first_name":"L"},"affiliations":[],"email":null},{"person_name":{"surname":"Pentz","first_name":"E"},"affiliations":[],"email":null},{"person_name":{"surname":"Ratner","first_name":"H"},"affiliations":[],"email":null}],"date":{"year":"2012","month":null,"day":null},"ids":{"DOI":"10.1087/20120404","arXiv":null},"target":"https://doi.org/10.1087/20120404","publisher":null,"journal":"Learn. Publish","series":null,"scope":{"volume":25,"pages":{"from_page":259,"to_page":264}}},"b3":{"title":"OpenAIREplus: the European scholarly communication data infrastructure","authors":[{"person_name":{"surname":"Manghi","first_name":"P"},"affiliations":[],"email":null},{"person_name":{"surname":"Bolikowski","first_name":"L"},"affiliations":[],"email":null},{"person_name":{"surname":"Manold","first_name":"N"},"affiliations":[],"email":null},{"person_name":{"surname":"Schirrwagen","first_name":"J"},"affiliations":[],"email":null},{"person_name":{"surname":"Smith","first_name":"T"},"affiliations":[],"email":null}],"date":{"year":"2012","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"D-Lib Mag","series":null,"scope":{"volume":18,"pages":{"from_page":1,"to_page":1}}},"b4":{"title":"","authors":[{"person_name":{"surname":"Fortunato","first_name":"S"},"affiliations":[],"email":null}],"date":{"year":"2018","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"Science of science. Science","series":null,"scope":{"volume":359,"pages":{"from_page":185,"to_page":185}}},"b5":{"title":"DOIBoost Dataset Dump (Version 1.0)","authors":[{"person_name":{"surname":"Bruzzo","first_name":"La"},"affiliations":[],"email":null},{"person_name":{"surname":"Manghi","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Mannocci","first_name":"P"},"affiliations":[],"email":null}],"date":{"year":"2018","month":null,"day":null},"ids":{"DOI":"10.5281/zenodo.1438356","arXiv":null},"target":"http://doi.org/10.5281/zenodo.1438356","publisher":null,"journal":null,"series":null,"scope":null},"b6":{"title":"DOIBoost Software Toolkit (Version 1.0)","authors":[{"person_name":{"surname":"Bruzzo","first_name":"La"},"affiliations":[],"email":null}],"date":{"year":"2018","month":"10","day":"01"},"ids":{"DOI":"10.5281/zenodo.1441058","arXiv":null},"target":"http://doi.org/10.5281/zenodo.1441058","publisher":null,"journal":"Zenodo","series":null,"scope":null}},"sections":[{"title":"Introduction","paragraphs":[{"text":"Research in information science and scholarly communication strongly relies on the availability of openly accessible datasets of metadata and, where possible, of relative payloads. In the context of literature publishing, Crossref is certainly playing a central role as mediator between publishers of scientific literature and consumers, which are often also producers in this process. Publisher services publish scientific literature, mint a DOI from Crossref, and push into the system a complete bibliographic record according to Crossref metadata scheme. In turn, Crossref provides CC-BY 4.0 access to its entire metadata collection via REST APIs 1 . Due to its longitudinal, pan-publisher and up-to-date content, this metadata collection has become the pivot of several other initiatives willing to (i) enrich/complete the collection with further information, not necessarily provided by publishers to Crossref, or (ii) willing to enrich their own collection(s) with DOIs and metadata from Crossref. Several well-known examples can be mentioned, such as Google Scholar, Dimensions, SemanticScholar, Microsoft Academic Graph, AMiner, OpenAIRE, ORCID, Unpaywall; many of them make their content freely available for research purposes, under CC-BY or CC-0 license. Researchers can either download or access via APIs such metadata collections and perform their experiments, but only after non-trivial efforts of (meta)data integration, cleaning, and harmonization. Efforts often given for granted by major players in scholarly knowledge analytics and dismissed in one sentence where a list of data sources, often behind paywall and thus not available to the general public, is provided; e.g. [5]. Typically, such integration efforts differ from experiment to experiment, where, violating principles of Open Science, provenance and lineage of data are often undocumented. This general misalignment spoils quality, and evaluation and comparison of different research endeavours, which should be rather based on common input data collections, transparently generated and recognized by the community.","refs":[{"start":650,"end":651,"marker":null,"target":"#foot_0"},{"start":1692,"end":1695,"marker":"bibr","target":"#b4"}]},{"text":"In response to this general demand, this paper presents DOIBoost [6], a collection of metadata records resulting from a transparent process of integration, harmonization, and cleaning of Crossref with Microsoft Academic Graph2 (via Azure Data Lake Store), ORCID, 3 and Unpaywall. 4 Such sources can considerably impact on the quality and richness of Crossref by adding publication access rights information, missing abstracts, author identifiers, and precious authors' affiliations equipped with organization identifiers. The result of our integration efforts, the DOIBoost dataset, is here described, i.e. its input sources, its data model (JSON schema), together with the methodology to generate the dataset, and the actual software (DOIBoost Software Toolkit) and machinery used to produce it. Both DOIBoost dataset and software are published in Zenodo.org [6,7] and made available for research purposes under CC-BY 4.0. DOIBoost will become an input source to the OpenAIRE information graph. 5","refs":[{"start":65,"end":68,"marker":"bibr","target":"#b5"},{"start":225,"end":226,"marker":null,"target":"#foot_1"},{"start":263,"end":264,"marker":null,"target":"#foot_2"},{"start":280,"end":281,"marker":null,"target":"#foot_3"},{"start":860,"end":863,"marker":"bibr","target":"#b5"},{"start":863,"end":865,"marker":"bibr","target":"#b6"},{"start":996,"end":997,"marker":null,"target":"#foot_4"}]}]},{"title":"The Dataset","paragraphs":[{"text":"DOIBoost is constructed by enriching Crossref records as shown in Fig. 1: the input sources described in Table 1 are collected and integrated by using Crossref DOIs as pivot for the data integration process. A final cleaning step is applied, to get rid of the records whose quality is too low or that are leftovers inserted in Crossref for testing reasons and never removed. In the following sections, we provide details on the input sources and describe the DOIBoost data model.","refs":[{"start":71,"end":72,"marker":"figure","target":"#fig_0"},{"start":111,"end":112,"marker":"table","target":"#tab_0"}]}]},{"title":"Input Dataset Sources","paragraphs":[{"text":"The input data sources are described in Table 1. Their metadata is provided under freeto-reuse and distribute license, although with slightly different constraints, which however do not prevent the dissemination of the collection. Such sources are relevant to Crossref due to the following reasons:","refs":[{"start":46,"end":47,"marker":"table","target":"#tab_0"}]},{"text":"• Unpaywall by ImpactStory [1] attempts to identify the Open Access records in Crossref by also crawling from the Web (e.g. from institutional repositories) the best Open Access URLs they can find for each record. Crossref DOIs can be enriched with such Open Access instances. • Microsoft Academic Graph, via Azure Data Lake Store (ADLS) [2] uses \"…AIpowered machine readers to process all documents discovered by Bing crawler and extract scholarly entities and their relationships to form a knowledge base…\". When possible, MAG links to DOIs and can therefore enrich Crossref with extra information, e.g. author identifiers, affiliation identifiers, abstracts. • ORCID [3] builds a world-wide record of researchers by providing them with a persistent identifier and allowing them to populate a publicly accessible curriculum, inclusive of article DOIs. As a result, ORCID gathers many more associations between articles in Crossref and ORCID IDs than Crossref is actually collecting from publishers.","refs":[{"start":27,"end":30,"marker":"bibr","target":"#b0"},{"start":338,"end":341,"marker":"bibr","target":"#b1"},{"start":670,"end":673,"marker":"bibr","target":"#b2"}]}]},{"title":"Dataset Model","paragraphs":[{"text":"Crossref, as well as the other sources, are integrated into a common (meta)data model and JSON schema, initially populated with Crossref records. The model is illustrated via an example in Listing 1 (For record types please refer to https://api.crossref.org/v1/ types). Data integration is relevance-driven in the sense that from the input data sources only a few properties, regarded as particularly important, are selected for integration into the Crossref dataset. Accordingly, the model has been conceived to include a set of Crossref properties, as provided by the relative dataset, and a set of properties that can be integrated from other sources, as described in Fig. 1. Each one of these \"inheritable\" properties is equipped with a provenance field, whose value currently includes \"Crossref\", \"MAG\", \"Unpaywall\", and \"ORCID\" in order to trace the origin of the information. Provenance plays a key role when processing a dataset in order to account for the origin of any possible misbehaviour or unexpected result and to fine-tune processing based on the data model and on specific provenance of given fields. More specifically, the properties are:","refs":[{"start":676,"end":677,"marker":"figure","target":"#fig_0"}]},{"text":"• Identifiers of authors: authors can be assigned identifiers from different \"authorities\", for example internal identifiers as provided by Microsoft or persistent identifiers as provided by ORCID. Accordingly, the model allows to gather multiple identifiers for the same author; to facilitate programmatic interpretation, for each identifier value the model includes the respective schema, intended as the authority issuing the identifier. • Affiliations of authors: authors are affiliated to an institution (or more), which is the organization of the author at the moment of publishing; the model allows the collection of different affiliations for the same author since these may be collected from different sources (e.g. Crossref and MAG); in turn, each institution may be associated to different identifiers provided by the same source, e.g. MAG may provide organization IDs internal to MAG as well as organization persistent identifiers released by the Global Research Identifier Database6 (hence, same provenance, but different schema). • Dates: publications in Crossref often miss the publishing date which we can collect from Crossref and MAG (provenance for this field is missing); • Abstracts: abstracts can be provided by different sources, e.g. Crossref or by MAG, hence require provenance information to track down their origin. { \"title\":\"My Title\", \"authors\":[ { \"given\":\"Marco\", \"family\":\"Rossi\", \"fullname\": \"Marco Rossi\", \"identifiers\":[ { \"schema\":\"ORCID\", \"value\":\"https://..../0000-0002-3337-2025\", \"provenance\":\"ORCID\" }, { \"schema\":\"MAG ID\", \"value\":\"https://.../1278293695\", \"provenance\":\"MAG\" } ],","refs":[{"start":994,"end":995,"marker":null,"target":"#foot_5"}]},{"text":"\"affiliations\":[ { \"value\":\"My Affiliation Name\", \"official-page\":\"www.affiliation.org\", \"identifiers\":[ { \"schema\":\"grid.ac\", \"value\":\"https://.../grid.12345.a\" }, { \"schema\":\"microsoftID\", \"value\":\"https://.../4213412341\" }, { \"schema\":\"wikipedia\", \"value\":\"https:///wiki/my_affiliation\" }],","refs":[]},{"text":"\"provenance\":\"MAG\" } ] }, { \"given\":\"Giuseppe\", \"family\":\"Trovato\", \"fullname\": \"Giuseppe Trovato\", \"identifiers\":[],","refs":[]},{"text":"\"affiliations\":[] } ],","refs":[]},{"text":"\"issued\":\"2016-07-01\", \"abstract\":[ { \"value\":\"Abstract Text\", \"provenance\":\"MAG\" }, { \"value\":\"Abstract Text\", \"provenance\":\"Crossref\" } ],","refs":[]},{"text":"\"subject\":[\"Agronomy and Crop Science\", \"Forestry\"],","refs":[]},{"text":"\"type\":\"journal-article\", \"license\":[ { \"url\":\"http://www.elsevier.com/tdm/userlicense/1.0/\", \"date-time\":\"2011-07-01T00:00:00Z\", \"content-version\":\"tdm\", \"delay-in-days\":0 } ],","refs":[]},{"text":"\"instances\":[ { \"url\":\"http://unkonwonInstance.org\", \"access-rights\":\"UNKNOWN\", \"provenance\":\"Crossref\" }, { \"url\":\"http://openAccessInstance.org\", \"access-rights\":\"OPEN\", \"provenance\":\"Unpaywall\" } ],","refs":[]},{"text":"\"published-online\":\"2016-08-01\", \"published-print\":\"2016-07-01\", \"accepted\":\"2016-01-01\", \"publisher\":\"Publisher Name\", \"doi\":\"10.1016/j.ffhfhgfhf\", \"doi-url\":\"http://dx.doi.org/10.1016/j.ffhfhgfhf\", \"issn\":[ { \"type\":\"print\", \"value\":\"01234-5678\" } ],","refs":[]},{"text":"\"collected-from\":[ \"Crossref\", \"MAG\", \"Unpaywall\", \"ORCID\" ],","refs":[]},{"text":"\"record-quality-report\": \"complete\" } Listing 1. DOIBoost: JSON record example.","refs":[]},{"text":"• Instances of the DOI work: instances represent the location of the files of a given DOI work at different source sites. Since these may represent different manifestationse.g. the published journal version, the open access version of an article in an institutional repository -each instance has its own list of files (URLs) and access rights7 . • Record quality report: in order to filter out \"invalid\" records when importing the dump into the OpenAIRE system, the records are marked with a report of quality. This information may be useful also to scientists re-using the data and is therefore captured in the model. The property can have the following values: incomplete (the record misses one or more of the OpenAIRE mandatory properties, i.e. Title, Authors, or Date); mock (mock records are very frequent in the scholarly communication, typically created by operators at publishers or institutional repositories to verify system functionalities and then never removed); complete (when the record is neither marked as incomplete nor mock).","refs":[{"start":342,"end":343,"marker":null,"target":"#foot_6"}]}]},{"title":"Methodology","paragraphs":[{"text":"Due to the large number of records, in the order of hundreds of millions, our solution relies on in-memory parallel processing techniques. To this end, the software we developed, named DOIBoost Software Toolkit [7], is deployed over the infrastructure depicted in Fig. 2. The architecture workflows support two distinct phases of (i) data collection and preparation for integration and (ii) data integration to deliver DOIBoost. In the following we described the actions involved in these two phases; knowledge on HDFS and Spark terminology and technologies is strongly advisable to fully understand the internals.","refs":[{"start":211,"end":214,"marker":"bibr","target":"#b6"},{"start":269,"end":270,"marker":"figure","target":"#fig_1"}]}]},{"title":"DOIBoost Toolkit Deployment","paragraphs":[{"text":"The infrastructure underlying DOIBoost Toolkit is shown in Fig. 2 and features: 20 virtual machines (VMs) for Apache HDFS Data Nodes and Spark workers, each VM with 16 cores, 32 GB of ram, and 250 GB of disk; plus 3 dedicated virtual machines for HDFS Name Nodes, each one with 8 cores, 16 GB of ram, and 40 GB of disk. Apache HDFS is used as the main storage for the objects and files collected from the sources, in order to exploit its fast writing and reading rates. Apache Spark is used to (i) read such content from HDFS in order to manipulate and transform it into Spark DataFrames that match the DOIBoost data model, and (ii) to perform the data integration pipeline that produces DOIBoost, by joining the data source DataFrames. All workflows are implemented and orchestrated via Apache Oozie 8 . The DOIBoost Toolkit is written in PySpark under AGPL Open Source license9 and is available for download and citation on Zenodo.org [7]. The package contains the scripts required to implement such workflows and reproduce the collection, which will be described in the following sections.","refs":[{"start":64,"end":65,"marker":"figure","target":"#fig_1"},{"start":878,"end":879,"marker":null,"target":"#foot_8"},{"start":937,"end":940,"marker":"bibr","target":"#b6"}]}]},{"title":"Data Collection and Preparation for Integration","paragraphs":[{"text":"As anticipated in the previous sections, each source is collected according to different methods, then transferred into HDFS as a corresponding sequence file, and finally manipulated in-memory via Spark jobs (generateXDataFrame.py), in order to produce a relative DOIBoost Spark DataFrame.","refs":[]},{"text":"We assume in the following that the datasets are manually collected and transferred into HDFS via Shell, creating corresponding sequence files: JSON format for Crossref and CSV text format for MAG, ORCID, and Unpaywall. More specifically:","refs":[]},{"text":"• Crossref is downloaded from the relative APIs using the GitHub repository Crossref REST API10 made available by Crossref; the execution of the script results in a dump on the file system. • ORCID and Unpaywall are manually downloaded as CSV text files on the file system. Each line in the dump from ORCID represents an author with his/her publication list and in Unpaywall represents a DOI entry with the relative OA status and URL access information.  Such dumps can be uploaded to HDFS as sequence files with a simple shell command (\"hdfs dfs -put fileName pathHDFS\"). Once the sequence files are created, the \"preparation to integration\" phase is performed by executing (in any order) the following Spark jobs:","refs":[{"start":93,"end":95,"marker":null,"target":"#foot_9"}]},{"text":"• generateCrossrefDataFrame.py The script reads from the Crossref sequence file and transforms the JSON records into a respective DOIBoost DataFrame. • generateMAGDataFrame.py The script generates DataFrames corresponding to the MAG tables and performs the joins required to recombine articles with authors, affiliations, and abstracts to deliver a DOIBoost DataFrame that only contains such fields for each article. The process also filters out articles from MAG that do not have a DOI. • generateORCIDDataFrame.py The ORCID sequence file contains rows relative to ORCID author identifiers, each followed by the list of publications of the author. First, the script builds an inverted list, where the key is the DOI followed by the list of authors (the ones that can be found in the sequence file for the DOI) with their first and second names and ORCID ID. Finally, the script builds a DOIBoost DataFrame from these representations, which only contain author information for each article: given, family, fullname, and identifier (schema, value, provenance). • generateUnPayWallDataFrame.py The Unpaywall sequence file contains rows relative to Crossref DOIs and their Open Access information12 as extracted by Unpaywall. The Spark script transforms the file in a DOIBoost DataFrame where articles are equipped with the instances derivable from each row (URL and accessrights) and are empty on other fields.","refs":[{"start":1193,"end":1195,"marker":null,"target":"#foot_11"}]},{"text":"The execution times of these jobs, given our current architectural specifications, are reported in Table 2 below.","refs":[{"start":105,"end":106,"marker":"table","target":"#tab_2"}]}]},{"title":"DOIBoost Integration Pipeline","paragraphs":[{"text":"Once all DOIBoost DataFrames for the input sources are generated a final integration script can be executed, named createDOIBoost.py. The script performs a join by DOI, starting from Crossref, and adding in sequence: MAG, ORCID, and Unpaywall. Each step of the pipeline progressively enriches DOIBoost DataFrame with one data source at a time (by performing joins on DOIs). Given an input DOIBoost record and one matching its DOI, two kinds of enrichments are possible:","refs":[]},{"text":"• Publication-level update: this happens when the joined data source adds incrementally information to the record, such as Unpaywall, which patches the input DataFrame by adding an Unpaywall instance or a date to the DOI, and MAG, which adds an abstract or a date to the record. • Author-match-dependent update: this happens when the information to be added by the data source is relative to the authors of a DOIBoost record (author identifiers and author affiliations); in this case the authors of the two records to be joined must be matched to find the correspondence and thus complete the information in the proper places; the match is string based and considered positive when the Levenshtein distance13 between author names is above 0.8. Note that to avoid quadratic performance issues, author-to-author match is not performed for records whose number of authors is greater than 300 (around 12,000 records). In such cases the authors from MAG are simply used as the authoritative set.","refs":[{"start":706,"end":708,"marker":null,"target":"#foot_12"}]},{"text":"In both cases, the information is added together with the appropriate provenance information. Finally, the last step of the workflow generates a DOIBoost dump in JSON format on the file system, to be openly shared and to be ingested into Open-AIRE. This step also marks the record with a quality report (complete, incomplete, mock), specified by the property record-quality-report. Currently, the validation steps identify two cases of mock records:","refs":[]},{"text":"• Basic test records: a record is considered as such if by normalizing the title (i.e. lower-case strings and removal of articles, special characters, etc.) and removing the word \"test\" the resulting string is empty; • Structured test records: a record is considered as such if an occurrence of the word \"test\" appears both in the title and at least in the name of one author. ","refs":[]}]},{"title":"Evaluation","paragraphs":[{"text":"In Table 3 we report the measure of the \"boost\" that each data source gives to the original dataset as obtained via Crossref APIs. For each property involved in the aggregation, we report both the number of records with a Crossref DOI that was enriched with the property and the number of records that was effectively \"boosted\", i.e. records for which the property was missing. For the sake of example, in Table 4 we quantify the \"boost\" for authors in DOI-Boost. To this end, we define authorship as the contribution a single author has in the context of a given paper. Hence, if a paper p has three authors a1; a2; a3 f g , we count three authorship in total. This said, the entire Crossref corpus rounds up 263,869,225 authorships; of these, only the 1.15% has is equipped with an author identifier in Crossref, while in DOIBoost the percentage jumps to 80.45%, thanks to the joint integration effort of both MAG and ORCID. Similarly, only the 9.83% of authorships is assigned an affiliation in Crossref, while in DOIBoost we reach the 62.63%.","refs":[{"start":9,"end":10,"marker":"table","target":"#tab_4"},{"start":412,"end":413,"marker":"table","target":"#tab_5"}]}]},{"title":"Conclusions","paragraphs":[{"text":"In this paper, we presented our reproducible data integration efforts in creating DOI-Boost, an open dataset in support of research in the field of scholarly communication and scholarly knowledge mining. The contribution of this work is twofold: first, the dataset itself, together with the description of its value and its content, i.e. the data sources involved in the integration process; secondly, in order not to fall in the perpetration of the infamous \"yet another resource\" series, the description of the methodology to generate it, embodied in an open source software toolkit that can be used to recreate, extend and update the DOIBoost dataset. ","refs":[]}]}],"tables":{"tab_0":{"heading":"Table 1 .","description":"DOIBoost: input datasets. Crossref APIs, http://api.Crossref.org. b ORCID download, https://orcid.org/content/download-file. c Microsoft Academic Graph obtained via the Azure Data Lake Store (ADLS), https://azure. microsoft.com/en-us/services/storage/data-lake-storage. d Unpaywall download, https://unpaywall.org/products/snapshot.","rows":[["Source","License Protocol & format","Approximate size","Download date"],["Crossref","CC0","API, JSON a","250 GB","Nov 2018"],["ORCID","CC0 1.0 Download, CSV (txt) b 32 GB (zipped)","Oct 2018"],["MAG (ADLS) ODC-BY Download, CSV (txt) c 120 GB","May 2018"],["","","","(relevant DB tables)",""],["Unpaywall","CC0","Download, CSV (txt) d 6 GB (zipped)","Jun 2018"],["a","","","",""]]},"tab_2":{"heading":"Table 2 .","description":"DOIBoost generation: workflow execution times.","rows":[["Execution step","Execution time"],["generateCrossrefDataFrame.py","6.1 min"],["generateMAGDataFrame.py","1.1 h"],["generateORCIDDataFrame.py","30 s"],["generateUnpaywallDataFrame.py 20 s"],["createDOIBoost.py","35 min"]]},"tab_3":{"heading":"Table 2","description":"reports on the execution time of the individual steps. Note that the final step createDOIBoost.py, performing a join between 105 million records in Crossref and 74 million from MAG, 11 million from ORCID, and 97 million from Unpaywall, runs in around 35 min. The records marked as incomplete or mock are around 25 million.","rows":[]},"tab_4":{"heading":"Table 3 .","description":"Input datasets and contributing properties.","rows":[["Source","Properties","# of Crossref DOIs","Boost: # of Crossref DOIs"],["","","enriched by the source","enriched by the source with a"],["","","with a property","missing property"],["ORCID","Author IDs","11,345,996","9,666,098"],["","(ORCID)","",""],["MAG","DOIs","71,654,334","68,561,516"],["","Affiliation","45,670,806","45,670,806"],["","(GRID.ac)","",""],["","Affiliation","51,630,810","47,528,221"],["","(Microsoft)","",""],["","Abstract","45,407,968","43,857,752"],["","Author ID","74,582,104","68,561,516"],["","(Microsoft)","",""],["","Date","71,654,334","2,542,773"],["Unpaywall","Instances","97,751,914","22,328,223"],["Crossref","All fields","100,507,347","91,365,868"]]},"tab_5":{"heading":"Table 4 .","description":"Authorships enhancement in DOIBoost.","rows":[["Indicator","Crossref","DOIBoost"],["# authorships assigned an identifier 3,060,804 212,291,232"],["# authorships assigned an affiliation 25,941,421 165,271,110"]]}},"abstract":{"title":"Abstract","paragraphs":[{"text":"Research in information science and scholarly communication strongly relies on the availability of openly accessible datasets of scholarly entities metadata and, where possible, their relative payloads. Since such metadata information is scattered across diverse, freely accessible, online resources (e.g. Crossref, ORCID), researchers in this domain are doomed to struggle with (meta)data integration problems, in order to produce custom datasets of often undocumented and rather obscure provenance. This practice leads to waste of time, duplication of efforts, and typically infringes open science best practices of transparency and reproducibility of science. In this article, we describe how to generate DOIBoost, a metadata collection that enriches Crossref with inputs from Microsoft Academic Graph, ORCID, and Unpaywall for the purpose of supporting high-quality and robust research experiments, saving times to researchers and enabling their comparison. To this end, we describe the dataset value and its schema, analyse its actual content, and share the software Toolkit and experimental workflow required to reproduce it. The DOIBoost dataset and Software Toolkit are made openly available via Zenodo. org. DOIBoost will become an input source to the OpenAIRE information graph.","refs":[]}]}}