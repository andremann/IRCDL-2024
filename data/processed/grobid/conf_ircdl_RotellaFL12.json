{"bibliography":{"title":"A Domain Based Approach to Information Retrieval in Digital Libraries","authors":[{"person_name":{"surname":"Rotella","first_name":"Fulvio"},"affiliations":[{"department":"Dipartimento di Informatica","institution":"Università di Bari","laboratory":null}],"email":"fulvio.rotella@uniba.it"},{"person_name":{"surname":"Ferilli","first_name":"Stefano"},"affiliations":[{"department":"Dipartimento di Informatica","institution":"Università di Bari","laboratory":null},{"department":"Centro Interdipartimentale per la Logica e sue Applicazioni","institution":"Università di Bari","laboratory":null}],"email":"stefano.ferilli@uniba.it"},{"person_name":{"surname":"Leuzzi","first_name":"Fabio"},"affiliations":[{"department":"Dipartimento di Informatica","institution":"Università di Bari","laboratory":null}],"email":"fabio.leuzzi@uniba.it"}],"date":null,"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"keywords":[],"citations":{"b0":{"title":"A semantic approach for resource cataloguing and query resolution","authors":[{"person_name":{"surname":"Angioni","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Demontis","first_name":"R"},"affiliations":[],"email":null},{"person_name":{"surname":"Tuveri","first_name":"F"},"affiliations":[],"email":null}],"date":{"year":"2008","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"Communications of SIWN. Special Issue on Distributed Agent-based Retrieval Tools","series":null,"scope":{"volume":5,"pages":{"from_page":62,"to_page":66}}},"b1":{"title":"An empirical study of required dimensionality for large-scale latent semantic indexing applications","authors":[{"person_name":{"surname":"Bradford","first_name":"R"},"affiliations":[],"email":null}],"date":{"year":"2008","month":null,"day":null},"ids":null,"target":null,"publisher":"ACM","journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":153,"to_page":162}}},"b2":{"title":"Improving Information Retrieval with Latent Semantic Indexing","authors":[{"person_name":{"surname":"Deerwester","first_name":"S"},"affiliations":[],"email":null}],"date":{"year":"1988","month":"10","day":null},"ids":null,"target":null,"publisher":"American Society for Information Science","journal":null,"series":null,"scope":{"volume":25,"pages":null}},"b3":{"title":"Concept decompositions for large sparse text data using clustering","authors":[{"person_name":{"surname":"Dhillon","first_name":"I"},"affiliations":[],"email":null},{"person_name":{"surname":"Modha","first_name":"D"},"affiliations":[],"email":null}],"date":{"year":"2001","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":143,"to_page":175}}},"b4":{"title":"WordNet: An Electronic Lexical Database","authors":[],"date":{"year":"1998","month":null,"day":null},"ids":null,"target":null,"publisher":"MIT Press","journal":null,"series":null,"scope":null},"b5":{"title":"Automatic Digital Document Processing and Management: Problems, Algorithms and Techniques","authors":[{"person_name":{"surname":"Ferilli","first_name":"S"},"affiliations":[],"email":null}],"date":{"year":"2011","month":null,"day":null},"ids":null,"target":null,"publisher":"Springer Publishing Company, Incorporated","journal":null,"series":null,"scope":null},"b6":{"title":"Plugging Taxonomic Similarity in First-Order Logic Horn Clauses Comparison","authors":[{"person_name":{"surname":"Ferilli","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Biba","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Di Mauro","first_name":"N"},"affiliations":[],"email":null},{"person_name":{"surname":"Basile","first_name":"T"},"affiliations":[],"email":null},{"person_name":{"surname":"Esposito","first_name":"F"},"affiliations":[],"email":null}],"date":{"year":"2009","month":null,"day":null},"ids":null,"target":null,"publisher":"Springer","journal":null,"series":null,"scope":{"volume":5883,"pages":{"from_page":131,"to_page":140}}},"b7":{"title":"Pictures of relevance: A geometric analysis of similarity measures","authors":[{"person_name":{"surname":"Jones","first_name":"W"},"affiliations":[],"email":null},{"person_name":{"surname":"Furnas","first_name":"G"},"affiliations":[],"email":null}],"date":{"year":"1987","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"Journal of the American Society for Information Science","series":null,"scope":{"volume":38,"pages":{"from_page":420,"to_page":442}}},"b8":{"title":"Concept indexing: A fast dimensionality reduction algorithm with applications to document retrieval and categorization","authors":[{"person_name":{"surname":"Karypis","first_name":"G"},"affiliations":[],"email":null},{"person_name":{"surname":"Han","first_name":"E.-H.(s"},"affiliations":[],"email":null}],"date":{"year":"2000","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b9":{"title":"Fast exact inference with a factored model for natural language parsing","authors":[{"person_name":{"surname":"Klein","first_name":"D"},"affiliations":[],"email":null},{"person_name":{"surname":"Manning","first_name":"C"},"affiliations":[],"email":null}],"date":{"year":"2003","month":null,"day":null},"ids":null,"target":null,"publisher":"MIT Press","journal":null,"series":null,"scope":{"volume":15,"pages":null}},"b10":{"title":"Some methods for classification and analysis of multivariate observations","authors":[{"person_name":{"surname":"Macqueen","first_name":"J"},"affiliations":[],"email":null}],"date":{"year":"1967","month":null,"day":null},"ids":null,"target":null,"publisher":"University of California Press","journal":null,"series":null,"scope":{"volume":1,"pages":{"from_page":281,"to_page":297}}},"b11":{"title":"Integrating subject field codes into wordnet","authors":[{"person_name":{"surname":"Magnini","first_name":"B"},"affiliations":[],"email":null},{"person_name":{"surname":"Cavaglià","first_name":"G"},"affiliations":[],"email":null}],"date":{"year":"2000","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":1413,"to_page":1418}}},"b12":{"title":"Okapi at trec-3","authors":[{"person_name":{"surname":"Robertson","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Walker","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Jones","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Hancock-Beaulieu","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Gatford","first_name":"M"},"affiliations":[],"email":null}],"date":{"year":"1994","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b13":{"title":"The SMART Retrieval System-Experiments in Automatic Document Processing","authors":[{"person_name":{"surname":"Salton","first_name":"G"},"affiliations":[],"email":null}],"date":{"year":"1971","month":null,"day":null},"ids":null,"target":null,"publisher":"Prentice-Hall, Inc","journal":null,"series":null,"scope":null},"b14":{"title":"Automatic term class construction using relevance-a summary of work in automatic pseudoclassification","authors":[{"person_name":{"surname":"Salton","first_name":"G"},"affiliations":[],"email":null}],"date":{"year":"1980","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"Inf. Process. Manage","series":null,"scope":{"volume":16,"pages":{"from_page":1,"to_page":15}}},"b15":{"title":"Introduction to Modern Information Retrieval","authors":[{"person_name":{"surname":"Salton","first_name":"G"},"affiliations":[],"email":null},{"person_name":{"surname":"Mcgill","first_name":"M"},"affiliations":[],"email":null}],"date":{"year":"1984","month":null,"day":null},"ids":null,"target":null,"publisher":"McGraw-Hill Book Company","journal":null,"series":null,"scope":null},"b16":{"title":"A vector space model for automatic indexing","authors":[{"person_name":{"surname":"Salton","first_name":"G"},"affiliations":[],"email":null},{"person_name":{"surname":"Wong","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Yang","first_name":"C"},"affiliations":[],"email":null}],"date":{"year":"1975","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"Commun. ACM","series":null,"scope":{"volume":18,"pages":{"from_page":613,"to_page":620}}},"b17":{"title":"Pivoted document length normalization","authors":[{"person_name":{"surname":"Singhal","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Buckley","first_name":"C"},"affiliations":[],"email":null},{"person_name":{"surname":"Mitra","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Mitra","first_name":"A"},"affiliations":[],"email":null}],"date":{"year":"1996","month":null,"day":null},"ids":null,"target":null,"publisher":"ACM Press","journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":21,"to_page":29}}},"b18":{"title":"Extracting lexical semantic knowledge from wikipedia and wiktionary","authors":[{"person_name":{"surname":"Zesch","first_name":"T"},"affiliations":[],"email":null},{"person_name":{"surname":"Müller","first_name":"C"},"affiliations":[],"email":null},{"person_name":{"surname":"Gurevych","first_name":"I"},"affiliations":[],"email":null}],"date":{"year":"2008","month":"05","day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null}},"sections":[{"title":"Introduction","paragraphs":[{"text":"The easy and cheap production of documents using computer technologies, plus the extensive digitization of legacy documents, have caused a significant flourishing of documents in electronic format, and the spread of Digital Libraries (DLs) aimed at collecting and making them available to the public, removing time and space barriers to distribution and fruition that are typical of paper material. In turn, the fact that anybody can produce and distribute documents (without even the cost of printing them) may negatively affect the average quality of their content. Although, as a particular kind of library, a DL has the mission of gathering a collection of documents which meets the quality standards chosen by the institution that maintains it, some repositories may adopt looser quality enforcing policies, and leave this responsibility to the authors, also due to the difficulty in manually checking and validating such a huge amount of material. In these cases, the effectiveness of document retrieval might be significantly tampered, affecting the fruition of the material in the repository as a consequence. Under both these attacks, anyone who is searching for information about a given topic is often overwhelmed by documents that only apparently are suitable for satisfying his information needs. In fact, most information in these documents is redundant, partial, sometimes even wrong or just unsuitable for the user's aims.","refs":[]},{"text":"A possible way out consists in automatic instruments that (efficiently) return significant documents as an answer to user queries, that is the branch of interest of Information Retrieval (IR).","refs":[]},{"text":"IR aims at providing the users with techniques for finding interesting documents in a repository, based on some kind of query. Although multimedia digital libraries are starting to gain more and more attention, the vast majority of the content of current digital document repositories is still in textual form. Accordingly, user queries are typically expressed in the form of natural language sentences, or sets of terms, based on which the documents are retrieved and ranked. This is clearly a tricky setting, due to the inherent ambiguity of natural language. Numerical/statistical manipulation of (key)words has been widely explored in the literature, but in its several variants seems unable to fully solve the problem. Achieving better retrieval performance requires to go beyond simple lexical interpretation of the user queries, and pass through an understanding of their semantic content and aims.","refs":[]},{"text":"This work focuses on improving fruition of a DL content, by means of advanced techniques for document retrieval that try to overcome the aforementioned ambiguity of natural language. For this reason, we looked at the typical behavior of humans, when they take into account the possible meanings underlying the most prominent words that make up a text, and select the most appropriate one according to the context of the discourse. To carry out this approach, we used a well-known lexical taxonomy, and its extension to deal with domain categories, as a background knowledge.","refs":[]},{"text":"The rest of this paper is organized as follows. After a brief recall of previous work on Information Retrieval, with a particular attention to techniques aimed at overcoming lexical limitations, toward semantic aspects, Section 3 introduces a new proposal for semantic information retrieval based on taxonomic information. Then, Section 4 proposes an experimental evaluation of the proposed technique, with associated discussion and evaluation. Lastly, Section 5 concludes the paper and outlines open issues and future work directions.","refs":[]}]},{"title":"Related Work","paragraphs":[{"text":"Many works, aimed at building systems that tackle the Information Retrieval problem, exist in the literature. Most of such works are based on the ideas in [17], a milestone in this field. This approach, called Vector Space Model (VSM), represents a corpus of documents D, and the set of terms T appearing in those documents, as a T × D matrix, in which the (i, j)-th cell contains a weight representing the importance of the i-th term in the j-th document (usually computed according to the number and distribution of its occurrences both in that document and in the whole collection). This allows to compute the degree of similarity of a user query to any document in the collection, simply using any geometrical distance measure on that space. Much research has been spent on developing effective similarity measures and weighting schemes, and on variations of their implementations to enhance retrieval performance. Most similarity approaches [8,16,15] and weighting schemes [14,13,18] are based on inner product and cosine measure. Motivations came, on one hand, from the growth of the Web, and, on the other, from the success of some implementations in Web search engines. One limitation of these approaches is their considering a document only from a lexical point of view, which is typically affected by several kinds of linguistic tricks: e.g., synonymy (different words having similar meaning), and polysemy (words having many different meanings).","refs":[{"start":155,"end":159,"marker":"bibr","target":"#b16"},{"start":946,"end":949,"marker":"bibr","target":"#b7"},{"start":949,"end":952,"marker":"bibr","target":"#b15"},{"start":952,"end":955,"marker":"bibr","target":"#b14"},{"start":978,"end":982,"marker":"bibr","target":"#b13"},{"start":982,"end":985,"marker":"bibr","target":"#b12"},{"start":985,"end":988,"marker":"bibr","target":"#b17"}]},{"text":"More recently, techniques based on dimensionality reduction have been explored for capturing the concepts present in the collection. The main idea behind these techniques is mapping both the documents in the corpus and the queries into a lower dimensional space that explicitly takes into account the dependencies between terms. Then, the associations provided by the low-dimensional representation can be used to improve the retrieval or categorization performance. Among these techniques, Latent Semantic Indexing (LSI) [3] and Concept Indexing (CI) [9] can be considered relevant. The former is a statistical method that is capable of retrieving texts based on the concepts they contain, not just by matching specific keywords, as in previous approaches. It starts from a classical VSM approach, and applies Singular Value Decomposition (SVD) to identify latent concepts underlying the collection, and the relationships between these concepts and the terms/documents. Since the concepts are weighted by relevance, dimensionality reduction can be carried out by filtering out less relevant concepts, and the associated relationships. In this way new associations emerge between terms that occur in similar contexts, and hence query results may include documents that are conceptually similar in meaning to the query even if they don't contain the same words as the query. The latter approach, CI, carries out an indexing of terms using concept decomposition (CD) [4] instead of SVD (as in the LSI). It represents a collection of documents in k-dimensions by first clustering the documents in k groups using a variant of the k-means algorithm [11], and considering each group as potentially representing a different concept in the collection. Then, the cluster centroids are taken as the axes of the reduced k-dimensional space. Although LSI and CI have had much success (e.g., LSI was implemented by Google) for their ability to reduce noise, redundancy, and ambiguity, they still pose some questions. First of all, their high computational requirements prevent exploitation in many digital libraries. Moreover, since they rely on purely numerical and automatic procedures, the noisy and redundant semantic information must be associated with a numerical quantity that must be reduced or minimized by the algorithms. Last but not least, a central issue is the choice of the matrix dimension [2].","refs":[{"start":522,"end":525,"marker":"bibr","target":"#b2"},{"start":552,"end":555,"marker":"bibr","target":"#b8"},{"start":1465,"end":1468,"marker":"bibr","target":"#b3"},{"start":1644,"end":1648,"marker":"bibr","target":"#b10"},{"start":2393,"end":2396,"marker":"bibr","target":"#b1"}]}]},{"title":"A Domain-Based Approach","paragraphs":[{"text":"This section describes a proposal for a domain-based approach to information retrieval in digital libraries. In order to get rid of the constraints imposed by the syntactic level, we switch from the terms in the collection to their meaning by choosing a semantic surrogate for each word, relying on the support of external resources. At the moment, we exploit WordNet [5], and its extension WordNet Domains [12], as readily available general-purpose resources, although the proposed technique applies to any other taxonomy.","refs":[{"start":368,"end":371,"marker":"bibr","target":"#b4"},{"start":407,"end":411,"marker":"bibr","target":"#b11"}]},{"text":"The first step consists in off-line preprocessing the digital library in order to obtain, for each document, a list of representative keywords, to each of which the corresponding meaning will be associated later on. Using a system based on the DOMINUS framework [6], each document in the digital library is progressively split into paragraphs, sentences, and single words. In particular, the Stanford Parser [10] is used to obtain the syntactic structure of sentences, and the lemmas of the involved words. In this proposal, only nouns are considered and used to build a classical VSM weighted according to the TF*IDF scheme. In addition to stopwords, typically filtered out by all term-based approaches, we ignore adverbs, verbs and adjectives as well, because their representation in WordNet is different than that of nouns (e.g., verbs are organized in a separate taxonomy), and so different strategies must be defined for exploiting these lexical categories, which will be the subject of future work. More specifically, only those nouns that are identified as keywords for the given documents, according to the techniques embedded in DOMINUS, are considered. In order to be noise-tolerant and to limit the possibility of including non-discriminative and very general words (i.e., common words that are present in all domains) in the semantic representation of a document, it can be useful to rank each document keyword list by decreasing TF*IDF weight and to keep only the top items (say, 15) of each list.","refs":[{"start":262,"end":265,"marker":"bibr","target":"#b5"},{"start":408,"end":412,"marker":"bibr","target":"#b9"}]},{"text":"The next step consists in mapping each keyword in the document to a corresponding synset (i.e., its semantic representative) in WordNet. Since this task is far from being trivial, due to the typical polysemy of many words, we adopt the one-domain-per-discourse (ODD) assumption as a simple criterion for Word Sense Disambiguation (WSD): the meanings of close words in a text tend to refer to the same domain, and such a domain is probably the dominant one among the words in that portion of text. Hence, to obtain such synsets, we need to compute for each document the prevalent domain. First we take from WordNet all the synsets of each word, then, for each synset, we select all the associated domains in WordNet Domains. Then, each domain is weighted according to the density function presented in [1], depending on the number of domains to which each synset belongs, on the number of synsets associated to each word, and on the number of words that make up the sentence. Thus, each domain takes as weight the sum of all the weights of synsets associated to it, which results in a ranking of domains by decreasing weight. This allows to perform the WSD phase, that associates a single synset to each term by solving possible ambiguities using the domain of discourse (as described in Algorithm 1). Now, each document is represented by means of WordNet synsets instead of terms.","refs":[{"start":801,"end":804,"marker":"bibr","target":"#b0"}]},{"text":"The output of the previous step, for each document, is a list of pairs, made up of keywords and their associated synsets. All these synsets are partitioned into different groups using pairwise clustering, as shown in Algorithm 2: initially each synset makes up a different singleton cluster; then, the procedure works by iteratively finding the next pair of clusters to merge according to the complete Algorithm 1. Find \"best synset\" for a word link strategy (shown in Algorithm 3), based on the similarity function proposed in [7]:","refs":[{"start":528,"end":531,"marker":"bibr","target":"#b6"}]},{"text":"i and i are the two items (synsets in this case) under comparison; n represents the information carried by i but not by i ; l is the common information between i and i ; m is the information carried by i but not by i ; α is a weight that determines the importance of i with respect to i (0.5 means equal importance).","refs":[]},{"text":"In particular, we adopt a global approach based on all the information provided by WordNet on the two synsets, rather than on just one of their subsumers as in other measures in the literature. Indeed, we compute the distance between each pair (i ,i ) by summing up three applications of this formula, using different parameters n, m and l. The first component works in depth, and obtains the parameters by counting the number of common and different hypernyms between i and i . The second one works in breadth, and considers all the synsets with which i and i are directly connected by any relationship in WordNet, and then takes the number of common related synsets as parameter l, and the rest of synsets, related to only i or i , as parameters n and m. Lastly, the third component is similar to the second one, but it considers the inverse relationships (incoming links) in the computation. The considered relationships in the last two measures are:","refs":[]},{"text":"member meronimy: the latter synset is a member meronym of the former; substance meronimy: the latter synset is a substance meronym of the former; part meronimy: the latter synset is a part meronym of the former; similarity: the latter synset is similar in meaning to the former; antonym: specifies antonymous word; attribute: defines the attribute relation between noun and adjective synset pairs in which the adjective is a value of the noun; additional information: additional information about the first word can be obtained by seeing the second word; part of speech based : specifies two different relations based on the parts of speech involved; participle: the adjective first word is a participle of the verb second word; hyperonymy: the latter synset is a hypernym of the former.","refs":[]},{"text":"Example 1. To give an idea of the breadth-distance between S 1 and S 2 , let us consider the following hypothetical facts in WordNet: for the inverse component, where rel i represents one of the relationships listed above. In the former list, the set of synsets linked to S 1 is {S 3 , S 4 , S 5 } and the set of synsets linked to S 2 is {S 5 , S 6 }. Their intersection is {S 5 }, hence we have n = 2, l = 1, m = 1 as parameters for the similarity formula. In the latter list, the set of synsets linked to S 1 is {S 7 , S 8 , S 9 } and the set of synsets linked to S 2 is {S 9 , S 3 , S 8 }, yielding n = 1, l = 2, m = 1 as parameters for the similarity formula. The depth-distance component considers only hypernyms, and collects the whole sets of ancestors of S 1 and S 2 . Now, each document is considered in turn, and each of its keywords votes for the cluster to which the associated synset has been assigned (as shown in Algorithm 4). The contribution of such a vote is equal to the TF*IDF value established in the keyword extraction phase normalized on the sum of the weights of the chosen keywords. However, associating each document to only one cluster as its descriptor would be probably too strong an assumption. To smooth this, clusters are ranked in descending order according to the votes they obtained, and the document is associated to the first three clusters in this ranking. This closes the off-line preprocessing macro-phase, aimed at suitably partitioning the whole document collection according to different sub-domains. In our opinion, the pervasive exploitation of domains in this phase justifies the claim that the proposed approach is domain-based. Indeed, we wanted to find sets of similar synsets that might be usefully exploited as a kind of 'glue' binding together a sub-collection of documents that are consistent with each other. In this perspective, the obtained clusters can be interpreted as intensional representations of","refs":[]}]},{"title":"Algorithm 4. Association of documents to clusters","paragraphs":[{"text":"Input: D: the list of documents; W : the list of words of each document; S : the list of synsets of each document; C : the set of clusters.","refs":[]},{"text":"Output: set of clusters with the assigned documents.","refs":[]},{"text":"V : vector of votes, one for cluster. For each such query, a similarity evaluation is performed against each cluster that has at least one associated document, using the same complex similarity function as for clustering, that takes as input two sets of synsets (those in the query and those associated to the cluster), computes the distance between each possible pair of synsets taken from such sets, and then returns the maximum distance between all such pairs. This evaluation has a twofold objective: finding the combination of synsets that represents the best word sense disambiguation, and obtaining the cluster to which the involved words are most similar. The main motivation for which this phase considers only clusters that have at least one associated document is that, as already stated, clusters can be interpreted a set of descriptors for document subsets, and hence it makes sense keeping only those descriptors that are useful to identify the best set of documents according to the user's search. At this point, the best combination is used to obtain the list of clusters ranked by descending relevance, that can be used as an answer to the user's search. It should be pointed out that the ranked list is exploited, instead of taking just the best cluster, to avoid the omission of potentially useful results contained in positions following the top, this way losing information.","refs":[]}]},{"title":"Evaluation","paragraphs":[{"text":"To understanding the contribution of each step in the overall result, we used a collection made up of 200 documents obtained by randomly drawing 50 documents from 4 Wikipedia top-categories (general science, music, politics, religion). A structured version of the Wikipedia dump was obtained exploiting the Java Wikipedia Library [19]. A selection of queries, with a corresponding performance evaluation, is summarized in Table 1. For each query, the ranked list of most similar clusters was considered, and the top 10 documents were exploited for evaluating two performance measures: classical Precision P , expressing how many retrieved documents belong to the intended category of the query, and a looser version thereof P , considering as good outcomes also documents in categories that are compatible with the query, even if that was not in the user intention.","refs":[{"start":330,"end":334,"marker":"bibr","target":"#b18"},{"start":428,"end":429,"marker":"table","target":"#tab_3"}]},{"text":"A first consideration is that the decision to take several clusters (not just the top-ranked one) improved the result for all queries as regards true positives.","refs":[]},{"text":"In addition to the best 10 documents used for computing P and P , we have also reported (preceded by a '+' symbol) the number of immediately following documents that were nevertheless relevant for the query, which shows that good performance is not limited to top items only. Going beyond the purely numerical figures expressing the above measures, also a deeper insight into the specific cases reveals interesting aspects. For instance all results for query # 1 can be accepted as good, taking into account that a scientific perspective might correctly satisfy the user's search about the creation of the mankind, as well. Also for query # 2, it is quite agreeable that both traditions and folks are strictly related to religion as well as popular music. This motivated further analysis of some specific queries. In the following, for the sake of readability, when dealing with concepts both the synset code, and the set of associated terms, along with the corresponding gloss, will be reported. We will focus specifically on two sample queries purposely selected to help the reader understand the corresponding behavior.","refs":[]},{"text":"The former is ornaments and melodies. Only 2 combinations were found, among which the best one was:","refs":[]},{"text":"synset : 103169390; lemmas: decoration, ornament and ornamentation; gloss: something used to beautify; synset : 107028373; lemmas: air, line, melodic line, melodic phrase, melody, strain and tune; gloss: a succession of notes forming a distinctive sequence.","refs":[]},{"text":"This combination was recognized by the technique to be most similar to the following cluster:","refs":[]},{"text":"- It's easy to note that this cluster contains elements that are consistent with each other, a positive result that we may trace back to the decision of using a complete link pair-wise clustering, which is more restrictive in grouping items. In particular, this cluster represents an intensional description of 8 documents returned as first (or more relevant) outcomes, all talking about music. Furthermore, it is noteworthy that this query result satisfies the initial aim, of retrieving queryrelated documents that do not necessarily contain the terms that are present in the query. Thus, the technique is actually able to go beyond simple lexical interpretation of the user queries, retrieving documents in which no occurrence of the words forming the query are present, even in cases in which those words are not present at all in the entire collection. The latter sample is market and new economy. It is made up of 2 nouns, yielding a total of 20 combinations to be analyzed, of which the system recognized as the best one the following:","refs":[]},{"text":"synset : 108424951; lemmas: market; gloss: the customers for a particular product or service; synset : 100192613; lemmas: economy, saving; gloss: an act of economizing; reduction in cost.","refs":[]},{"text":"The most similar cluster was:","refs":[]},{"text":"synset : 108166552; lemmas: country, land, nation; gloss: the people who live in a nation or country;","refs":[]},{"text":"synset : 108179689; lemmas: populace, public, world; gloss: people in general considered as a whole; synset : 107965937; lemmas: domain, world; gloss: people in general, especially a distinctive group of people with some shared interest.","refs":[]},{"text":"Here we obtained 8 main results talking about politics. As in the former case, we can appreciate both the benefits of returning as a result the ranked list of clusters instead just the best one, and the consistency of the cluster elements. Again, it should be noted that, although very simple, the WSD technique based on the one-domain-per-discourse assumption was able to select a strongly consistent solution.","refs":[]}]},{"title":"Conclusions","paragraphs":[{"text":"This work proposed an approach to extract information from digital libraries trying to go beyond simple lexical matching, toward the semantic content underlying the actual aims of user queries. For all the documents in the corpus, after a keyword extraction phase, all keywords are disambiguated with a simple domain-driven WSD approach. The synsets obtained in this way are clustered, and each document is assigned to the cluster which contains more synsets related to its keywords. Then, given a user query, due to the typically low number of words in a query, that would affect the reliability of the WSD technique, all possible combinations of word meanings are considered, and the one that is most similar to a cluster is chosen. The outcome of the query presents the set of retrieved documents ranked by decreasing similarity of the associated cluster with such a combination. Preliminary experiments show that the approach can be viable, although extensions and refinements are needed to improve its effectiveness.","refs":[]},{"text":"In particular, the substitution of the ODD assumption with a more elaborated strategy for WSD might produce better results. Another issue regards incrementality: the current version of the approach requires a pre-processing, due to the underlying techniques for keyword extraction and clustering; this might be limiting when new documents are progressively included in the collection, a case that is very important in some digital libraries. Moreover, it might be interesting to evaluate the inclusion of adverbs, verbs and adjectives in order to improve the quality of the semantic representatives of the documents, and to explore other approaches to choose better intensional descriptions of each document.","refs":[]}]}],"tables":{"tab_3":{"heading":"Table 1 .","description":"Performance evaluation","rows":[["#","Query","Outcomes","P","P"],["","","[1 to 5] religion","",""],["1 creation of the mankind","[6 to 10] science","0.5","1.0"],["","","[+3] science","",""],["","","[1 to 8] music","",""],["2","traditions and folks","[9 to 10] religion","0.8","1.0"],["","","[+3] religion","",""],["","","[1 to 8] music","",""],["3 ornaments and melodies","[9] science","0.8","0.9"],["","","[10] religion","",""],["","","[1 to 2] religion","",""],["4 capitalism vs communism","[3 to 10] politics","0.8","0.8"],["","","[+4] politics","",""],["5 markets and new economy","[1 to 10] politics 1.0 [+1] politics","1.0"],["","","[1 to 2] science","",""],["","","[3] religion","",""],["6 gene structure and function","[4] politics","0.8","0.8"],["","","[5 to 10] science","",""],["","","[+2] science","",""]]}},"abstract":{"title":"Abstract","paragraphs":[{"text":"The current abundance of electronic documents requires automatic techniques that support the users in understanding their content and extracting useful information. To this aim, improving the retrieval performance must necessarily go beyond simple lexical interpretation of the user queries, and pass through an understanding of their semantic content and aims. It goes without saying that any digital library would take enormous advantage from the availability of effective Information Retrieval techniques to provide to their users. This paper proposes an approach to Information Retrieval based on a correspondence of the domain of discourse between the query and the documents in the repository. Such an association is based on standard general-purpose linguistic resources (WordNet and WordNet Domains) and on a novel similarity assessment technique. Although the work is at a preliminary stage, interesting initial results suggest to go on extending and improving the approach.","refs":[]}]}}