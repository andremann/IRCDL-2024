{"bibliography":{"title":"A novel model-based dewarping technique for advanced Digital Library systems","authors":[{"person_name":{"surname":"Pugliese","first_name":"Alessandro"},"affiliations":[{"department":null,"institution":"Università degli Studi di Bari \"Aldo Moro\"","laboratory":null}],"email":"alessandro.pugliese@uniba.it"},{"person_name":{"surname":"Pomes","first_name":"Silvestro"},"affiliations":[{"department":null,"institution":"Università degli Studi di Bari \"Aldo Moro\"","laboratory":null}],"email":null},{"person_name":{"surname":"Ferilli","first_name":"Stefano"},"affiliations":[{"department":null,"institution":"Università degli Studi di Bari \"Aldo Moro\"","laboratory":null}],"email":null},{"person_name":{"surname":"Redavid","first_name":"Domenico"},"affiliations":[{"department":null,"institution":"Artificial Brain S.r.l","laboratory":null}],"email":null}],"date":null,"ids":{"DOI":"10.1016/j.procs.2014.10.018","arXiv":null},"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"keywords":["Document processing","Layout analysis","Dewarping"],"citations":{"b0":{"title":"Dewarping of document images using coupled-snakes","authors":[{"person_name":{"surname":"Bukhari","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Shafait","first_name":"F"},"affiliations":[],"email":null},{"person_name":{"surname":"Breuel","first_name":"T"},"affiliations":[],"email":null}],"date":{"year":"2009","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b1":{"title":"Automatic Digital Document Processing and Management -Problems, Algorithms and Techniques","authors":[{"person_name":{"surname":"Ferilli","first_name":"S"},"affiliations":[],"email":null}],"date":{"year":"2011","month":null,"day":null},"ids":null,"target":null,"publisher":"Springer","journal":null,"series":null,"scope":null},"b2":{"title":"A histogram-based technique for auto matic threshold assessment in a run length smoothing-based algorithm","authors":[{"person_name":{"surname":"Ferilli","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Basile","first_name":"Tma"},"affiliations":[],"email":null},{"person_name":{"surname":"Esposito","first_name":"F"},"affiliations":[],"email":null}],"date":{"year":"2010","month":null,"day":null},"ids":null,"target":null,"publisher":"ACM","journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":349,"to_page":356}}},"b3":{"title":"A distance-based technique for non-manhattan layout analysis","authors":[{"person_name":{"surname":"Ferilli","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Biba","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Esposito","first_name":"F"},"affiliations":[],"email":null},{"person_name":{"surname":"Basile","first_name":"Tma"},"affiliations":[],"email":null}],"date":{"year":"2009","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":231,"to_page":235}}},"b4":{"title":"Dominus plus -document management intelligent universal system (plus)","authors":[{"person_name":{"surname":"Ferilli","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Esposito","first_name":"F"},"affiliations":[],"email":null},{"person_name":{"surname":"Basile","first_name":"Tma"},"affiliations":[],"email":null},{"person_name":{"surname":"Redavid","first_name":"D"},"affiliations":[],"email":null},{"person_name":{"surname":"Villani","first_name":"I"},"affiliations":[],"email":null}],"date":{"year":"2011","month":null,"day":null},"ids":null,"target":null,"publisher":"Springer","journal":"IRCDL","series":null,"scope":{"volume":249,"pages":{"from_page":123,"to_page":126}}},"b5":{"title":"Segmentation based recovery of arbitrarily warped document images","authors":[{"person_name":{"surname":"Gatos","first_name":"B"},"affiliations":[],"email":null},{"person_name":{"surname":"Pratikakis","first_name":"I"},"affiliations":[],"email":null},{"person_name":{"surname":"Ntirogiannis","first_name":"K"},"affiliations":[],"email":null}],"date":{"year":"2007","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":989,"to_page":993}}},"b6":{"title":"Comparison of some thresholding algorithms for text/background segmentation in difficult document images","authors":[{"person_name":{"surname":"Leedham","first_name":"Graham"},"affiliations":[],"email":null},{"person_name":{"surname":"Takru","first_name":"C"},"affiliations":[],"email":null},{"person_name":{"surname":"Tan","first_name":"K"},"affiliations":[],"email":null},{"person_name":{"surname":"Mian","first_name":"Jhn"},"affiliations":[],"email":null}],"date":{"year":"2003","month":null,"day":null},"ids":null,"target":null,"publisher":"Citeseer","journal":null,"series":null,"scope":{"volume":2,"pages":{"from_page":859,"to_page":864}}},"b7":{"title":"Computer and robot visi ón. Number v. 1 in Computer and Robot Vision","authors":[{"person_name":{"surname":"Haralick","first_name":"R"},"affiliations":[],"email":null},{"person_name":{"surname":"Shapiro","first_name":"L"},"affiliations":[],"email":null}],"date":{"year":"1992","month":null,"day":null},"ids":null,"target":null,"publisher":"Addison-Wesley Pub. Co","journal":null,"series":null,"scope":null},"b8":{"title":"Geometric rectification of camera-captured document images","authors":[{"person_name":{"surname":"Liang","first_name":"J"},"affiliations":[],"email":null},{"person_name":{"surname":"Dementhon","first_name":"D"},"affiliations":[],"email":null},{"person_name":{"surname":"Doermann","first_name":"D"},"affiliations":[],"email":null}],"date":{"year":"2008","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"Pattern Analysis and Machine Intelligence","series":null,"scope":{"volume":30,"pages":{"from_page":591,"to_page":605}}},"b9":{"title":"Perspective rectification of document images using fuzzy set and morphological operations","authors":[{"person_name":{"surname":"Lu","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Chen","first_name":"B"},"affiliations":[],"email":null},{"person_name":{"surname":"Ko","first_name":"C"},"affiliations":[],"email":null}],"date":{"year":"2005","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"Image Vision Comput","series":null,"scope":{"volume":23,"pages":{"from_page":541,"to_page":553}}},"b10":{"title":"Document flattening through grid modeling and regularization","authors":[{"person_name":{"surname":"Luand","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Tan","first_name":"C"},"affiliations":[],"email":null}],"date":{"year":"2006","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":1,"pages":{"from_page":971,"to_page":974}}},"b11":{"title":"A methodology for document image dewarping techniques performance evaluation","authors":[{"person_name":{"surname":"Stamatopoulos","first_name":"N"},"affiliations":[],"email":null},{"person_name":{"surname":"Gatos","first_name":"B"},"affiliations":[],"email":null},{"person_name":{"surname":"Pratikakis","first_name":"I"},"affiliations":[],"email":null}],"date":{"year":"2009","month":null,"day":null},"ids":null,"target":null,"publisher":"IEEE","journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":956,"to_page":960}}},"b12":{"title":"Grid-based modelling and correction of arbitrarily warped historical document images for large-scale digitisation","authors":[{"person_name":{"surname":"Yang","first_name":"P"},"affiliations":[],"email":null},{"person_name":{"surname":"Antonacopoulos","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Clausner","first_name":"C"},"affiliations":[],"email":null},{"person_name":{"surname":"Pletschacher","first_name":"S"},"affiliations":[],"email":null}],"date":{"year":"2011","month":null,"day":null},"ids":null,"target":null,"publisher":"ACM","journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":106,"to_page":111}}},"b13":{"title":"Arbitrary warped document image restoration based on segmentation and thin-plate splines","authors":[{"person_name":{"surname":"Zhang","first_name":"Y"},"affiliations":[],"email":null},{"person_name":{"surname":"Liu","first_name":"C"},"affiliations":[],"email":null},{"person_name":{"surname":"Ding","first_name":"X"},"affiliations":[],"email":null},{"person_name":{"surname":"Zou","first_name":"Y"},"affiliations":[],"email":null}],"date":{"year":"2008","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":1,"to_page":4}}},"b14":{"title":"Correcting document image warping based on regression of curved text lines","authors":[{"person_name":{"surname":"Zhang","first_name":"Z"},"affiliations":[],"email":null},{"person_name":{"surname":"Tan","first_name":"C"},"affiliations":[],"email":null}],"date":{"year":"2003","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":1,"pages":{"from_page":589,"to_page":593}}}},"sections":[{"title":"Introduction","paragraphs":[{"text":"When digitizing bound documents, often the pages cannot be completely flattened, causing an odd curvature in the spine region. While for documents made up of a few pages the distortion is limited to the blank margins, for documents having a medium or large number of pages it soon affects the page content. In particular, text lines look curved rather than straight, which might be a problem both for the final user (the digitized document is not aesthetically pleasing) and for automatic document processing (e.g., segmentation techniques based on rectangular white background pieces or OCR fail). The problem is worse if documents are not acquired using flatbed scanners. For instance, when digitizing ancient documents these devices would stress the pages and might damage unique and precious copies. In these cases, the use of (very costly) planetary (or \"orbital\") scanners is advised, where the document is open with its face up and is acquired by a camera placed above it. Still worse is the case in which the documents are photographed, because additional perspective distortions are introduced. In fact, this latter option is being given more and more attention, due to both the availability of cheap and compact high-resolution cameras, and it causing a reduced stress on the document under digitization (an extremely important requirement in the case of historical and fragile items). Fig. 1 shows a sample picture of an open book, from which one would like to restore the original page image and automatically recognize the corresponding content. In fact, the current availability of handy and low-cost but high-performance devices for image acquisition opens new landscapes for librarians and archivists, that may easily and quickly acquire documents, or parts thereof, to be included in their repositories or to be used for the recording activity. The LIBR@RIAN application, by Artificial Brain S.r.l., provides these professionals with a feature that allows them to take pictures of document pages using their smartphone, and directly send them to the Digital Library system for processing. For instance, pictures of document pages may be used by the system to rebuild the entire document and store it in the repository; or, photos of a book's front matter pages may be used to enrich the book's record with a picture of its title page and/or to automatically extract metadata and other kinds of information useful to help the librarian in his cataloging activity. In the latter case, OCR techniques must be applied to obtain machine-readable text from the picture. Dewarping is the term usually used to denote the process of compensating for the distortion introduced by the flat (2D) representation of spatially spread (3D) documents 2 . In this paper, we propose a model-based dewarping technique, that is integrated in LIBR@RIAN, aimed at overcoming some of the limitations of the existing literature through the use of a mixture of image processing and standard numerical analysis tools. Essential step of the method, and also its main point of novelty, is the construction of a curvilinear grid on each page of the document based on piecewise polynomial fitting and appropriate equipartition of a selection of its curved text lines. After recalling background notions and related work in next section, we describe and evaluate the proposed technique in Sections 3 and 4, respectively. Lastly, Section 5 concludes the paper and outlines anticipated directions for future work. ","refs":[{"start":1401,"end":1402,"marker":"figure","target":"#fig_0"},{"start":2751,"end":2752,"marker":"bibr","target":"#b1"}]}]},{"title":"Background and Related Work","paragraphs":[{"text":"The approaches proposed so far as potential solutions to restore the original flat version of a warped document have focused either on the hardware side (by exploiting dedicated acquisition devices that can better capture the 3D nature of the digitized object) or on the software one (by working on a standard picture of the item obtained using conventional means). The latter option is cheaper and ensures wider applicability, for which reason it gradually replaced the former. As already mentioned, one of the typical aims of dewarping is improving the outcome of OCR systems on distorted document pages. While OCR systems might be developed that can directly deal with the problem and fix it, a more straightforward and general approach consists in embedding a dewarping step in the normal pre-processing of the page to remove this kind of distortion before applying standard OCR.","refs":[]},{"text":"Two kinds of approaches can be distinguished for identifying the deformations to be corrected, one based on the geometrical features of the overall document image, and one based on the distortions detected on specific elements of the document, such as text lines (a quite significant and straightforward indicator of the problem). Techniques and tools that attempted to carry out the image dewarping task include polynomial fit 12 , splines 14 , snakes 1 , regression 15 , grid modeling 11 and even fuzzy sets 10 . The technique proposed by Gatos et al. 6 exploits segmentation of the document image and linear regression, leveraging the slope of single words. Liang et al. 9 attempt to recover 3D information from 2D images in order to rectify the document images. Yang et al. 13 propose a grid-based technique which is not based on curve fitting.","refs":[{"start":428,"end":430,"marker":"bibr","target":"#b11"},{"start":441,"end":443,"marker":"bibr","target":"#b13"},{"start":468,"end":470,"marker":"bibr","target":"#b14"},{"start":487,"end":489,"marker":"bibr","target":"#b10"},{"start":510,"end":512,"marker":"bibr","target":"#b9"},{"start":554,"end":555,"marker":"bibr","target":"#b5"},{"start":674,"end":675,"marker":"bibr","target":"#b8"},{"start":778,"end":780,"marker":"bibr","target":"#b12"}]}]},{"title":"Dewarping Technique","paragraphs":[{"text":"Before describing in details the approach we propose, we briefly elucidate, and motivate, its main points of novelty. As already mentioned, we propose a model-based approach. The decision of using a model to characterize the shape of curved text lines is driven by specific aims:","refs":[]},{"text":"• to be able to construct a curvilinear grid with meshes of arbitrary size via interpolation of the model's parameters (which will be coefficients of polynomials); • to extend the grid to zones of the image that did not give any contribution to the determination of the shape of the text lines (due to the absence of text for reasons such as presence of figures or formulas, short text lines or indentation); • to be able to \"smooth\" the parameters characterizing the model to cope with local errors.","refs":[]},{"text":"The model we choose is piecewise polynomial, and based on least squares spline fit. Indeed, using one single polynomial (e.g. a cubic one 12 ) throughout an entire text line did not seem to be an adequate choice for the specific case of interest. When dealing with bound documents, text lines seem to follow different patterns depending on the distance from the binding, and the use of piecewise polynomials allows to correctly follow each pattern. In order to resolve the document distortion along the direction perpendicular to the text lines, we perform an equipartition of the piecewise polynomial curves by arc-length, which proves to be quite effective. Methods based on the detection of the vertical texture flow field 9 do not always produce acceptable results, and relying on detection of the character's orientation can be misleading (we implemented it but observed erroneous behavior in presence of italic characters). The dewarping technique we propose only relies on 2D information of the document. It works in several steps on the input image. A graphical summarization of the two main steps applied to a sample document (left page in Figure 1) is shown in Figure 5. For images, we adopt the standard coordinate system , with the origin being on the upper-left point, x-coordinate pointing to the right and y-coordinate pointing down. The coordinates will be normalized, so that . First, the portion that contains text is extracted from the image using the RLSO automatic segmentation technique 4,3 provided by the DOMINUS plus platform 5 underlying LIBR@RIAN. Throughout the rest of the paper, we assume that most of the image contains text, and that the text lines have horizontal orientation. We split the description of the workflow into two stages, separating pre-processing steps from the actual dewarping.","refs":[{"start":138,"end":140,"marker":"bibr","target":"#b11"},{"start":726,"end":727,"marker":"bibr","target":"#b8"},{"start":1156,"end":1157,"marker":"figure","target":"#fig_0"},{"start":1178,"end":1179,"marker":"figure","target":"#fig_3"},{"start":1509,"end":1511,"marker":"bibr","target":"#b3"},{"start":1511,"end":1512,"marker":"bibr","target":"#b2"},{"start":1551,"end":1552,"marker":"bibr","target":"#b4"}]}]},{"title":"Pre-processing","paragraphs":[{"text":"The pre-processing work consists of the following steps: 1. Binarization; 2. Connected Components Labeling (CCL); 3. Tracing of the text lines.","refs":[]},{"text":"Binarization. For our purpose, a successful binarization should retain all the characters, not introduce noise, and remove the shade typically present along the spine of thick bound documents. We have used a modified version of Niblack's binarization method presented by Graham Leedham et al. 7 , specifically designed for bound documents.","refs":[{"start":293,"end":294,"marker":"bibr","target":"#b6"}]}]},{"title":"Ccl.","paragraphs":[{"text":"All the connected components are identified through the CCL algorithm outlined by Haralick and Shapiro 8 . Most of them will be characters (or part thereof), but some will be apostrophes, accents, punctuation marks, or even \"noise pixels\" introduced by the binarization algorithm. It is important to filter those last components out, as they would compromise the success of the tracing process (see below). The filtering technique we have adopted is based on the idea that the connected components that need to be filtered out are generally convex. In practice, we excluded from the tracing all components whose convex hull exceeds the number of pixels by more than 10%, and it proved to be effective (see Fig. 2). Tracing of the text lines. After all connected components are labeled and examined, we extract from each of them (which, at this point, is presumed to be a character) the upper pixels (where the value of is minimum) and lower ones. We collect those pixels in two sets and , that contain, respectively, the coordinates of all points that belong to the top and bottom portion of characters. Now the curved text lines can be traced: for each text line, the goal is to identify points in (respectively, ) that belong to the same base line (respectively, x line), see Fig. 3.a. This is done through an adaptation of the technique by Lu et al. 10 to our context. Let us briefly sketch this process for the case of base lines. Given points on the base line , the next point is added to the same base line according to the following criterion: is chosen as the closest point to among those in that lie inside a strip that is adaptively constructed to (locally) follow the direction of the curved text line (see Fig. 3.b) while excluding points that belong to other base lines. For more details, we refer the reader to Lu et al. 10 , where the case of straight text lines is discussed.","refs":[{"start":103,"end":104,"marker":"bibr","target":"#b7"},{"start":711,"end":712,"marker":"figure","target":"#fig_1"},{"start":1283,"end":1284,"marker":"figure","target":"#fig_2"},{"start":1353,"end":1355,"marker":"bibr","target":"#b9"},{"start":1723,"end":1724,"marker":"figure","target":"#fig_2"},{"start":1835,"end":1837,"marker":"bibr","target":"#b9"}]}]},{"title":"Dewarping","paragraphs":[{"text":"The actual dewarping consists of the following steps:","refs":[]},{"text":"1. x lines and base lines fit; 2. construction of the curvilinear grid; 3. homography.","refs":[]}]},{"title":"Fitting x lines and base lines.","paragraphs":[{"text":"As already mentioned, to fit the text lines we have chosen a least squares cubic splines model. To exemplify, consider a base line , and let the points that belong to , with having coordinates , for . Then, we seek a twice continuously differentiable function that minimizes and such that Note that each function is characterized by 12 coefficients, but only has 6 degrees of freedom due to the conditions of being at the breakpoints . The line fit is performed only for text lines that are sufficiently long. In our experience, fitting a short text line would most likely result in a polynomial curve that soon departs from the actual text line we seek to approximate. Once all the relevant base lines and x lines have been fitted (see left picture in Fig. 4.a), we have 24 vectors made of polynomial coefficients (12 for the base lines and 12 for the x lines). Each vector is further \"smoothed\" via a simple centered moving average, in order to smooth out the influence of erroneous fits. For instance, fitting a text line that exhibits indentation often results in aberrations towards the left margin of the page. The smoothing has to be performed with care in order not to introduce aberrations in case of uneven spacing of text lines. Finally, for every base line and x line we have traced, we detect the x coordinates where the text begins and ends. Also these data are \"smoothed\" to void the influence of indentation.","refs":[{"start":758,"end":759,"marker":"figure","target":null}]},{"text":"Curvilinear grid. The coordinates of the leftmost and rightmost point on the upper x line and lower base line are detected. Now, given an arbitrary user supplied integer , we create \"equidistant\" curves between the topmost x line and the bottommost base line. We do this by appropriately interpolating the coefficients of all the x lines and base lines detected so far. Now the x lines and base lines are dropped, and simply piecewise polynomial curves are retained. For each curve, the points where the text actually begins and ends are approximated by interpolation of the already possessed data. Then the relevant portion of each curve is partitioned into an","refs":[]},{"text":"original one. In the remaining 4 cases, the dewarped version provided slightly worse results (especially on character performance) than the original one. This can be explained by looking at the last 4 rows in Table 1, that reveal that most errors are squeezed in the recognition of the first and/or last line of the document. Considering only the rest of the document, performance is much better, both compared to the non-dewarped version and as absolute error rate. This behavior can be easily associated to the specificity of the technique, that when building and processing the grid sometimes cuts away part of the first and/or last line. Future work will take care of fixing this problem.Overall, the proposed dewarping technique was effective in significantly improving the performance of OCR reading on warped documents, showing marginal problems that are well-defined and can be likely tackled in next versions.  the document, that captures both its vertical and horizontal geometrical distortions, especially those caused by the presence of thick bindings.","refs":[{"start":215,"end":216,"marker":"table","target":null}]},{"text":"Throughout the experiments we performed, the technique proved to be very effective. Yet, we have identified some points that need improvement. For instance, the pre-processing stage is time-consuming (in the present implementation, it accounts for nearly 2/3 of the total computation time), and fairly sensitive to the choice of several parameters and thresholds. For this reason, we anticipate working on a new, greatly simplified, line tracing strategy that is expected to be more robust and not require most of the present pre-processing steps.","refs":[]}]}],"tables":{},"abstract":{"title":"Abstract","paragraphs":[{"text":"Digitization often introduces distortions in the form of odd perspective and curved text lines, especially toward the spine region of bound documents, that tamper both the creation of an acceptable digital reproduction of the document and the successful extraction of its textual content using OCR techniques. While already known for traditional acquisition means such as flatbed or planetary scanners, the problem gets even worse with the use of cameras, whose current widespread availability may open new opportunities for librarians and archivists. Dewarping is in charge of handling this kind of problems. This paper proposes a novel model-based dewarping method, aimed at solving some of the shortcomings of existing approaches through the use of a mixture of image processing and numerical analysis tools. The method is based on the construction of a curvilinear grid on each page of the document by means of piecewise polynomial fit and appropriate equipartition of a selection of its curved text lines. We show the method on sample documents and evaluate its impact on successful rate of OCR on a dataset.","refs":[]}]}}