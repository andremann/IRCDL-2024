{"bibliography":{"title":"Exploring domain and task adaptation of LamBERTa models for article retrieval on the Italian Civil Code","authors":[{"person_name":{"surname":"Simeri","first_name":"Andrea"},"affiliations":[{"department":"Dept. Computer Engineering, Modeling, Electronics, and Systems Engineering (DIMES)","institution":"University of Calabria","laboratory":null}],"email":"andrea.simeri@dimes.unical.it"},{"person_name":{"surname":"Tagarelli","first_name":"Andrea"},"affiliations":[{"department":"Dept. Computer Engineering, Modeling, Electronics, and Systems Engineering (DIMES)","institution":"University of Calabria","laboratory":null}],"email":"andrea.tagarelli@unical.it"}],"date":null,"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"keywords":["Law article retrieval","Domain adaptation","Artificial intelligence and law","Legal language models"],"citations":{"b0":{"title":"Artificial intelligence and law: An overview, 35 GA","authors":[{"person_name":{"surname":"Surden","first_name":"H"},"affiliations":[],"email":null}],"date":{"year":"2019","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"ST. U. L. REV","series":null,"scope":{"volume":null,"pages":{"from_page":1305,"to_page":1305}}},"b1":{"title":"BERT: pre-training of deep bidirectional transformers for language understanding","authors":[{"person_name":{"surname":"Devlin","first_name":"J"},"affiliations":[],"email":null},{"person_name":{"surname":"Chang","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Lee","first_name":"K"},"affiliations":[],"email":null},{"person_name":{"surname":"Toutanova","first_name":"K"},"affiliations":[],"email":null}],"date":{"year":"2019","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":4171,"to_page":4186}}},"b2":{"title":"Combining similarity and transformer methods for case law entailment","authors":[{"person_name":{"surname":"Rabelo","first_name":"J"},"affiliations":[],"email":null},{"person_name":{"surname":"Kim","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Goebel","first_name":"R"},"affiliations":[],"email":null}],"date":{"year":"2019","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":290,"to_page":296}}},"b3":{"title":"Neural legal judgment prediction in english","authors":[{"person_name":{"surname":"Chalkidis","first_name":"I"},"affiliations":[],"email":null},{"person_name":{"surname":"Androutsopoulos","first_name":"I"},"affiliations":[],"email":null},{"person_name":{"surname":"Aletras","first_name":"N"},"affiliations":[],"email":null}],"date":{"year":"2019","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":4317,"to_page":4323}}},"b4":{"title":"Easing legal news monitoring with learning to rank and BERT","authors":[{"person_name":{"surname":"Sanchez","first_name":"L"},"affiliations":[],"email":null},{"person_name":{"surname":"He","first_name":"J"},"affiliations":[],"email":null},{"person_name":{"surname":"Manotumruksa","first_name":"J"},"affiliations":[],"email":null},{"person_name":{"surname":"Albakour","first_name":"D"},"affiliations":[],"email":null},{"person_name":{"surname":"Martinez","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Lipani","first_name":"A"},"affiliations":[],"email":null}],"date":{"year":"2020","month":null,"day":null},"ids":null,"target":null,"publisher":"Springer","journal":null,"series":"Lecture Notes in Computer Science","scope":{"volume":12036,"pages":{"from_page":336,"to_page":343}}},"b5":{"title":"BERT-PLI: modeling paragraphlevel interactions for legal case retrieval","authors":[{"person_name":{"surname":"Shao","first_name":"Y"},"affiliations":[],"email":null},{"person_name":{"surname":"Mao","first_name":"J"},"affiliations":[],"email":null},{"person_name":{"surname":"Liu","first_name":"Y"},"affiliations":[],"email":null},{"person_name":{"surname":"Ma","first_name":"W"},"affiliations":[],"email":null},{"person_name":{"surname":"Satoh","first_name":"K"},"affiliations":[],"email":null},{"person_name":{"surname":"Zhang","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Ma","first_name":"S"},"affiliations":[],"email":null}],"date":{"year":"2020","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":3501,"to_page":3507}}},"b6":{"title":"LEGAL-BERT: the muppets straight out of law school","authors":[{"person_name":{"surname":"Chalkidis","first_name":"I"},"affiliations":[],"email":null},{"person_name":{"surname":"Fergadiotis","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Malakasiotis","first_name":"P"},"affiliations":[],"email":null},{"person_name":{"surname":"Aletras","first_name":"N"},"affiliations":[],"email":null},{"person_name":{"surname":"Androutsopoulos","first_name":"I"},"affiliations":[],"email":null}],"date":{"year":"2020","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b7":{"title":"JNLP team: Deep learning approaches for legal processing tasks in COLIEE 2021","authors":[{"person_name":{"surname":"Nguyen","first_name":"H"},"affiliations":[],"email":null},{"person_name":{"surname":"Nguyen","first_name":"P"},"affiliations":[],"email":null},{"person_name":{"surname":"Vuong","first_name":"T"},"affiliations":[],"email":null},{"person_name":{"surname":"Bui","first_name":"Q"},"affiliations":[],"email":null},{"person_name":{"surname":"Nguyen","first_name":"C"},"affiliations":[],"email":null},{"person_name":{"surname":"Dang","first_name":"T"},"affiliations":[],"email":null},{"person_name":{"surname":"Tran","first_name":"V"},"affiliations":[],"email":null},{"person_name":{"surname":"Nguyen","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Satoh","first_name":"K"},"affiliations":[],"email":null}],"date":{"year":"2021","month":null,"day":null},"ids":null,"target":"2106.13405","publisher":null,"journal":null,"series":null,"scope":null},"b8":{"title":"BERT-based ensemble methods with data augmentation for legal textual entailment in COLIEE statute law task","authors":[{"person_name":{"surname":"Yoshioka","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Aoki","first_name":"Y"},"affiliations":[],"email":null},{"person_name":{"surname":"Suzuki","first_name":"Y"},"affiliations":[],"email":null}],"date":{"year":"2021","month":null,"day":null},"ids":null,"target":null,"publisher":"ACM","journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":278,"to_page":284}}},"b9":{"title":"Don't stop pretraining: Adapt language models to domains and tasks","authors":[{"person_name":{"surname":"Gururangan","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Marasovic","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Swayamdipta","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Lo","first_name":"K"},"affiliations":[],"email":null},{"person_name":{"surname":"Beltagy","first_name":"I"},"affiliations":[],"email":null},{"person_name":{"surname":"Downey","first_name":"D"},"affiliations":[],"email":null},{"person_name":{"surname":"Smith","first_name":"N"},"affiliations":[],"email":null}],"date":{"year":"2020","month":null,"day":null},"ids":null,"target":null,"publisher":"ACL","journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":8342,"to_page":8360}}},"b10":{"title":"AlBERTo: Italian BERT Language Understanding Model for NLP Challenging Tasks Based on Tweets","authors":[{"person_name":{"surname":"Polignano","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Basile","first_name":"P"},"affiliations":[],"email":null},{"person_name":{"surname":"De Gemmis","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Semeraro","first_name":"G"},"affiliations":[],"email":null},{"person_name":{"surname":"Basile","first_name":"V"},"affiliations":[],"email":null}],"date":{"year":"2019","month":null,"day":null},"ids":null,"target":"-WS.org","publisher":null,"journal":null,"series":"CEUR Workshop Proceedings, CEUR","scope":{"volume":2481,"pages":null}},"b11":{"title":"Fixing comma splices in italian with BERT","authors":[{"person_name":{"surname":"Puccinelli","first_name":"D"},"affiliations":[],"email":null},{"person_name":{"surname":"Demartini","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"D'aoust","first_name":"R"},"affiliations":[],"email":null}],"date":{"year":"2019","month":null,"day":null},"ids":null,"target":"-WS.org","publisher":null,"journal":null,"series":"CEUR Workshop Proceedings, CEUR","scope":{"volume":2481,"pages":null}},"b12":{"title":"How \"BERTology\" Changed the State-of-the-Art also for Italian NLP","authors":[{"person_name":{"surname":"Tamburini","first_name":"F"},"affiliations":[],"email":null}],"date":{"year":"2020","month":null,"day":null},"ids":null,"target":"-WS.org","publisher":null,"journal":null,"series":"CEUR Workshop Proceedings, CEUR","scope":{"volume":2769,"pages":null}},"b13":{"title":"Unsupervised law article mining based on deep pre-trained language representation models with application to the Italian civil code","authors":[{"person_name":{"surname":"Tagarelli","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Simeri","first_name":"A"},"affiliations":[],"email":null}],"date":{"year":"2021","month":null,"day":null},"ids":{"DOI":"10.1007/s10506-021-09301-8","arXiv":null},"target":null,"publisher":null,"journal":"Artif. Intell. Law","series":null,"scope":{"volume":30,"pages":{"from_page":417,"to_page":473}}},"b14":{"title":"ITALIAN-LEGAL-BERT: A Pre-trained Transformer Language Model for Italian Law","authors":[{"person_name":{"surname":"Licari","first_name":"D"},"affiliations":[],"email":null},{"person_name":{"surname":"Comand√®","first_name":"G"},"affiliations":[],"email":null}],"date":{"year":"2022","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":"CEUR Workshop Proceedings, CEUR","scope":{"volume":3256,"pages":null}},"b15":{"title":"The Italian Civil Code network analysis","authors":[{"person_name":{"surname":"La Cava","first_name":"L"},"affiliations":[],"email":null},{"person_name":{"surname":"Simeri","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Tagarelli","first_name":"A"},"affiliations":[],"email":null}],"date":{"year":"2021","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":"CEUR Workshop Proceedings, CEUR-WS","scope":{"volume":2896,"pages":{"from_page":3,"to_page":16}}},"b16":{"title":"LawNet-Viz: A Web-based System to Visually Explore Networks of Law Article References","authors":[{"person_name":{"surname":"La Cava","first_name":"L"},"affiliations":[],"email":null},{"person_name":{"surname":"Simeri","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Tagarelli","first_name":"A"},"affiliations":[],"email":null}],"date":{"year":"2022","month":null,"day":null},"ids":{"DOI":"10.1145/3477495.3531668","arXiv":null},"target":null,"publisher":"ACM","journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":3300,"to_page":3305}}},"b17":{"title":"LamBERTa: Law Article Mining Based on Bert Architecture for the Italian Civil Code","authors":[{"person_name":{"surname":"Tagarelli","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Simeri","first_name":"A"},"affiliations":[],"email":null}],"date":{"year":"2022","month":null,"day":null},"ids":null,"target":"-WS.org","publisher":null,"journal":null,"series":"CEUR Workshop Proceedings, CEUR","scope":{"volume":3160,"pages":null}},"b18":{"title":"Semi-supervised methods for explainable legal prediction","authors":[{"person_name":{"surname":"Branting","first_name":"K"},"affiliations":[],"email":null},{"person_name":{"surname":"Weiss","first_name":"B"},"affiliations":[],"email":null},{"person_name":{"surname":"Brown","first_name":"B"},"affiliations":[],"email":null},{"person_name":{"surname":"Pfeifer","first_name":"C"},"affiliations":[],"email":null},{"person_name":{"surname":"Chakraborty","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Ferro","first_name":"L"},"affiliations":[],"email":null},{"person_name":{"surname":"Pfaff","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Yeh","first_name":"A"},"affiliations":[],"email":null}],"date":{"year":"2019","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":22,"to_page":31}}},"b19":{"title":"Explainable AI under contract and tort law: legal incentives and technical challenges","authors":[{"person_name":{"surname":"Hacker","first_name":"P"},"affiliations":[],"email":null},{"person_name":{"surname":"Krestel","first_name":"R"},"affiliations":[],"email":null},{"person_name":{"surname":"Grundmann","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Naumann","first_name":"F"},"affiliations":[],"email":null}],"date":{"year":"2020","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"Artif. Intell. Law","series":null,"scope":{"volume":28,"pages":{"from_page":415,"to_page":439}}},"b20":{"title":"Explaining the Predictions of Any Classifier","authors":[{"person_name":{"surname":"Ribeiro","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Singh","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Guestrin","first_name":"C"},"affiliations":[],"email":null}],"date":{"year":"2016","month":null,"day":null},"ids":{"DOI":"10.1145/2939672.2939778","arXiv":null},"target":null,"publisher":"ACM","journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":1135,"to_page":1144}}},"b21":{"title":"LEGAL-BERT: the muppets straight out of law school","authors":[{"person_name":{"surname":"Chalkidis","first_name":"I"},"affiliations":[],"email":null},{"person_name":{"surname":"Fergadiotis","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Malakasiotis","first_name":"P"},"affiliations":[],"email":null},{"person_name":{"surname":"Aletras","first_name":"N"},"affiliations":[],"email":null},{"person_name":{"surname":"Androutsopoulos","first_name":"I"},"affiliations":[],"email":null}],"date":{"year":"2020","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b22":{"title":"Legal Transformer Models May Not Always Help","authors":[{"person_name":{"surname":"Geng","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Lebret","first_name":"R"},"affiliations":[],"email":null},{"person_name":{"surname":"Aberer","first_name":"K"},"affiliations":[],"email":null}],"date":{"year":"2021","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null}},"sections":[{"title":"Introduction","paragraphs":[{"text":"Artificial Intelligence (AI) is increasingly used in the legal domain, which finds main motivations in the huge amount of information produced and in the involvement of different actors, such as legal professionals, law courts, legislators, law firms, and even citizens [1].","refs":[{"start":270,"end":273,"marker":"bibr","target":"#b0"}]},{"text":"Starting with BERT [2], deep contextualized pre-trained language models (PLMs) have emerged in the NLP field showing outstanding performance in several discriminative and generative tasks. BERT and BERT-like models have also represented a breakthrough for the legal domain, especially concerning classification problems (e.g., [3,4,5,6,7,8,9]).","refs":[{"start":19,"end":22,"marker":"bibr","target":"#b1"},{"start":327,"end":330,"marker":"bibr","target":"#b2"},{"start":330,"end":332,"marker":"bibr","target":"#b3"},{"start":332,"end":334,"marker":"bibr","target":"#b4"},{"start":334,"end":336,"marker":"bibr","target":"#b5"},{"start":336,"end":338,"marker":"bibr","target":"#b6"},{"start":338,"end":340,"marker":"bibr","target":"#b7"},{"start":340,"end":342,"marker":"bibr","target":"#b8"}]},{"text":"Early applications of such models to the legal domain include approaches that make PLMs adaptive to a specific legal data analysis task, i.e., they directly fine-tune a general-domain pretrained model to the task at hand. In contrast to such task-adaptive methods, domain-adaptive pre-training allows for deeply tailoring a pre-trained model to the domain of the target task [10]. To specialize a pre-trained model on the legal domain, there are two main strategies that stand as alternative to the direct application of an out-of-the-box pre-trained model for the downstream task, namely either to continue pre-training the model on a legal corpus, or to pre-train the model from scratch on a legal corpus.","refs":[{"start":375,"end":379,"marker":"bibr","target":"#b9"}]},{"text":"Our study in this paper concerns the above topic contextualized to the Italian legal domain.","refs":[]},{"text":"In this respect, it should be noted that, despite a number of Italian BERT models exist (e.g., [11,12,13]), they mostly refer to general-domain language. In particular, no study leveraging BERT for the Italian civil law has been proposed until LamBERTa [14], the first BERT-based framework for law article retrieval as a prediction problem. LamBERTa is in fact designed to learn prediction models by fine-tuning an Italian pre-trained BERT on the Italian Civil Code (ICC), and to answer natural language queries by retrieving the most relevant ICC articles. Much more recently, a new contribution to the Italian legal domain has been offered by the release of the first Italian BERT pre-trained on legal corpora, named ITALIAN-LEGAL-BERT [15].","refs":[{"start":95,"end":99,"marker":"bibr","target":"#b10"},{"start":99,"end":102,"marker":"bibr","target":"#b11"},{"start":102,"end":105,"marker":"bibr","target":"#b12"},{"start":253,"end":257,"marker":"bibr","target":"#b13"},{"start":738,"end":742,"marker":"bibr","target":"#b14"}]},{"text":"Given this premise, in this paper we aim to answer the following research questions:","refs":[]},{"text":"‚Ä¢ RQ1: How does the behavior of LamBERTa models change when fine-tuning a legal Italian BERT rather than a general-domain Italian BERT? ‚Ä¢ RQ2: What is the impact of injecting out-of-vocabulary legal terms into LamBERTa models during the fine-tuning stage? Does it depend on how such terms' representation is initialized? ‚Ä¢ RQ3: What aspects arise from the explanation of the different LamBERTa models through the interpretation of their predictions? ‚Ä¢ RQ4: Overall, is the combined effect of domain-adaptation and task-adaptation of a pre-trained Italian BERT model helpful to improve performance on the task of article retrieval from the Italian Civil Code?","refs":[]},{"text":"To answer the above questions, we provide the following main contributions. We advance research on AI-based NLP for the Italian legal domain by updating the current state-of-the-art of PLMs for law article retrieval as a prediction task. Starting over our early LamBERTa framework, we develop a new variant of LamBERTa, which makes it domain-adaptive besides task-adaptive; we accomplish this by designing LamBERTa so as to learn ICC article classification models through a fine-tuning of an Italian legal pre-trained BERT on the ICC (Section 4). We further investigate on the domain-adaptation of LamBERTa models by gaining insights into the effect of injecting into them a few domain-specific terms, selected from the target legal corpus, and previously unseen in the pre-trained model's vocabulary (Section 5). Moreover, we perform a qualitative analysis of the different LamBERTa models by explaining their underlying behaviors on a number of query instances (Section 6). We finally provide a discussion on our main findings that were drawn for our LamBERTa model variants based on an extensive collection of query sets at varying degrees of length and lexical complexity (Section 7).","refs":[]}]},{"title":"Background","paragraphs":[{"text":"In this section, we provide background concepts on the Italian Civil Code, the LamBERTa framework [14], and ITALIAN-LEGAL-BERT [15].","refs":[{"start":98,"end":102,"marker":"bibr","target":"#b13"},{"start":127,"end":131,"marker":"bibr","target":"#b14"}]}]},{"title":"The Italian Civil Code","paragraphs":[{"text":"The Italian Civil Code (ICC) is divided into six books, each of which provides rules for a specific theme in civil law. Book-1 (on Persons and the Family, articles 1-455) contains the discipline of For an analysis of the ICC article citation network and relating visualization tool, the interested reader might refer to [16] and [17].","refs":[{"start":320,"end":324,"marker":"bibr","target":"#b15"},{"start":329,"end":333,"marker":"bibr","target":"#b16"}]}]},{"title":"The LamBERTa framework","paragraphs":[{"text":"Figure 1 shows the conceptual architecture of LamBERTa [14,18]. The starting point is ITALIAN-XXL-UNCASED, a pre-trained Italian BERT model whose data source consists of a large Wikipedia dump, various texts from the OPUS corpora collection, and the Italian part of the OSCAR corpus; the final training corpus has a size of 81GB and 13 138 379 147 tokens. 1LamBERTa models are generated by fine-tuning the pre-trained BERT models on a sequence classification task (i.e., BERT with a single linear classification layer on top) given in input the articles of the ICC or a portion of it. This fine-tuning is accomplished by using a typical configuration of BERT for masked language modeling, with 12 attention heads and 12 hidden Notably, LamBERTa is flexible w.r.t. two peculiar modeling aspects: (i) the training-instance labeling scheme for a given set of ICC articles, and (ii) the learning approach. The former will be discussed later in Section 3, whereas the latter concerns the possibility of training models either on the individual books or on the entire ICC corpus; due to space limitations of this paper, we shall focus on the book-specific models.","refs":[{"start":7,"end":8,"marker":"figure","target":"#fig_0"},{"start":55,"end":59,"marker":"bibr","target":"#b13"},{"start":59,"end":62,"marker":"bibr","target":"#b17"},{"start":356,"end":357,"marker":null,"target":"#foot_0"}]},{"text":"Another feature of LamBERTa is the injection of previously unseen legal terms, selected from the task-specific corpus (i.e., ICC), that are out-of-vocabulary of the Italian pre-trained model. This way, the BERT tokenizer is enabled to recognize those terms appearing in the ICC, while fine-tuning on it, and hence, to avoid breaking them down into subwords. To select such terms to be added as new tokens in LamBERTa, the text of each book in the ICC is processed to remove Italian stopwords and filter out overly frequent terms (as occurring in more than 50% of the articles in ùê∑) as well as hapax terms. Table 1 reports the number of added tokens and the final number of tokens, for each book of the ICC.","refs":[{"start":612,"end":613,"marker":"table","target":"#tab_0"}]}]},{"title":"Training and evaluation data","paragraphs":[{"text":"One important model aspect of LamBERTa corresponds to the unsupervised article-labeling schemes that are used to produce a training set for each of the books in the ICC. This is not trivial since two main requirements need to be satisfied: (i) a one-to-one association must hold for classes and articles, since a LamBERTa model is designed to be a classifier at article level, i.e., class labels correspond to the articles in the book(s) covered by the model, and (ii) the entire ICC must be used to fully embed its knowledge. Therefore, a key issue is how to create as many training instances as possible for each article to make LamBERTa learn effectively. To this purpose, in [14], we defined different strategies for selecting and combining portions from each article to build the training set for any specific book, paying also attention to balance the contributions of each article, which are originally varying in length. Given a minimum number of training units per article (ùëöùëñùëõùëá ùëà ), by default set to 32, each of the article labeling schemes implements a round-robin (RR) method that iterates over replicas of the same group of training units per article until at least ùëöùëñùëõùëá ùëà are generated. The most effective scheme turned out to be the unigram with parameterized emphasis on the title, which builds the set of training units for each article as comprised of two subsets: the one containing the article's sentences with round-robin selection, and the other one containing only replicas of the article's title.","refs":[{"start":679,"end":683,"marker":"bibr","target":"#b13"}]},{"text":"In [14], LamBERTa models are assessed through extensive experiments by considering singlelabel and multi-label evaluation tasks, based on different types of queries, which vary by source, length and lexical characteristics. In this work, we shall use the following query-sets, each defined for any specific book ùêµ of the ICC: ‚Ä¢ (QT1) Randomly selected sentences from the articles of book ùêµ; ‚Ä¢ (QT2) Same as QT1, but the sentences are paraphrased through an Italian-English-Italian translation of the queries; ‚Ä¢ (QT3) Comments on the articles of book ùêµ, i.e., annotations about the interpretation of the meanings and law implications associated to an article (laleggepertutti.it); ‚Ä¢ (QT4) Case law decisions from the civil section of the Italian Court of Cassation that contains jurisprudential sentences associated with the articles of ùêµ.","refs":[{"start":3,"end":7,"marker":"bibr","target":"#b13"}]},{"text":"It should be noted that the above query sets represent different testbeds, whose \"difficulty\" is highly varying, from lower (QT1) to higher (QT3 and QT4). Due to space limitations of this paper, we refer the reader to [14] for further details on the characteristics of the query sets.","refs":[{"start":218,"end":222,"marker":"bibr","target":"#b13"}]},{"text":"As concerns the assessment criteria, here we consider single-label evaluation criteria only. For each article ùê¥ ùëñ , we start by measuring the precision for ùê¥ ùëñ (ùëÉ ùëñ ), i.e., the number of times (queries) ùê¥ ùëñ was correctly predicted out of all predictions of ùê¥ ùëñ , the recall for ùê¥ ùëñ (ùëÖ ùëñ ), i.e., the number of times (queries) ùê¥ ùëñ was correctly predicted out of all queries actually pertinent to ùê¥ ùëñ , and the F-measure for ùê¥ ùëñ (ùêπ ùëñ ). Then, we averaged over all articles to obtain the per-article average precision (ùëÉ ), recall (ùëÖ), micro-averaged F-measure (ùêπ ùúá ) as the average over all ùêπ ùëñ s, and macro-averaged F-measure (ùêπ ùëÄ ) as the harmonic mean of ùëÉ and ùëÖ. In addition, we account for the top-ùëò predictions and the position (rank) of the correct article in predictions: the former is the fraction of correct article labels that are found in the top-ùëò predictions (i.e., top-ùëò-probability results in response to each query), and averaging over all queries, which is the recall@ùëò (ùëÖ@ùëò); the latter is the mean reciprocal rank (ùëÄ ùëÖùëÖ) considering for each query the rank of the correct prediction over the classification probability distribution, and averaging over all queries.","refs":[]}]},{"title":"Table 2","paragraphs":[{"text":"LamBERTa-V1 vs. LamBERTa-V2 for all books of the ICC on book-sentence-queries (QT1), paraphrasedsentence-queries (QT2), comment-queries (QT3), and case-queries (QT4): Recall, Precision, micro-and macro-averaged F-measures, Recall@ùëò, and MRR. (Bold values correspond to the best model for each book, evaluation criterion, and query set)","refs":[]}]},{"title":"Rebuilding LamBERTa based on ITALIAN-LEGAL-BERT","paragraphs":[{"text":"To answer our first research question (RQ1), we develop a new version of LamBERTa by replacing the general-domain pre-trained Italian model (i.e., ITALIAN-XXL-UNCASED) with a legal-specific pre-trained Italian model (i.e., ITALIAN-LEGAL-BERT); recall that the latter model is the result of a further pre-training of the former, although on a cased version. We hereinafter refer to this version of LamBERTa as LamBERTa-V2, to distinguish from the original in [14] hereinafter denoted as LamBERTa-V1. Table 2 summarizes results of the comparison between the two versions based on their evaluation through all query sets. Note that results by original LamBERTa models are borrowed from [14]. 2At a first glance, it can be noticed that although there is no absolute winner, LamBERTa-V1 generally achieves better performance than LamBERTa-V2. For all query types, LamBERTa-V2 appears to lose more when evaluated on queries pertaining the largest books (i.e., Book-4 and Book-5). Moreover, regardless of the book, the gap of LamBERTa-V2 is particularly evident for the most difficult query sets, i.e., QT3 and QT4, which contain queries that are the most distant, both lexically and semantically, from the language used in the training instances. On","refs":[{"start":458,"end":462,"marker":"bibr","target":"#b13"},{"start":505,"end":506,"marker":"table","target":null},{"start":683,"end":687,"marker":"bibr","target":"#b13"},{"start":689,"end":690,"marker":null,"target":"#foot_1"}]}]},{"title":"Table 3","paragraphs":[{"text":"LamBERTa-V1-NoDST vs. LamBERTa-V2-NoDST for all books of the ICC on book-sentence-queries (QT1), paraphrased-sentence-queries (QT2), comment-queries (QT3), and case-queries (QT4): Recall, Precision, micro-and macro-averaged F-measures, Recall@ùëò, and MRR. (Bold values correspond to the best model for each book, evaluation criterion, and query set) average over all books, LamBERTa-V2 has indeed a percentage decrease of above 40% on case queries (QT4) and above 27% on comment queries (QT3); remarkably, while this holds for all criteria, the negative peaks are reached for the top-3 and top-10 predictions: -46.4% ùëÖ@3 and -48.8% ùëÖ@10, on QT4, and about 29% ùëÖ@3 and ùëÖ@10, on QT3. In light of the above results, it stands out that using a legal pre-trained model does not bring advantage over a general-domain pre-trained model to fine-tune on the downstream task of ICC article prediction, and actually the legal pre-trained model can often achieve worse performance.","refs":[]}]},{"title":"Investigating on the domain-specific token injection","paragraphs":[{"text":"Effect of token injection removal. A major goal of this work is to delve into the effect of the domain-specific token injection into LamBERTa models. To answer our RQ2, we first analyze the changes in the behavior of LamBERTa when no out-of-vocabulary tokens are added. We shall use suffix NoDST to distinguish this setting from the original one using token injection.","refs":[]},{"text":"Results obtained by LamBERTa-V1-NoDST and LamBERTa-V2-NoDST are shown in Table 3. First, we notice that the performance difference of LamBERTa-V2-NoDST w.r.t. LamBERTa-V1-NoDST is reduced, though still remaining negative, with the exception of QT4, where LamBERTa-V2-NoDST achieves average percentage increase of about 3% ùëÉ up to 9% ùëÖ. More interesting is to compare the obtained results against those in Table 2. The new setting leads to an improvement of both versions of LamBERTa in most cases, where the ITALIAN-LEGAL-BERT based version takes major benefits. More precisely, the two versions of LamBERTa improves slightly on QT1 and QT2, and more significantly on QT3. By contrast, on QT4, while LamBERTa-V2-NoDST achieves average percentage increase vs. LamBERTa-V2 (from 65% to about 90%), taking light advantage on other models in terms of ùëÖ, ùëÉ, ùêπ criteria, LamBERTa-V1-NoDST tends to be worse than the original LamBERTa-V1 which remains the absolute winner according to the ùëÖ@ùëò and ùëÄ ùëÖùëÖ criteria.","refs":[{"start":79,"end":80,"marker":"table","target":null},{"start":411,"end":412,"marker":"table","target":null}]},{"text":"Effect of embedding initialization for token injection. The above results prompted us to further investigate on the effect of injecting out-of-vocabulary legal terms into LamBERTa models, by focusing now on the initialization of the added tokens. In fact, it should be noted that in the original setting of LamBERTa, the selected domain-specific tokens are added to the Italian pre-trained tokenizer using a random initialization. Therefore, to provide a more exhaustive answer to our RQ2, we define an enhanced setting for the domain-specific tokens to be added. Our goal is to compute initial embeddings for the new tokens that are not random but incorporate proper knowledge of the legal language. One approach we tried is to initialize each word ùë§ to be added by getting the [CLS] output embedding computed when prompting the Italian pre-trained model, or alternatively ITALIAN-LEGAL-BERT, with just ùë§. Similarly, we tried by averaging the output embeddings of the tokens corresponding to the subwords of ùë§ detected by the BERT tokenizer. Unfortunately, in both cases, exploiting the output embeddings shows to be inappropriate, which might be due to the fact that these contextualized representations incorporate also the segment and position embeddings. Then, we shifted our attention to vectors extracted from the token embeddings matrix (i.e., the first level of BERT input representation). Given a word ùë§ to be added, we get the token embedding of each of its subwords (excluding [CLS] and [SEP] embeddings) and tried different pooling strategies. The one leading to the best results is initializing the ùë§'s embedding with the initial embedding of its root subword. This setting is hereinafter referred to as ReDST.","refs":[]},{"text":"Results for this new setting are reported in Table 4. A first remark that stands out is the benefit brought by this new setting of initialization of the injected tokens w.r.t. a random initialization, for both versions of LamBERTa. This holds always, with the exception of QT4 according to ùëÖ@ùëò and ùëÄ ùëÖùëÖ criteria, whereby LamBERTa-V1 is the absolute winner over all models. Besides, LamBERTa-V1-ReDST and LamBERTa-V2-ReDST actually perform comparably or better than LamBERTa-V1-NoDST and LamBERTa-V2-NoDST on QT1, and clearly better than LamBERTa-V1-NoDST and LamBERTa-V2-NoDST on QT4 according to ùëÖ@ùëò and ùëÄ ùëÖùëÖ.","refs":[{"start":51,"end":52,"marker":"table","target":null}]}]},{"title":"Explainability aspects","paragraphs":[{"text":"Like for any machine and deep learning models, explainability of PLMs is central to understand their solutions provided for a given NLP task. This becomes even more crucial when artificial intelligence meets a challenging field like law (e.g., [19,20]).","refs":[{"start":244,"end":248,"marker":"bibr","target":"#b18"},{"start":248,"end":251,"marker":"bibr","target":"#b19"}]},{"text":"Since our earlier study [14], we investigated explainability of our LamBERTa models, with a focus on how they form complex relationships between the textual tokens, and their distinctive attention patterns. In this paper, we take a different perspective, which is more suited for not hold for LamBERTa-V2. This may hint at a phenomenon of fresh-knowledge acquisition, by a model (i.e., LamBERTa-V1) that fine-tunes a general-domain one, against early-knowledge update exhibited by a model (i.e., LamBERTa-V2) that fine-tunes a domain-specific one.","refs":[{"start":24,"end":28,"marker":"bibr","target":"#b13"}]}]},{"title":"Discussion","paragraphs":[{"text":"Here we summarize main findings according to our previously stated research questions.","refs":[]},{"text":"Concerning RQ1, the behavior of LamBERTa models changes depending on the Italian pre-trained BERT model, especially on more difficult query testbeds. However, for the task of ICC article retrieval, fine-tuning on Italian legal pre-trained model does not bring particular advantage w.r.t. fine-tuning on Italian general-domain pre-trained model; therefore, original LamBERTa turns out to be preferable in most query scenarios. We point out this should not be surprising -after all, in [22], Legal-BERT was shown to achieve only slightly better performances than BERT, on both classification and entity recognition tasks. More importantly, our result is in line with some recent studies that demonstrated how domain adaptive pretraining leads to significant improvements only with low-resource downstream tasks [23].","refs":[{"start":484,"end":488,"marker":"bibr","target":"#b21"},{"start":809,"end":813,"marker":"bibr","target":"#b22"}]},{"text":"Injecting domain-specific tokens, i.e., out-of-vocabulary legal terms (RQ2) can provide low or no benefits in most cases; however, a non-random initialization of the new tokens to be injected significantly improves LamBERTa performance. Moreover, remarks drawn from the explanation of the LamBERTa models' predictions (RQ3) have confirmed that different domain-specific token-injection settings might be required to successfully address different query scenarios.","refs":[]},{"text":"To sum up, thus answering our RQ4, domain adaptation reveals to be less determinant than task adaptation for the task of article retrieval w.r.t. the Italian Civil Code. Nonetheless, there are specific situations relating to the underlying lexical/semantic aspects of the input queries that require to be handled by different variants of LamBERTa for successfully accomplishing the retrieval task.","refs":[]}]}],"tables":{"tab_0":{"heading":"Table 1","description":"Main statistics on the ICC books, and additional statistics depending on the domain-specific token injection (number of domain-specific tokens to inject into the pre-trained BERT vocabulary, final vocabulary size, and percentage increase)","rows":[["ICC","# articles # sentences # words # injected vocabulary","%"],["portion","","","","tokens","size","increase"],["Book-1","395","1979","32354","833","31935","2.68%"],["Book-2","345","1561","24520","698","31800","2.24%"],["Book-3","364","1619","25971","1072","32174","3.45%"],["Book-4","891","3595","50509","1383","32485","4.45%"],["Book-5","713","3937","75764","2048","33150","6.58%"],["Book-6","331","1453","25937","829","31931","2.66%"],["All","3039","14131","234945","3993","35095","12.84%"]]}},"abstract":{"title":"Abstract","paragraphs":[{"text":"This paper is concerned with AI-based NLP solutions to the law article retrieval problem, with application to the Italian legal domain and, particularly, to the Italian Civil Code. Based upon the current state-ofthe-art on this topic, we revise our early LamBERTa framework in a twofold way relating its domainadaptation feature: replacing the general-domain pre-trained model with a legal-specific one to fine-tune for the task of article retrieval, and delving into the injection of out-of-vocabulary legal terms into the models' tokenizer. Extensive experimental evaluation based on different collections of query sets, along with qualitative analysis on the models' prediction interpretability, have unveiled interesting findings about the combined effect of domain-and task-adaptation of an Italian BERT model on the task of law article retrieval.","refs":[]}]}}