{"bibliography":{"title":"Text Line Extraction in Handwritten Historical Documents","authors":[{"person_name":{"surname":"Capobianco","first_name":"Samuele"},"affiliations":[{"department":"Dipartimento di Ingegneria dell'Informazione","institution":"Università degli Studi di Firenze","laboratory":null}],"email":"samuele.capobianco@unifi.it"},{"person_name":{"surname":"Marinai","first_name":"Simone"},"affiliations":[{"department":"Dipartimento di Ingegneria dell'Informazione","institution":"Università degli Studi di Firenze","laboratory":null}],"email":"simone.marinai@unifi.it"}],"date":null,"ids":{"DOI":"10.1007/978-3-319-68130-6","arXiv":null},"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"keywords":[],"citations":{"b0":{"title":"Text line segmentation of historical documents: a survey","authors":[{"person_name":{"surname":"Likforman-Sulem","first_name":"L"},"affiliations":[],"email":null},{"person_name":{"surname":"Zahour","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Taconet","first_name":"B"},"affiliations":[],"email":null}],"date":{"year":"2007","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"Int. J. Doc. Anal. Recognit","series":null,"scope":{"volume":9,"pages":{"from_page":123,"to_page":138}}},"b1":{"title":"Complete system for text line extraction using convolutional neural networks and watershed transform","authors":[{"person_name":{"surname":"Pastor-Pellicer","first_name":"J"},"affiliations":[],"email":null},{"person_name":{"surname":"Afzal","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Liwicki","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Castro-Bleda","first_name":"M"},"affiliations":[],"email":null}],"date":{"year":"2016","month":"04","day":"14"},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":30,"to_page":35}}},"b2":{"title":"Gradient-based learning applied to document recognition","authors":[{"person_name":{"surname":"Lecun","first_name":"Y"},"affiliations":[],"email":null},{"person_name":{"surname":"Bottou","first_name":"L"},"affiliations":[],"email":null},{"person_name":{"surname":"Bengio","first_name":"Y"},"affiliations":[],"email":null},{"person_name":{"surname":"Haffner","first_name":"P"},"affiliations":[],"email":null}],"date":{"year":"1998","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":2278,"to_page":2324}}},"b3":{"title":"Fully convolutional networks for semantic segmentation","authors":[{"person_name":{"surname":"Long","first_name":"J"},"affiliations":[],"email":null},{"person_name":{"surname":"Shelhamer","first_name":"E"},"affiliations":[],"email":null},{"person_name":{"surname":"Darrell","first_name":"T"},"affiliations":[],"email":null}],"date":{"year":"2015","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":3431,"to_page":3440}}},"b4":{"title":"Integrated recognition, localization and detection using convolutional networks","authors":[{"person_name":{"surname":"Sermanet","first_name":"P"},"affiliations":[],"email":null},{"person_name":{"surname":"Eigen","first_name":"D"},"affiliations":[],"email":null},{"person_name":{"surname":"Zhang","first_name":"X"},"affiliations":[],"email":null},{"person_name":{"surname":"Mathieu","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Fergus","first_name":"R"},"affiliations":[],"email":null},{"person_name":{"surname":"Lecun","first_name":"Y"},"affiliations":[],"email":null}],"date":{"year":"2014","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b5":{"title":"DeepDocClassifier: document classification with deep convolutional neural network","authors":[{"person_name":{"surname":"Afzal","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Capobianco","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Malik","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Marinai","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Breuel","first_name":"T"},"affiliations":[],"email":null},{"person_name":{"surname":"Dengel","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Liwicki","first_name":"M"},"affiliations":[],"email":null}],"date":{"year":"2015","month":"08","day":"26"},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":1111,"to_page":1115}}},"b6":{"title":"Deep Learning","authors":[{"person_name":{"surname":"Goodfellow","first_name":"I"},"affiliations":[],"email":null},{"person_name":{"surname":"Bengio","first_name":"Y"},"affiliations":[],"email":null},{"person_name":{"surname":"Courville","first_name":"A"},"affiliations":[],"email":null}],"date":{"year":"2016","month":null,"day":null},"ids":null,"target":"http://www.deeplearningbook.org","publisher":"MIT Press","journal":null,"series":null,"scope":null},"b7":{"title":"Network in network","authors":[{"person_name":{"surname":"Lin","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Chen","first_name":"Q"},"affiliations":[],"email":null},{"person_name":{"surname":"Yan","first_name":"S"},"affiliations":[],"email":null}],"date":{"year":"2013","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b8":{"title":"Deep sparse rectifier neural networks","authors":[{"person_name":{"surname":"Glorot","first_name":"X"},"affiliations":[],"email":null},{"person_name":{"surname":"Bordes","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Bengio","first_name":"Y"},"affiliations":[],"email":null}],"date":{"year":"2011","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":"Journal of Machine Learning Research -Workshop and Conference Proceedings","scope":{"volume":15,"pages":{"from_page":315,"to_page":323}}},"b9":{"title":"Improving neural networks by preventing co-adaptation of feature detectors","authors":[{"person_name":{"surname":"Hinton","first_name":"G"},"affiliations":[],"email":null},{"person_name":{"surname":"Srivastava","first_name":"N"},"affiliations":[],"email":null},{"person_name":{"surname":"Krizhevsky","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Sutskever","first_name":"I"},"affiliations":[],"email":null},{"person_name":{"surname":"Salakhutdinov","first_name":"R"},"affiliations":[],"email":null}],"date":{"year":"2012","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b10":{"title":"Adam: a method for stochastic optimization","authors":[{"person_name":{"surname":"Kingma","first_name":"D"},"affiliations":[],"email":null},{"person_name":{"surname":"Ba","first_name":"J"},"affiliations":[],"email":null}],"date":{"year":"2014","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b11":{"title":"The HisDoc project. Automatic analysis, recognition, and retrieval of handwritten historical documents for digital libraries","authors":[{"person_name":{"surname":"Fischer","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Bunke","first_name":"H"},"affiliations":[],"email":null},{"person_name":{"surname":"Naji","first_name":"N"},"affiliations":[],"email":null},{"person_name":{"surname":"Savoy","first_name":"J"},"affiliations":[],"email":null},{"person_name":{"surname":"Baechler","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Ingold","first_name":"R"},"affiliations":[],"email":null}],"date":{"year":"2014","month":null,"day":null},"ids":null,"target":null,"publisher":"De Gruyter","journal":null,"series":null,"scope":{"volume":38,"pages":{"from_page":91,"to_page":106}}},"b12":{"title":"Record counting in historical handwritten documents with convolutional neural networks","authors":[{"person_name":{"surname":"Capobianco","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Marinai","first_name":"S"},"affiliations":[],"email":null}],"date":{"year":"2016","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null}},"sections":[{"title":"Introduction","paragraphs":[{"text":"Handwritten historical documents are widely available in Digital Libraries and archives. In particular, handwritten forms have been used in the past to record important personal facts. Significant examples are documents containing census information, birth records, and other public or private collections. The analysis of these documents is essential to reconstruct genealogies and to perform demographic studies. One first step to address the automatic transcription and information extraction from these documents is to localize and extract the text lines. Several techniques have been presented to address this task [1] and recent work focused on the use of artificial neural networks to find text lines in handwritten documents [2]. In this work we propose one solution to recognize the text lines and the separation between text lines using a Convolutional Neural Network. In the last few years, Convolutional Neural Networks [3] have been widely used to solve several tasks. In particular, these techniques have been used to segment [4] and localize objects [5] in Computer Vision as well as for page classification [6].","refs":[{"start":620,"end":623,"marker":"bibr","target":"#b0"},{"start":733,"end":736,"marker":"bibr","target":"#b1"},{"start":932,"end":935,"marker":"bibr","target":"#b2"},{"start":1040,"end":1043,"marker":"bibr","target":"#b3"},{"start":1065,"end":1068,"marker":"bibr","target":"#b4"},{"start":1123,"end":1126,"marker":"bibr","target":"#b5"}]},{"text":"In our research we deal with documents having different layouts. However, one common feature is that in each page the distance between text lines is quite regular. We can use this prior to initialize the document analysis; subsequently by using a suitably trained neural model we can localize the text lines and the line separators. As in most training-based approaches, the solution is based on two main steps.","refs":[]},{"text":"We first train a suitable CNN model. To this purpose, for each training page we estimate the average distance between text lines and use this information to label random patches used to train the model. Subsequently, we design a text line separator. For each test page we estimate the average distance between text lines and use this information to extract dense patches that are classified using the trained model. The classified patches are then combined to obtain an overall page segmentation.","refs":[]},{"text":"In the following sections we first describe in detail the initial step aimed at computing the adaptive patch dimension. We then analyze the proposed application scenario and discuss the experiments performed on two datasets.","refs":[]}]},{"title":"Adaptive Estimation of Patch Dimension","paragraphs":[{"text":"The proposed approach to solve the line segmentation is based on a patch-wise representation of the document images. The patch is expected to cover an area of the image containing text and background. In the pre-processing phase we therefore need to estimate an average distance between text lines. As we can notice from the example in Fig. 1a in our data there is a certain regularity in the distance between lines in each page and this regularity can be used to adapt the patch dimension in the page. Subsequently, we can label the patch as text line separator (if it covers the space, or other separators, between text lines) or as text (if it contains text in the middle of the patch area).","refs":[{"start":341,"end":343,"marker":"figure","target":"#fig_0"}]}]},{"title":"Estimate Average Text Line Distance","paragraphs":[{"text":"To estimate the average text line distance we measure the vertical projection profile of the page and then compute the distribution of the distance between peaks in the profile. The distribution is fitted into two clusters and the smallest value is used as estimate for the distance between text lines.","refs":[]},{"text":"In details the computation is composed by several steps. In the first step we remove black bands at the top and bottom borders of the page. Then, we compute the projection profile along the vertical dimension and find its local maxima. We then retain only the peaks larger than 20% of the maximum.","refs":[]},{"text":"Considering these peaks, we can compute the distribution of the distance between pairs of neighboring peaks. This set of values can be fitted by two centroids using the k-means algorithm. At the end we take the minimum centroid which defines the average distance between lines. The main steps are depicted in Fig. 1 where we show one input image, the computed projection profiles, and the peaks used to define the distance distribution.","refs":[{"start":314,"end":315,"marker":"figure","target":"#fig_0"}]},{"text":"With this procedure it is possible to estimate the text line distance in each page. This distance is subsequently used to define the size of the patches extracted from the page.","refs":[]}]},{"title":"Patch Definition","paragraphs":[{"text":"After the computation of the average text line distance, ĥ, we define the patch size (height, width) as ( ĥ • √ 2, ĥ √ 2 ). The patch height is defined in order to  capture almost two text lines with one separator (space or other) in between.","refs":[]},{"text":"The patch size has been chosen after some preliminary tests. In general we can observe that the classification of variable-size patches is a good option to be independent from the textline distance in the input image. A dense patch extraction with overlapping can cover all the lines providing a good hint on the presence of text line separators.","refs":[]}]},{"title":"The Proposed Method","paragraphs":[{"text":"Taking into account the physical structure of the documents we can define two types of areas: the text line and the separator that is the area between two contiguous text lines. For the subsequent steps it is also useful to define the median line and the separator line. The median line is the middle line between the top profile of the text and the bottom profile. The separator line is the middle line between two consecutive median lines in the text area. We can see these lines in Fig. 2, in particular we show three text lines extracted from one page representing the top and bottom profiles with black lines, the separator and median lines in blue and red. These two types of lines can be used to label the extracted patches and subsequently train the CNN-based line segmentation. However, this information is not always available as groundtruth for the datasets. We therefore need to compute this representation from the available ground truth data.","refs":[{"start":490,"end":491,"marker":"figure","target":"#fig_1"}]},{"text":"For example, the ground truth of the Saint Gall dataset (more details in Sect. 4.1) is available as pixel-label images defining the text area. In this case we can compute the median line for each text line starting from the pixel level information. After that, we compute the separator line as the mid point between two consecutive median lines. In Fig. 6 we can see one example of the Saint Gall dataset and the corresponding ground truth information. Using this information we can compute the previously defined guidelines to label the extracted patches.","refs":[{"start":354,"end":355,"marker":"figure","target":"#fig_5"}]},{"text":"Unfortunately, sometimes the ground truth data does not contain this pixellevel labeling. In these cases we manually define only the separator lines and use this information to identify also text lines. More details on how to extract and label patches are described in Sect. 4. Using the document structure and the ground truth information, we can label each patch as separator, text, or background. The assigned class depends on the area covered by the patch on the document.","refs":[]}]},{"title":"Network Architecture","paragraphs":[{"text":"Convolutional Neural Networks (CNN) are a particular type of Artificial Neural Networks that consist of alternated convolutional layers and spatial pooling layers [7]. The convolutional layers generate feature maps by linear convolutional filters followed by nonlinear activation functions (e.g. rectifier, sigmoid, tanh). In our work we extend the Network in Network (NIN) model proposed by Lin et al. [8]. This architecture (Fig. 4) consists of stacked blocks where each block is composed by a convolution operator followed by a Multi Layer Perceptron (MLP) (Fig. 3). This latter structure is called mlpconv layer.","refs":[{"start":163,"end":166,"marker":"bibr","target":"#b6"},{"start":403,"end":406,"marker":"bibr","target":"#b7"},{"start":432,"end":433,"marker":"figure","target":"#fig_3"},{"start":566,"end":567,"marker":"figure","target":"#fig_2"}]},{"text":"The mlpconv maps the input area to the output feature vector with a multilayer perceptron (MLP) consisting of two fully connected layers with nonlinear activation functions. The MLP is shared among all local receptive fields. The feature maps are computed by sliding the MLP over the input similarly to CNNs and are then fed into the next layer. We show one example of mlpconv layer in Fig. 3. This layer computes a convolutional transformation with a defined kernel dimension followed by two inner product transformations. Each transformation layer is followed by a ReLU [9] layer. The results are computed considering the same number of feature maps for each transformation in the mlpconv.","refs":[{"start":391,"end":392,"marker":"figure","target":"#fig_2"},{"start":572,"end":575,"marker":"bibr","target":"#b8"}]},{"text":"In this work we use a modified version of the original architecture. In the first mlpconv layer we have a convolutional layer with a kernel dimension of 3 × 7 × 7 computing 96 feature maps with stride 2 and padding 3. In the second mlpconv layer we have a convolutional layer with a kernel dimension 96 × 5 × 5 computing 256 feature maps with stride 1 and padding 2. In the third mlpconv layer we have a convolutional layer with a kernel dimension 256 × 3 × 3 computing 384 feature maps with stride 1 and padding 1. After that, we have a dropout layer [10] which randomly selects 50% of input neurons during the training phase. In the fourth mlpconv layer we have a convolutional layer with a kernel dimension 384 × 3 × 3 computing 1024 feature maps with stride 1 and padding 1. For the last mlpconv layer, we define the number of feature maps according to the number of classes in our task. We compute the global average pooling over the previous feature maps providing them to the softmax layer which models the distribution over the class labels. We use the stochastic gradient descent to train a model initialized with random weight values. The training is stopped on the best classification accuracy on a validation set. We use the Adam optimization algorithm [11] with a fixed learning rate 0.0001 with momentum (0.9, 0.999) for the training phase.","refs":[{"start":552,"end":556,"marker":"bibr","target":"#b9"},{"start":1265,"end":1269,"marker":"bibr","target":"#b10"}]},{"text":"The overall architecture can be seen in Fig. 4. The model is composed by four mlpconv and pooling layers. In orange, we denote the mlpconv transformation, in green the pooling transformation layer. For the last mlpconv we compute 1024 feature maps until the second inner product, instead, for the last inner product we have the number of neurons according to the output classes.","refs":[{"start":45,"end":46,"marker":"figure","target":"#fig_3"}]}]},{"title":"Experiments","paragraphs":[{"text":"In this section we describe the experiments for localizing text lines using two different dataset.","refs":[]}]},{"title":"Data Corpus","paragraphs":[{"text":"It is no easy to find public databases for the tasks addressed in this work. The HisDoc [12] project presents some datasets for developing handwriting recognition systems. In particular, we use the Saint Gall dataset that consists of 60 pages scanned from one medieval manuscript written in Latin (Fig. 5a). The ground truth information is gathered from the Hisdoc Divadia site1 . The proposed approach has been tested also on handwritten documents from a private collection provided by the Ancestry company (the global leader in family history and consumer genomics). This database consists of 54 structured documents written in English by several writers (Fig. 5b). The line ground truth has been produced using one tool specifically developed to manually segment the lines. The images in this collection have been also used for performing experiments aiming at automatically counting the number of records in handwritten pages [13].","refs":[{"start":88,"end":92,"marker":"bibr","target":"#b11"},{"start":303,"end":305,"marker":"figure","target":"#fig_4"},{"start":377,"end":378,"marker":null,"target":"#foot_0"},{"start":663,"end":665,"marker":"figure","target":"#fig_4"},{"start":930,"end":934,"marker":"bibr","target":"#b12"}]}]},{"title":"Extracted Patches","paragraphs":[{"text":"As previously mentioned, the first step is to extract the patches used to train the model. This extraction is based on the ground truth of the training set. As previously mentioned, for this research we consider two datasets with two different ground truth information.","refs":[]},{"text":"In the Saint Gall dataset the ground truth is very detailed since it contains one XML file for each document. The XML file contains one pixel-wise representation for each text line. One example of this dataset is presented in Fig. 6 where we can see an input document, a pixel-wise representation of the ground truth for each text line, the computed median text line, and the text separator.","refs":[{"start":231,"end":232,"marker":"figure","target":"#fig_5"}]},{"text":"The ground truth produced for the Ancestry dataset defines only the separator line between two adjacent text lines. In Fig. 7 we can see one image example and its ground truth image.","refs":[{"start":124,"end":125,"marker":"figure","target":"#fig_6"}]},{"text":"Considering these labeled datasets we can build a large patch dataset to learn the CNN. We have seen how to compute the average distance between text lines; using this information we can define a patch with a suitable dimension  approximately the same number of patches for each class. We then scale each extracted patch to a fixed size of 128 × 64 pixels.","refs":[]},{"text":"In the two experiments for each class we considered 100,000 training patches and 30,000 validation patches. The test patches are extracted from the test image as described in the following Section. For instance in the Saint Gall dataset the number of patches for each pages are around 600,000 (the number is not fixed, since the patches are extracted at random from background areas).","refs":[]}]},{"title":"Textline Identification","paragraphs":[{"text":"The textline identification is made by extracting dense patches on the test images. Using a sliding window with the estimated dimensions, we classify the patches. The model used gives us an estimate of the probability for each class. This information can be used to infer the position of the lines in the image. We move the sliding window on the image with a stride size equal to 10% of the patch width and 1% of the patch height. In this way, we are able to compute several scores for the same position. We then compute the average probability along the horizontal direction with respect to the patch dimension. This approach is depicted in Fig. 8 where we describe the test procedure. In particular, we can see how we extract dense patches form the input image and also how we compute the probability score for the whole page.","refs":[{"start":647,"end":648,"marker":"figure","target":"#fig_7"}]}]},{"title":"Preliminary Results","paragraphs":[{"text":"We present in the following the preliminary results obtained for the two datasets.","refs":[]},{"text":"In the Ancestry dataset, the ground truth defines only the text line separators, therefore we can consider only two classes: text separator and no text separator (background or text). The architecture used for this task is the same presented before. However, in the last mlpconv we have one output with only two feature maps. After the test step we can compute the results as we can see in Fig. 9. In the figure we present the results of the model considering only the probability value after the softmax function on the text line separator concept. The testing phase is made moving a window over the input image, therefore we have a probability score for each position. We are able to define the text areas computing the average probability score as we can see in Fig. 9b. This probability  value represents the text line separator presence. The final result will be a probability map where we can discover the separator position. In this case, having only two classes, we want to discriminate better the result and using a threshold on this probability score, we can predict the text line separator as shown in Fig. 9c. In the figure, where we used a threshold of 0.7 to define the text line separators, we can see the probability map with the corresponding prediction results.","refs":[{"start":395,"end":396,"marker":"figure","target":"#fig_8"},{"start":770,"end":772,"marker":"figure","target":"#fig_8"},{"start":1118,"end":1120,"marker":"figure","target":"#fig_8"}]},{"text":"In the HisDoc project, we have more detailed ground truth where we can define median text lines and separator text lines. Using this information, we can use the proposed solution to predict three classes: text, separator and background. With this information, we can infer the text and separator line positions. An example of this approach is shown in Fig. 10 where we can see two different results, one for each class. In particular, in Fig. 10b we present the area for the separator class, while in Fig. 10c we present the area for the text line class.","refs":[{"start":357,"end":359,"marker":"figure","target":"#fig_9"},{"start":443,"end":446,"marker":"figure","target":"#fig_9"},{"start":506,"end":509,"marker":"figure","target":"#fig_9"}]}]},{"title":"Conclusions","paragraphs":[{"text":"In this paper we presented our preliminary work on the use of Convolutional Neural Networks to perform text line segmentation in historical documents. With these preliminary results we obtained a good starting point to extract the text lines in two different collections containing manuscript records. These results have been obtained considering a training made on small datasets and relatively homogeneous data. In future experiments, we will check the model with more data and we will investigate about possible changes in the model to improve the results.","refs":[]}]}],"tables":{},"abstract":{"title":"Abstract","paragraphs":[{"text":"We present a novel approach for the extraction of text lines in handwritten documents using a Convolutional Neural Network to label document image patches as text lines or separators. We first process the document to identify the most suitable patch size on the basis of an overall text line distance estimation. Using this information, we then extract several patches to train the CNN model. Finally, we use the trained model to segment text lines. We have evaluated this technique on the public database Saint Gall and on a private collection provided by the Ancestry company.","refs":[]}]}}