{"bibliography":{"title":"Improving Classification and Retrieval of Illuminated Manuscript with Semantic Information","authors":[{"person_name":{"surname":"Grana","first_name":"Costantino"},"affiliations":[{"department":null,"institution":"Università degli Studi di Modena e Reggio Emilia","laboratory":null}],"email":null},{"person_name":{"surname":"Borghesani","first_name":"Daniele"},"affiliations":[{"department":null,"institution":"Università degli Studi di Modena e Reggio Emilia","laboratory":null}],"email":null},{"person_name":{"surname":"Cucchiara","first_name":"Rita"},"affiliations":[{"department":null,"institution":"Università degli Studi di Modena e Reggio Emilia","laboratory":null}],"email":null}],"date":null,"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"keywords":[],"citations":{"b0":{"title":"AGORA: the interactive document image analysis tool of the BVH project","authors":[{"person_name":{"surname":"Ramel","first_name":"J"},"affiliations":[],"email":null},{"person_name":{"surname":"Busson","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Demonet","first_name":"M"},"affiliations":[],"email":null}],"date":{"year":"2006","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":145,"to_page":155}}},"b1":{"title":"Madonne: Document Image Analysis Techniques for Cultural Heritage Documents","authors":[{"person_name":{"surname":"Ogier","first_name":"J"},"affiliations":[],"email":null},{"person_name":{"surname":"Tombre","first_name":"K"},"affiliations":[],"email":null}],"date":{"year":"2006","month":null,"day":null},"ids":null,"target":null,"publisher":"Oesterreichische Computer Gesellschaft","journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":107,"to_page":114}}},"b2":{"title":"Document Images Analysis Solutions for Digital libraries","authors":[{"person_name":{"surname":"Bourgeois","first_name":"Le"},"affiliations":[],"email":null},{"person_name":{"surname":"Trinh","first_name":"F"},"affiliations":[],"email":null},{"person_name":{"surname":"Allier","first_name":"E"},"affiliations":[],"email":null},{"person_name":{"surname":"Eglin","first_name":"B"},"affiliations":[],"email":null},{"person_name":{"surname":"Emptoz","first_name":"V"},"affiliations":[],"email":null}],"date":{"year":"2004","month":null,"day":null},"ids":null,"target":null,"publisher":"IEEE Computer Society","journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":2,"to_page":24}}},"b3":{"title":"One-shot learning of object categories","authors":[{"person_name":{"surname":"Fei-Fei","first_name":"L"},"affiliations":[],"email":null},{"person_name":{"surname":"Fergus","first_name":"R"},"affiliations":[],"email":null},{"person_name":{"surname":"Perona","first_name":"P"},"affiliations":[],"email":null}],"date":{"year":"2006","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"IEEE Transactions on Pattern Analysis and Machine Intelligence","series":null,"scope":{"volume":28,"pages":{"from_page":594,"to_page":594}}},"b4":{"title":"Learning to detect objects in images via a sparse, partbased representation","authors":[{"person_name":{"surname":"Agarwal","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Awan","first_name":"A"},"affiliations":[],"email":null}],"date":{"year":"2004","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"IEEE Transactions on Pattern Analysis and Machine Intelligence","series":null,"scope":{"volume":26,"pages":{"from_page":1475,"to_page":1490}}},"b5":{"title":"Video google: a text retrieval approach to object matching in videos","authors":[{"person_name":{"surname":"Sivic","first_name":"J"},"affiliations":[],"email":null},{"person_name":{"surname":"Zisserman","first_name":"A"},"affiliations":[],"email":null}],"date":{"year":"2003","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":2,"pages":{"from_page":1470,"to_page":1477}}},"b6":{"title":"Visual categorization with bags of keypoints","authors":[{"person_name":{"surname":"Dance","first_name":"C"},"affiliations":[],"email":null},{"person_name":{"surname":"Csurka","first_name":"G"},"affiliations":[],"email":null},{"person_name":{"surname":"Fan","first_name":"L"},"affiliations":[],"email":null},{"person_name":{"surname":"Willamowski","first_name":"J"},"affiliations":[],"email":null},{"person_name":{"surname":"Bray","first_name":"C"},"affiliations":[],"email":null}],"date":{"year":"2004","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":1,"to_page":22}}},"b7":{"title":"Object recognition from local scale-invariant features","authors":[{"person_name":{"surname":"Lowe","first_name":"D"},"affiliations":[],"email":null}],"date":{"year":"1999","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":2,"pages":{"from_page":1150,"to_page":1157}}},"b8":{"title":"A thousand words in a scene","authors":[{"person_name":{"surname":"Quelhas","first_name":"P"},"affiliations":[],"email":null},{"person_name":{"surname":"Monay","first_name":"F"},"affiliations":[],"email":null},{"person_name":{"surname":"Odobez","first_name":"J"},"affiliations":[],"email":null},{"person_name":{"surname":"Gatica-Perez","first_name":"D"},"affiliations":[],"email":null},{"person_name":{"surname":"Tuytelaars","first_name":"T"},"affiliations":[],"email":null}],"date":{"year":"2007","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"IEEE Transactions on Pattern Analysis and Machine Intelligence","series":null,"scope":{"volume":29,"pages":{"from_page":1575,"to_page":1589}}},"b9":{"title":"Sampling strategies for bag-of-features image classification","authors":[{"person_name":{"surname":"Nowak","first_name":"E"},"affiliations":[],"email":null},{"person_name":{"surname":"Jurie","first_name":"F"},"affiliations":[],"email":null},{"person_name":{"surname":"Triggs","first_name":"B"},"affiliations":[],"email":null}],"date":{"year":"2006","month":null,"day":null},"ids":null,"target":null,"publisher":"Springer","journal":null,"series":null,"scope":{"volume":3954,"pages":{"from_page":490,"to_page":503}}},"b10":{"title":"Affine-invariant local descriptors and neighborhood statistics for texture recognition","authors":[{"person_name":{"surname":"Lazebnik","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Schmid","first_name":"C"},"affiliations":[],"email":null},{"person_name":{"surname":"Ponce","first_name":"J"},"affiliations":[],"email":null}],"date":{"year":"2003","month":null,"day":null},"ids":null,"target":null,"publisher":"IEEE Computer Society","journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":649,"to_page":649}}},"b11":{"title":"Real-time bag of words, approximately","authors":[{"person_name":{"surname":"Uijlings","first_name":"J"},"affiliations":[],"email":null},{"person_name":{"surname":"Smeulders","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Scha","first_name":"R"},"affiliations":[],"email":null}],"date":{"year":"2009","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b12":{"title":"Early versus late fusion in semantic video analysis","authors":[{"person_name":{"surname":"Snoek","first_name":"C"},"affiliations":[],"email":null},{"person_name":{"surname":"Worring","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Smeulders","first_name":"A"},"affiliations":[],"email":null}],"date":{"year":"2005","month":null,"day":null},"ids":null,"target":null,"publisher":"ACM","journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":399,"to_page":402}}},"b13":{"title":"The semantic pathfinder: Using an authoring metaphor for generic multimedia indexing","authors":[{"person_name":{"surname":"Snoek","first_name":"C"},"affiliations":[],"email":null},{"person_name":{"surname":"Worring","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Geusebroek","first_name":"J"},"affiliations":[],"email":null},{"person_name":{"surname":"Koelma","first_name":"D"},"affiliations":[],"email":null},{"person_name":{"surname":"Seinstra","first_name":"F"},"affiliations":[],"email":null},{"person_name":{"surname":"Smeulders","first_name":"A"},"affiliations":[],"email":null}],"date":{"year":"2006","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"IEEE Trans. Pattern Anal. Mach. Intell","series":null,"scope":{"volume":28,"pages":{"from_page":1678,"to_page":1689}}},"b14":{"title":"Web mining for web image retrieval","authors":[{"person_name":{"surname":"Chen","first_name":"Z"},"affiliations":[],"email":null},{"person_name":{"surname":"Wenyin","first_name":"L"},"affiliations":[],"email":null},{"person_name":{"surname":"Zhang","first_name":"F"},"affiliations":[],"email":null},{"person_name":{"surname":"Li","first_name":"M"},"affiliations":[],"email":null}],"date":{"year":"2001","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"J. Am. Soc. Inf. Sci. Technol","series":null,"scope":{"volume":52,"pages":{"from_page":831,"to_page":839}}},"b15":{"title":"A unified framework for image retrieval using keyword and visual features","authors":[{"person_name":{"surname":"Jing","first_name":"F"},"affiliations":[],"email":null},{"person_name":{"surname":"Li","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Zhang","first_name":"H"},"affiliations":[],"email":null},{"person_name":{"surname":"Zhang","first_name":"B"},"affiliations":[],"email":null}],"date":{"year":"2005","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"IEEE Transactions on Image Processing","series":null,"scope":{"volume":14,"pages":{"from_page":979,"to_page":989}}},"b16":{"title":"Describing Texture Directions with Von Mises Distributions","authors":[{"person_name":{"surname":"Grana","first_name":"C"},"affiliations":[],"email":null},{"person_name":{"surname":"Borghesani","first_name":"D"},"affiliations":[],"email":null},{"person_name":{"surname":"Cucchiara","first_name":"R"},"affiliations":[],"email":null}],"date":{"year":"2008","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b17":{"title":"Properties of Embedding Methods for Similarity Searching in Metric Spaces","authors":[{"person_name":{"surname":"Hjaltason","first_name":"G"},"affiliations":[],"email":null},{"person_name":{"surname":"Samet","first_name":"H"},"affiliations":[],"email":null}],"date":{"year":"2003","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"IEEE Transactions on Pattern Analysis and Machine Intelligence","series":null,"scope":{"volume":25,"pages":{"from_page":530,"to_page":549}}},"b18":{"title":"Speeded-up robust features (surf)","authors":[{"person_name":{"surname":"Bay","first_name":"H"},"affiliations":[],"email":null},{"person_name":{"surname":"Ess","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Tuytelaars","first_name":"T"},"affiliations":[],"email":null},{"person_name":{"surname":"Van Gool","first_name":"L"},"affiliations":[],"email":null}],"date":{"year":"2008","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"Computer Vision and Image Understanding","series":null,"scope":{"volume":110,"pages":{"from_page":346,"to_page":359}}}},"sections":[{"title":"Introduction","paragraphs":[{"text":"The availability of semantic data is a well known advantage for different image retrieval tasks. In the recent literature, a lot of works have been proposed to create, manage and further exploit semantic information to be used in multimedia systems. The reason is that the information retrieval from textual data is quite successful. Semantic data are typically exploited in web searches in the form of tags (i.e. Google Images), but the process of tagging an image is known to be very boring from a user perspective, and likewise the process of linking correctly textual information about an image to the image itself is very tricky and error-prone. Nevertheless, often the amount of information and details held by a human-made description of an image is very precious, and it cannot be fully extracted using techniques based on vision.","refs":[]},{"text":"Regarding the system globally, it is known that many artistic or historical documents cannot be made available to the public, due to their value and fragility, so museum visitors are usually very limited in their appreciation of this kind of artistic productions. For this reason, the availability of digital versions of the artistic works, made accessible -both locally at the museums owning the original version and remotely-with a suitable software, represents undoubtedly an intriguing possibility of enjoyment (from the tourist perspective) and study (from an expert perspective).","refs":[]},{"text":"Italy, in particular, has a huge collection of illuminated manuscripts, but many of them are not freely accessible to the public. These masterpieces contain thousands of valuable illustrations: different mythological and real animals, biblical episodes, court life illustrations, and some of them even testify the first attempts in exploring perspective for landscapes. Usually manual segmentation and annotation for all of them is dramatically time consuming. For this reason, the accomplishment of the same task with an automatic procedure is very desirable but, at the same time, really challenging due to the visual appearance of these pictures (their arrangement over the page, their various framing into the decorative parts and so on).","refs":[]},{"text":"In this papers, we propose a solution for automatic manuscript segmentation and pictures extraction. A modification of the bag-of-keypoints approach is used to efficiently apply it in the context of automatic categorization of artistic handdrawn illustrations (i.e. separating illustrations depending on their content, e.g. people vs animals). On the top of this system, we integrated the knowledge of a complete commentary available for the specific manuscript we have. A standard keyword clustering approach (usually known as tagcloud) has been used to find out the most relevant subject within the entire book on in a smaller section, then we explored the correspondence of clusters of words and clusters of extracted pictures to verify if we can use textual data to help and improve the object recognition. The final goal is to provide automatic content-based functionalities such as searches for similarity, comparison, recognition of specific elements (people, life scenes, animals, etc...) in artistic manuscripts including also the textual search in the retrieval engine.","refs":[]}]},{"title":"Related Work","paragraphs":[{"text":"The problem of image analysis and classification of historical manuscripts is becoming a significant subject of research in recent years, even if the availability of complete systems for the automatic management of illuminated manuscripts digital libraries is quite limited. The AGORA [1] software performs a map of the foreground and the background and consequently propose a user interface to assist in the creation of an XML annotation of the page components. The Madonne system [2] is another initiative to use document image analysis techniques for the purpose of preserving and exploiting cultural heritage documents. In [3], Le Bourgeois et al. highlighted some problems with acquisition and compression, then authors gave a brief subdivision of documents classes, and for each of them provided a proposal of analysis. They distinguished between medieval manuscripts, early printed documents of the Renaissance, authors manuscripts from 18th to 19th century and, finally, administrative documents of the 18th -20th century: the authors perform color depth reduction, then a layout segmentation that is followed by the main body segmentation using text zones location. The feature analysis step uses some color, shape and geometrical features, and a PCA is performed in order to reduce the dimensionality. Finally the classification stage implements a K-NN approach.","refs":[{"start":285,"end":288,"marker":"bibr","target":"#b0"},{"start":482,"end":485,"marker":"bibr","target":"#b1"},{"start":627,"end":630,"marker":"bibr","target":"#b2"}]},{"text":"The bag-of-keypoints approach has become increasingly popular and successful in many object recognition and scene categorization tasks. The first proposals constructed a vocabulary of visual words by extracting image patches, sampled from a grid [4]. More advanced approaches used an interest point detector to select the most representative patches within the image [5]. The idea was finally evolved toward the clustering and quantization of local invariant features into visual words as initially proposed by [6] for object matching in videos. Lately the same approach was exploited in [7], which proposed the use of visual words in a bag-of-words representation built from SIFT descriptors [8] and various classifiers for scene categorization. SIFT in particular was one of the first algorithms which combined an interest points detector and a local descriptor to gather a good robustness to background clutter and good accuracy in description. Lately several aspects have been investigated. For example, as shown in [9], the bagof-words approach creates a simple representation but potentially introduces synonymy and polysemy ambiguities, which can be solved using probabilistic latent semantic analysis (PLSA) in order to capture co-occurrence information between elements. In [10] the influence of different strategies for keypoint sampling in the categorization accuracy has been studied: the Laplacian of Gaussian (LoG), the Harris-Laplace detector used in [11] and random sampling. A recent comparison of vocabulary construction techniques is proposed in [12].","refs":[{"start":246,"end":249,"marker":"bibr","target":"#b3"},{"start":367,"end":370,"marker":"bibr","target":"#b4"},{"start":511,"end":514,"marker":"bibr","target":"#b5"},{"start":588,"end":591,"marker":"bibr","target":"#b6"},{"start":693,"end":696,"marker":"bibr","target":"#b7"},{"start":1020,"end":1023,"marker":"bibr","target":"#b8"},{"start":1283,"end":1287,"marker":"bibr","target":"#b9"},{"start":1466,"end":1470,"marker":"bibr","target":"#b10"},{"start":1565,"end":1569,"marker":"bibr","target":"#b11"}]},{"text":"The idea to exploit text in order to improve or integrate image (and video) retrieval is not new. In fact a lot of complex retrieval systems present a fusion stage in which visual features are somehow fused to audio and/or textual features. This process is usually referred as multimodal fusion. For instance, in [13] and [14] in the context of video retrieval, Snoek et al. learned a list of concept-specific keywords, and based on this list they construct a word frequency histogram from shot-based speech transcripts. In [15], Chen et al. aimed to improve image web search engines by extracting textual information from the image \"environment\" (tags, urls, page content, etc. . . ) and users' logs. The text description (which semantically describes the image) is then combined with other low-level features extracted from the image itself to compute a similarity assessment. Textual information are also a very familiar approach for querying the system (since web search engines rely on it), so several works propose the query-by-keyword functionality along with the query-by-example and query-by-concept modalities. For references about this topic, please refer to [16].","refs":[{"start":313,"end":317,"marker":"bibr","target":"#b12"},{"start":322,"end":326,"marker":"bibr","target":"#b13"},{"start":524,"end":528,"marker":"bibr","target":"#b14"},{"start":1170,"end":1174,"marker":"bibr","target":"#b15"}]}]},{"title":"Automatic Segmentation and Retrieval","paragraphs":[{"text":"In [17] we described a system and the techniques used for text extraction and picture segmentation of illuminated manuscripts. The goal of the automatic segmentation system is to subdivide the document into its main semantic parts, in order to enable the design of new processing modules to manage and analyze each part, relieving the user of the task of manual annotation of the whole book collection. In that work we also introduced a first module for content-based retrieval functionalities by visual similarity with an ad-hoc designed user interface.","refs":[{"start":3,"end":7,"marker":"bibr","target":"#b16"}]},{"text":"The module for text segmentation computes the autocorrelation matrix over gray-scale image patches and converts them into a polar representation called direction histogram: a statistical framework able to handle angular datasets (i.e. a mixture of Von Mises distributions) generates a compact representation of such histograms that are then the final features used to classify each block through an SVM classifier.","refs":[]},{"text":"The text-free parts of the image are then passed to a second module that separates plain background, decorations and miniatures. We use here a sliding window approach and represent each window with a descriptor that joins color features (RGB and Enhanced HSV Histograms) and texture features (Gradient Spatial Dependency Matrix (GSDM). As in [18], we exploited a Lipschitz embedding technique to reduce the dimensionality of the feature space and again used an SVM classifier to obtain the desired classification.","refs":[{"start":342,"end":346,"marker":"bibr","target":"#b17"}]},{"text":"Some examples of picture extraction results are shown in Fig. 1. ","refs":[{"start":62,"end":63,"marker":"figure","target":"#fig_0"}]}]},{"title":"Bag-of-Keypoints Classification","paragraphs":[{"text":"One of the most successful strategies to perform object and scene recognition is the bag-of-keypoints approach. The main idea comes from the text categorization (bag-of-words), and it consists in defining, during the training phase: a. a set of \"words\" that is rich enough to provide a representative description of each and all the classes; b. the occurrences of these \"words\" for each class.","refs":[]},{"text":"In our context, since we cannot directly extract high level semantic words, we can define \"visual words\" by clustering accordingly visual descriptors (e.g. keypoint descriptors): the set of centroids of each cluster creates the so called vocabulary. After having counted, for each class, the occurrences of each word, the classification can then be easily performed extracting the histogram of the visual words of an example, and then finding the class that has the most similar occurrences distribution. In [7] scene categorization is accomplished following this procedure and making use of Harris affine detector as keypoint detector (mapped to a circular region in order to normalize them for affine transformations) and SIFT as keypoint descriptors. In our system, for performance reasons, we preferred the use of SURF [19]: it is a very successful local descriptor, it relies on integral images for image convolutions, it uses a fast Hessian matrix based interest point detector, performed with box filters (an approximation procedure again relying on integral images), and eventually it uses a simple Haar wavelet distribution descriptor resulting in a 64 feature vector. These factors make SURF computationally more affordable than SIFT, and very similar in terms of accuracy of the point matching performances. The training set in our system is composed of patches of miniatures belonging to different classes. SURF keypoint descriptors are extracted over all patches and the visual vocabulary V is then made of the k cluster centroids obtained running a k-means clustering procedure: V = {v i , i = 1 . . . k}, with v i cluster centroid. Once the vocabulary is computed, each class is characterized by a specific distribution of visual word occurrences: therefore we obtain p(v i |C j ), for each class j, for each visual word i. In order to avoid later numerical problems, we apply a Laplace smoothing. The number k is a key parameter of the whole process: low k will generate a poorly descriptive vocabulary, while high k will over fit the training data, therefore the training phase will slide through several k, finding the best value through cross validation.","refs":[{"start":508,"end":511,"marker":"bibr","target":"#b6"},{"start":823,"end":827,"marker":"bibr","target":"#b18"}]},{"text":"On any new image patch I to classify, the SURF descriptors are extracted and each casts a vote for the closest cluster centroid; I can be thus described as a histogram of the visual words of the vocabulary: each bin N (v i ) counts the number of times in which a word v t has been extracted from the image, constituting the feature vector. The final classification has been accomplished using Naïve Bayes.","refs":[]},{"text":"Naïve Bayes is a simple but effective classification technique based on Bayes' rule: given the image I and a prior probability p(C j ) for j-th class, the classifier assigns to I the class with the largest posterior p(C i |I) according to Eq. 1 (thus assuming the independence of visual words within the image):","refs":[]}]},{"title":"Knowledge Summary by Tag Cloud","paragraphs":[{"text":"Given a text describing a dataset of images, the correlation between frequency (thus importance) of terms and visual content is conceptually straightforward. For this reason, a clustering of keyword in such a document probably will lead to highlighting the most important concepts (and thus visual objects) included in the dataset. The standard text processing approach in text retrieval systems is the following:","refs":[]},{"text":"1. Parsing of the document into words. 2. Representing words by their stems. For example: \"draw\", \"drawing\" and \"drawn\" are represented by the stem \"draw\". 3. Definition of a stop list, i.e. a list of words to reject due to being common (such as \"the\") or recurring often in most documents and thus not being discriminant for a particular document. 4. Representation of the document as a vector of words and the relative frequency of occurrence within the document (different weighting techniques are possible).","refs":[]},{"text":"A very rough but effective clustering procedure to extract important keywords from a document is the tag cloud. Tag clouds are a common visual representation of user-generated tags or generally word content of a document, at different size and/or color based on its incidence, typically employed to describe the content of the document itself. We employed this simple procedure to generate and select from the commentary some keywords to use in our tests.","refs":[]}]},{"title":"Retrieval by Similarity","paragraphs":[{"text":"The final application devised for this work is a system aimed at presenting all the details of illuminated manuscripts in a user friendly way. Eventually an interface is proposed to the user, that can perform ranked image retrieval by content similarity: given a query picture, relative histograms are compared using histogram intersection metric, and similarity values are normalized, fused and finally ranked, from the most similar to the query. As referred in the next section, text information can also be used along with visual descriptors to describe visual content. In fact, the scope of this work is just focused on investigating the relation between textual data (retrieved from commentaries) and pictures in order to propose a multimodal search engine with a joint use of both pictorial and textual information.","refs":[]},{"text":"So far, the similarity results provided as an example in Fig. 3 are computed using a color histograms (HSV and RGB) as visual descriptors.","refs":[{"start":62,"end":63,"marker":"figure","target":"#fig_2"}]}]},{"title":"Results","paragraphs":[{"text":"In this paper, we used the digitalized pages of the Holy Bible of Borso d'Este, which is considered one of the best Renaissance illuminated manuscripts. Tests have been performed on a dataset of 320 high resolution digitalized images (3894x2792), a total amount of 640 pages. Each page of the dataset is an illuminated manuscript composed by a two-column layered text in Gothic font, spaced out with some decorated drop caps. The entire surrounding is highly decorated.","refs":[]},{"text":"The segmentation procedure described in Section 3 was run over the bible pages, providing us a set of valuable illustrations within the decoration texture (miniature illustrations of scenes, symbols, people and animals), rejecting border decorations (ornaments) and text. Some samples are shown in Fig. 2. Once the pictures have been processed, a set of tag clouds has been generated from the commentary.","refs":[{"start":303,"end":304,"marker":"figure","target":"#fig_1"}]},{"text":"The first tag we analyzed was \"Geremia\" (Fig. 4), that is the Italian for prophet Jeremiah. A section of the Bible is dedicated to him, so in that section there are a lot of visual references about him. In the relative section of the commentary, the whole word count is about 2900. In this section of the bible, a total amount of 68 pictures was extracted from the illustration of the pages. The same features used in the retrieval by similarity module referred in 6 have been extracted from the pictures, and then clustered using the hierarchical Complete Link algorithm with an automatic clustering level selection. We obtained 4 clusters, and over the most populated one we obtained a recall of 73.68% and a precision of 84.85%. See Fig. 5 for some samples coming from this cluster.","refs":[{"start":46,"end":47,"marker":"figure","target":"#fig_3"},{"start":741,"end":742,"marker":"figure","target":null}]},{"text":"The second tag we analyzed was \"cervo\" (Fig. 6), that is the Italian for deer. A lot of animals are depicted within this book, and the deer is one of the most frequent, so there are a lot of visual references about it. The whole word count within the relative commentary pages in this case is about 9100.","refs":[{"start":45,"end":46,"marker":"figure","target":null}]},{"text":"In the subset of pages where a deer is depicted, a total amount of 99 pictures was extracted from the illustration of the pages. Visual features have been extracted and then clustered. We obtained 3 clusters, and over the most populated one we obtained a recall of 84.31% and a precision of 72.88%. See Fig. 7 for some samples coming from this cluster.  ","refs":[{"start":308,"end":309,"marker":"figure","target":"#fig_5"}]}]},{"title":"Conclusions","paragraphs":[{"text":"In this paper, we investigated the use of textual keywords from experts-made commentaries in a system for automatic segmentation and extraction of pictures from illuminated manuscripts. We showed that textual information (if available) can be considered a useful addition not only to help categorization of the visual content, but also to give to the user the possibility to search the visual content with textual keywords, providing a multimodal search engine analogously to common web search engine.","refs":[]}]}],"tables":{},"abstract":{"title":"Abstract","paragraphs":[{"text":"In this paper we detail a proposal of exploitation of expertmade commentaries in a unified system for illuminated manuscripts images analysis. In particular we will explore the possibility to improve the automatic segmentation of meaningful pictures, as well as the retrieval by similarity search engine, using clusters of keywords extracted from commentaries as semantic information.","refs":[]}]}}