{"bibliography":{"title":"Object Recognition and Tracking for Smart Audio Guides","authors":[{"person_name":{"surname":"Seidenari","first_name":"Lorenzo"},"affiliations":[{"department":null,"institution":"University of Florence","laboratory":null}],"email":"lorenzo.seidenari@unifi.it"},{"person_name":{"surname":"Baecchi","first_name":"Claudio"},"affiliations":[{"department":null,"institution":"University of Florence","laboratory":null}],"email":"claudio.baecchi@unifi.it"},{"person_name":{"surname":"Uricchio","first_name":"Tiberio"},"affiliations":[{"department":null,"institution":"University of Florence","laboratory":null}],"email":"tiberio.uricchio@unifi.it"},{"person_name":{"surname":"Ferracani","first_name":"Andrea"},"affiliations":[{"department":null,"institution":"University of Florence","laboratory":null}],"email":"andrea.ferracani@unifi.it"},{"person_name":{"surname":"Bertini","first_name":"Marco"},"affiliations":[{"department":null,"institution":"University of Florence","laboratory":null}],"email":"marco.bertini@unifi.it"},{"person_name":{"surname":"Bimbo","first_name":"Alberto"},"affiliations":[{"department":null,"institution":"University of Florence","laboratory":null}],"email":"alberto.delbimbo@unifi.it"}],"date":null,"ids":{"DOI":"10.1007/978-3-319-73165-0_16","arXiv":null},"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"keywords":[],"citations":{"b0":{"title":"Personalization and the web from a museum perspective","authors":[{"person_name":{"surname":"Bowen","first_name":"J"},"affiliations":[],"email":null},{"person_name":{"surname":"Filippini-Fantoni","first_name":"S"},"affiliations":[],"email":null}],"date":{"year":"2004","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b1":{"title":"Personalized multimedia content delivery on an interactive table by passive observation of museum visitors","authors":[{"person_name":{"surname":"Karaman","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Bagdanov","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Landucci","first_name":"L"},"affiliations":[],"email":null},{"person_name":{"surname":"D'amico","first_name":"G"},"affiliations":[],"email":null},{"person_name":{"surname":"Ferracani","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Pezzatini","first_name":"D"},"affiliations":[],"email":null},{"person_name":{"surname":"Del Bimbo","first_name":"A"},"affiliations":[],"email":null}],"date":{"year":"2016","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"Multimedia Tools Appl","series":null,"scope":{"volume":75,"pages":{"from_page":3787,"to_page":3811}}},"b2":{"title":"Deep artwork detection and retrieval for automatic context-aware audio guides","authors":[{"person_name":{"surname":"Seidenari","first_name":"L"},"affiliations":[],"email":null},{"person_name":{"surname":"Baecchi","first_name":"C"},"affiliations":[],"email":null},{"person_name":{"surname":"Uricchio","first_name":"T"},"affiliations":[],"email":null},{"person_name":{"surname":"Ferracani","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Bertini","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Bimbo","first_name":"A"},"affiliations":[],"email":null}],"date":{"year":"2017","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"ACM Trans. Multimedia Comput. Commun. Appl","series":null,"scope":{"volume":13,"pages":{"from_page":21,"to_page":21}}},"b3":{"title":"Gesture recognition using wearable vision sensors to enhance visitors' museum experiences","authors":[{"person_name":{"surname":"Baraldi","first_name":"L"},"affiliations":[],"email":null},{"person_name":{"surname":"Paci","first_name":"F"},"affiliations":[],"email":null},{"person_name":{"surname":"Serra","first_name":"G"},"affiliations":[],"email":null},{"person_name":{"surname":"Benini","first_name":"L"},"affiliations":[],"email":null},{"person_name":{"surname":"Cucchiara","first_name":"R"},"affiliations":[],"email":null}],"date":{"year":"2015","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"IEEE Sens. J","series":null,"scope":{"volume":15,"pages":{"from_page":2705,"to_page":2714}}},"b4":{"title":"Cyberguide: a mobile context-aware tour guide","authors":[{"person_name":{"surname":"Abowd","first_name":"G"},"affiliations":[],"email":null},{"person_name":{"surname":"Atkeson","first_name":"C"},"affiliations":[],"email":null},{"person_name":{"surname":"Hong","first_name":"J"},"affiliations":[],"email":null},{"person_name":{"surname":"Long","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Kooper","first_name":"R"},"affiliations":[],"email":null},{"person_name":{"surname":"Pinkerton","first_name":"M"},"affiliations":[],"email":null}],"date":{"year":"1997","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"Wireless Netw","series":null,"scope":{"volume":3,"pages":{"from_page":421,"to_page":433}}},"b5":{"title":"Cultivating personalized museum tours online and on-site","authors":[{"person_name":{"surname":"Wang","first_name":"Y"},"affiliations":[],"email":null},{"person_name":{"surname":"Stash","first_name":"N"},"affiliations":[],"email":null},{"person_name":{"surname":"Sambeek","first_name":"R"},"affiliations":[],"email":null},{"person_name":{"surname":"Schuurmans","first_name":"Y"},"affiliations":[],"email":null},{"person_name":{"surname":"Aroyo","first_name":"L"},"affiliations":[],"email":null},{"person_name":{"surname":"Schreiber","first_name":"G"},"affiliations":[],"email":null},{"person_name":{"surname":"Gorgels","first_name":"P"},"affiliations":[],"email":null}],"date":{"year":"2009","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"Interdisc. Sci. Rev","series":null,"scope":{"volume":34,"pages":{"from_page":139,"to_page":153}}},"b6":{"title":"Analyzing museum visitors' behavior patterns","authors":[{"person_name":{"surname":"Zancanaro","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Kuflik","first_name":"T"},"affiliations":[],"email":null},{"person_name":{"surname":"Boger","first_name":"Z"},"affiliations":[],"email":null},{"person_name":{"surname":"Goren-Bar","first_name":"D"},"affiliations":[],"email":null},{"person_name":{"surname":"Goldwasser","first_name":"D"},"affiliations":[],"email":null}],"date":{"year":"2007","month":null,"day":null},"ids":{"DOI":"10.1007/978-3-540-73078-1_27","arXiv":null},"target":"https://doi.org/10.1007/978-3-540-73078-127","publisher":"Springer","journal":null,"series":null,"scope":{"volume":4511,"pages":{"from_page":238,"to_page":246}}},"b7":{"title":"Analysis and prediction of museum visitors' behavioral pattern types","authors":[{"person_name":{"surname":"Kuflik","first_name":"T"},"affiliations":[],"email":null},{"person_name":{"surname":"Boger","first_name":"Z"},"affiliations":[],"email":null},{"person_name":{"surname":"Zancanaro","first_name":"M"},"affiliations":[],"email":null}],"date":{"year":"2012","month":null,"day":null},"ids":{"DOI":"10.1007/978-3-642-27663-7_10","arXiv":null},"target":"https://doi.org/10.1007/978-3-642-27663-710","publisher":"Springer","journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":161,"to_page":176}}},"b8":{"title":"Ethnographie de l'exposition","authors":[{"person_name":{"surname":"Eliseo","first_name":"V"},"affiliations":[],"email":null},{"person_name":{"surname":"Martine","first_name":"L"},"affiliations":[],"email":null}],"date":{"year":"1991","month":null,"day":null},"ids":null,"target":null,"publisher":"Biblioth√®que publique d'information","journal":null,"series":null,"scope":null},"b9":{"title":"A digital look at physical museum exhibits: designing personalized stories with handheld augmented reality in museums","authors":[{"person_name":{"surname":"Keil","first_name":"J"},"affiliations":[],"email":null},{"person_name":{"surname":"Pujol","first_name":"L"},"affiliations":[],"email":null},{"person_name":{"surname":"Roussou","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Engelke","first_name":"T"},"affiliations":[],"email":null},{"person_name":{"surname":"Schmitt","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Bockholt","first_name":"U"},"affiliations":[],"email":null},{"person_name":{"surname":"Eleftheratou","first_name":"S"},"affiliations":[],"email":null}],"date":{"year":"2013","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b10":{"title":"You only look once: unified, real-time object detection","authors":[{"person_name":{"surname":"Redmon","first_name":"J"},"affiliations":[],"email":null},{"person_name":{"surname":"Divvala","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Girshick","first_name":"R"},"affiliations":[],"email":null},{"person_name":{"surname":"Farhadi","first_name":"A"},"affiliations":[],"email":null}],"date":{"year":"2016","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null}},"sections":[{"title":"Introduction","paragraphs":[{"text":"According to recent statistics from the US National Travel and Tourism Office, a new record of tourism-related activities 1 has been set recently. Museum visits are rising steadily thanks to the availability of new digital and mobile technologies. Modern visitors do not follow fixed paths, but they expect personalization and interaction. As a result, new companion tools are needed, providing content sized to the needs and interests of the visitors [1].","refs":[{"start":452,"end":455,"marker":"bibr","target":"#b0"}]},{"text":"In order to automatically gather the behavior of users, these tools have been using cameras to observe where the users go and what they observe. Several approaches resorted to computer vision systems to offer recommendation based on passive external behaviour observation [2] or, more recently, to develop a wearable smart audio guide [3] to automatically play content or interact with artworks [4]. These modern approaches work by constantly matching the user point of view with a visual database of the known artworks, deciding, depending on user behavior, whether to start or not the audio description generated by means of text to speech technology [3] or to show additional content on gestures [4]. Although designed to work in different settings, they all require a computer vision expert to train and test a computer vision models for artworks, person or statues. Moreover, every time an artwork is added or removed, the database has to be updated and a new model has to be trained. We argue that a more efficient solution would be to let the museum curator add new artworks by himself, without requiring to retrain the model from scratch.","refs":[{"start":272,"end":275,"marker":"bibr","target":"#b1"},{"start":335,"end":338,"marker":"bibr","target":"#b2"},{"start":395,"end":398,"marker":"bibr","target":"#b3"},{"start":653,"end":656,"marker":"bibr","target":"#b2"},{"start":699,"end":702,"marker":"bibr","target":"#b3"}]},{"text":"In this paper we propose a wearable audio guide system that, by observing in real time what the user is looking at and by following him in the visit, provides personalized content when needed. The device observes the wearer behavior through a computer vision system and decides when to start and stop the reproduction of the audio content. In contrast to previous work [3], artworks are recognized from an on-board database that can be easily made by museum curators using a novel, easy to use procedure. To this end, we show how to avoid re-training the object detector by learning a generic artwork detector based on convolutional neural network (CNN). We develop a novel artwork tracking technique that is used to populate the database using the same CNN object detector we trained for recognition.","refs":[{"start":369,"end":372,"marker":"bibr","target":"#b2"}]},{"text":"We implemented an Android application that a museum curator can use to build a dataset of artworks adaptively. After a recording phase, it performs all required computation on board and outputs ready to use models for in smart audio guides like [3].","refs":[{"start":245,"end":248,"marker":"bibr","target":"#b2"}]}]},{"title":"Related Work","paragraphs":[{"text":"Our work is mainly related to the personalization of the cultural experience and content recommendation on mobile devices. Many works propose to use mobile systems to enjoy an augmented personalized experience on cultural heritage. One of the first concept was that of Abowd et al. [5], that marked the difference between indoor and outdoor systems. We thus follow that division and differentiate between local systems to be used in cultural heritage sites, where there is control over the artworks, and outdoor systems that can be used while traveling in a city.","refs":[{"start":282,"end":285,"marker":"bibr","target":"#b4"}]},{"text":"Local systems are mostly developed for museums. In [6] the Cultural Heritage Information Personalization (CHIP) system was proposed, where a personalized tour could be created through a web interface. The tour can be downloaded to a mobile device using RFID present in the museum, and keeps track of the visited artwork on the server side user profile for successive tours. Analyzing and predicting the behavioral patterns of museum visitors, through the use of interactive digital guides was proposed in [7,8]. They follow the four identified patterns, emerged through ethnographic observations in [9]. Augmented reality on a mobile device was explored in [10] to offer a personalized interactive storytelling experience. Based on the age of the visitor, the system provides a gamified experience to children. In [2] a non-intrusive computer-vision system has been employed to perform re-identification and tracking of users in a museum. By observing the interest of the visitors, it can build a user profile that is then used to create a personalized exploration of multimedia content on an interactive table . \nDifferently from all of these works, we developed a wearable agent that observes the same scene as the user and provides a novel contextually aware interaction based on audio only, that is unintrusive. Moreover, all the computation required is performed onboard.","refs":[{"start":51,"end":54,"marker":"bibr","target":"#b5"},{"start":505,"end":508,"marker":"bibr","target":"#b6"},{"start":508,"end":510,"marker":"bibr","target":"#b7"},{"start":599,"end":602,"marker":"bibr","target":"#b8"},{"start":657,"end":661,"marker":"bibr","target":"#b9"},{"start":814,"end":817,"marker":"bibr","target":"#b1"}]}]},{"title":"Efficient Object Detection and Recognition","paragraphs":[{"text":"The smart audio guide we developed is based on an efficient computer vision pipeline that simultaneously performs artwork localization and recognition. The guide requires two main computer vision tasks to be solved: (i) detection of relevant object categories: e.g. persons and artworks; and (ii) for every detected artwork, reliable recognition of the specific artwork framed. Moreover, since we are dealing with a sequence of frames, in order to improve artwork recognition we take advantage from temporal coherence to make the output more stable. Our method is based on [3], which we briefly cover in the following. We use YOLO [11] network that has the main advantage of processing each frame just once to locate all the objects of interest yielding accurate results even for moderate size networks. The architecture is derived from Tiny Net, a small CNN pre-trained on ImageNet, which allows the application to run at 10 FPS and fitting on the memory of a Shield Tablet. The system was fine-tuned to recognize artworks and people using our dataset. Recognizing people is relevant for two reasons: first we can exploit the presence of people in the field of view to create a better understanding of context; secondly, without learning a person model it is hard to avoid false positives on people, since artwork training data contains statues which may picture human figures. Learning jointly a person and an artwork model, the network features can be trained to discriminate between this two classes.","refs":[{"start":573,"end":576,"marker":"bibr","target":"#b2"},{"start":631,"end":635,"marker":"bibr","target":"#b10"}]}]},{"title":"Artwork Recognition","paragraphs":[{"text":"The rich features computed by the convolutional layers are exploited and reused to compute an object descriptor for artwork recognition. To obtain a low dimensional fixed size descriptor of a region, we apply a global max-pooling over two convolutional feature activation maps and concatenate the result, as shown in Fig. 1. The region is remapped from the frame to the convolutional activation map with a simple similarity transformation. As also detailed in [3], through experimental evaluation, we selected the features from layers 3 and 4, yielding a feature of size 768.","refs":[{"start":322,"end":323,"marker":"figure","target":"#fig_0"},{"start":460,"end":463,"marker":"bibr","target":"#b2"}]},{"text":"Considering a pre-acquired dataset of artwork patches p i ‚àà D and their artwork labels y, for each detected artwork d we predict a specific artwork label y p finding the nearest neighbor patch p = arg max","refs":[]},{"text":"The recognition system observes each frame independently and predicts artwork labels according to Eq. 1, this approach, in case of motion blur or quick lighting changes may produce incorrect recognition results. ","refs":[]}]},{"title":"Automatic Dataset Creation","paragraphs":[{"text":"Extending the dataset with our architecture is extremely straightforward. We rely on a simple multi-target tracking algorithm. With respect to [3] we added a functionality to manage two new use cases: (i) adding a new artwork, which is needed in the deployment phase of our system to populate the Artwork DB and whenever a new piece is added to the exhibition; and (ii) adding examples of an existing artwork, which arise at any time the position of artworks or any other environmental condition has caused a decrease in performance of the recognition. Moreover, this second use case allows the exhibition curator to acquire artwork samples at multiple times making the acquisition process easier and less fatiguing.","refs":[{"start":143,"end":146,"marker":"bibr","target":"#b2"}]},{"text":"We perform tracking by data association, first we detect all artworks using our CNN, then we greedily associate bounding boxes to the one detected on the previous frame, allowing association only if intersection over union is above 60%. All unassociated boxes are stored and an association is attempted with all boxes at the following frame. All boxes from the previous frame which could not be associated are removed. This method may fail in case the detector skips a frame, nonetheless we found out that this is a very infrequent case and we allow the user to re-initialize the tracker in case the tracked object is lost.","refs":[]},{"text":"We only retrieve features for an artwork at a time. When the acquisition view is started, the user is prompted to select one of the detected artworks, enclosed in a dashed bounding boxes as show in Fig. 2, from the rolling video. Once an artwork is selected with a tap, its bounding box is drawn with a solid line and a tracking id is shown (for debugging purposes). For every associated detection our CV system stores in the App database the feature extracted using the method described in Sect. 3 together with the relative frame snapshots. ","refs":[{"start":203,"end":204,"marker":"figure","target":"#fig_1"}]}]},{"title":"Experiments","paragraphs":[{"text":"To show the benefits of our approach for dataset extension in our system, we conduct a simple experiment. We progressively increase the dataset of our artworks reaching a maximum of roughly 2k samples for eight artworks. It can be seen in Fig. 3 that recognition accuracy increases with the amount of samples. It has to be noted that just with the 10% of our acquisition we can reach more than 90% accuracy. Nonetheless increasing the samples reaches almost 100% accuracy. As also shown in [3] the 1-NN approach is best. ","refs":[{"start":244,"end":245,"marker":"figure","target":"#fig_2"},{"start":490,"end":493,"marker":"bibr","target":"#b2"}]}]},{"title":"Conclusions","paragraphs":[{"text":"We have presented a mobile application able to deliver real-time audio information. The main issue of computer vision systems is the training and deployment. We avoid re-training the object detector by learning a generic artwork detector. We show how to populate the database using the same CNN object detector we trained for recognition. Experiments show the benefit of increasing the samples in our pipeline. The system have been deployed and tested on a NVIDIA Shield with TK1.","refs":[]}]}],"tables":{},"abstract":{"title":"Abstract","paragraphs":[{"text":"In this paper we address the problem of creating a smart audio guide that adapts to the actions and interests of tourists. Our guide performs automatic recognition of artworks and allows the users instant or deferred fruition of multimedia content. We use a compact CNN as computer vision system to back the whole application to performs object classification, localization and recognition. Tracking is used to improve the recognition accuracy over sequences of detections. We also provide an automatic pipeline for dataset creation based on the same tracking algorithm. The system, deployed on an NVIDIA Jetson TK1 and an NVIDIA Shield Tablet, has been tested in a real world environment.","refs":[]}]}}