{"bibliography":{"title":"Semantically Aware Text Categorisation for Metadata Annotation","authors":[{"person_name":{"surname":"Carducci","first_name":"Giulio"},"affiliations":[{"department":"Dipartimento di Filosofia","institution":"Università degli Studi di Torino","laboratory":null}],"email":"giulio.carducci@protonmail.com"},{"person_name":{"surname":"Leontino","first_name":"Marco"},"affiliations":[{"department":"Dipartimento di Informatica","institution":"Università degli Studi di Torino","laboratory":null}],"email":"marco.leontino@unito.it"},{"person_name":{"surname":"Radicioni","first_name":"Daniele"},"affiliations":[{"department":"Dipartimento di Informatica","institution":"Università degli Studi di Torino","laboratory":null}],"email":"daniele.radicioni@unito.it"},{"person_name":{"surname":"Bonino","first_name":"Guido"},"affiliations":[{"department":"Dipartimento di Filosofia","institution":"Università degli Studi di Torino","laboratory":null}],"email":"guido.bonino@unito.it"},{"person_name":{"surname":"Pasini","first_name":"Enrico"},"affiliations":[{"department":"Dipartimento di Filosofia","institution":"Università degli Studi di Torino","laboratory":null}],"email":"enrico.pasini@unito.it"},{"person_name":{"surname":"Tripodi","first_name":"Paolo"},"affiliations":[{"department":"Dipartimento di Filosofia","institution":"Università degli Studi di Torino","laboratory":null}],"email":"paolo.tripodi@unito.it"}],"date":null,"ids":{"DOI":"10.1007/978-3-030-11226-4_25","arXiv":null},"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"keywords":["Text categorization","Lexical resources","Language models","Semantics","Nlp"],"citations":{"b0":{"title":"Classification of phishing email using random forest machine learning technique","authors":[{"person_name":{"surname":"Akinyelu","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Adewumi","first_name":"A"},"affiliations":[],"email":null}],"date":{"year":"2014","month":null,"day":null},"ids":null,"target":null,"publisher":"Hindawi","journal":"J. Appl. Math","series":null,"scope":{"volume":null,"pages":{"from_page":2014,"to_page":2014}}},"b1":{"title":"Automatic text summarization using support vector machine","authors":[{"person_name":{"surname":"Begum","first_name":"N"},"affiliations":[],"email":null},{"person_name":{"surname":"Fattah","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Ren","first_name":"F"},"affiliations":[],"email":null}],"date":{"year":"2009","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"Int. J. Innov. Comput. Inf. Control","series":null,"scope":{"volume":5,"pages":{"from_page":1987,"to_page":1996}}},"b2":{"title":"Random forests","authors":[{"person_name":{"surname":"Breiman","first_name":"L"},"affiliations":[],"email":null}],"date":{"year":"2001","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"Mach. Learn","series":null,"scope":{"volume":45,"pages":{"from_page":5,"to_page":32}}},"b3":{"title":"Semantically Aware Text Categorisation for Metadata Annotation: Implementation and Test Files","authors":[{"person_name":{"surname":"Carducci","first_name":"G"},"affiliations":[],"email":null},{"person_name":{"surname":"Leontino","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Radicioni","first_name":"D"},"affiliations":[],"email":null}],"date":{"year":"2018","month":null,"day":null},"ids":{"DOI":"10.5281/zenodo.1446128","arXiv":null},"target":"https://doi.org/10.5281/zenodo.1446128","publisher":null,"journal":null,"series":null,"scope":null},"b4":{"title":"Feature selection for text classification with naïve bayes","authors":[{"person_name":{"surname":"Chen","first_name":"J"},"affiliations":[],"email":null},{"person_name":{"surname":"Huang","first_name":"H"},"affiliations":[],"email":null},{"person_name":{"surname":"Tian","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Qu","first_name":"Y"},"affiliations":[],"email":null}],"date":{"year":"2009","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"Expert Syst. Appl","series":null,"scope":{"volume":36,"pages":{"from_page":5432,"to_page":5435}}},"b5":{"title":"Tell me why: computational explanation of conceptual similarity judgments","authors":[{"person_name":{"surname":"Colla","first_name":"D"},"affiliations":[],"email":null},{"person_name":{"surname":"Mensa","first_name":"E"},"affiliations":[],"email":null},{"person_name":{"surname":"Radicioni","first_name":"D"},"affiliations":[],"email":null},{"person_name":{"surname":"Lieto","first_name":"A"},"affiliations":[],"email":null}],"date":{"year":"2018","month":null,"day":null},"ids":{"DOI":"10.1007/978-3-319-91473-2_7","arXiv":null},"target":"https://doi.org/10.1007/978-3-319-91473-2","publisher":"Springer","journal":null,"series":null,"scope":{"volume":853,"pages":{"from_page":74,"to_page":85}}},"b6":{"title":"Random forests for classification in ecology","authors":[{"person_name":{"surname":"Cutler","first_name":"D"},"affiliations":[],"email":null}],"date":{"year":"2007","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"Ecology","series":null,"scope":{"volume":88,"pages":{"from_page":2783,"to_page":2792}}},"b7":{"title":"Automatic turkish text categorization in terms of author, genre and gender","authors":[{"person_name":{"surname":"Amasyalı","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Diri","first_name":"B"},"affiliations":[],"email":null}],"date":{"year":"2006","month":null,"day":null},"ids":{"DOI":"10.1007/11765448_22","arXiv":null},"target":"https://doi.org/10.1007/1176544822","publisher":"Springer","journal":null,"series":null,"scope":{"volume":3999,"pages":{"from_page":221,"to_page":226}}},"b8":{"title":"Cooperating techniques for extracting conceptual taxonomies from text","authors":[{"person_name":{"surname":"Ferilli","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Leuzzi","first_name":"F"},"affiliations":[],"email":null},{"person_name":{"surname":"Rotella","first_name":"F"},"affiliations":[],"email":null}],"date":{"year":"2011","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b9":{"title":"Incorporating non-local information into information extraction systems by gibbs sampling","authors":[{"person_name":{"surname":"Finkel","first_name":"J"},"affiliations":[],"email":null},{"person_name":{"surname":"Grenager","first_name":"T"},"affiliations":[],"email":null},{"person_name":{"surname":"Manning","first_name":"C"},"affiliations":[],"email":null}],"date":{"year":"2005","month":null,"day":null},"ids":null,"target":null,"publisher":"Association for Computational Linguistics","journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":363,"to_page":370}}},"b10":{"title":"Overcoming the brittleness bottleneck using wikipedia: enhancing text categorization with encyclopedic knowledge","authors":[{"person_name":{"surname":"Gabrilovich","first_name":"E"},"affiliations":[],"email":null},{"person_name":{"surname":"Markovitch","first_name":"S"},"affiliations":[],"email":null}],"date":{"year":"2006","month":null,"day":null},"ids":null,"target":null,"publisher":"AAAI Press","journal":null,"series":null,"scope":{"volume":2,"pages":{"from_page":1301,"to_page":1306}}},"b11":{"title":"Typicality-based inference by plugging conceptual spaces into ontologies","authors":[{"person_name":{"surname":"Ghignone","first_name":"L"},"affiliations":[],"email":null},{"person_name":{"surname":"Lieto","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Radicioni","first_name":"D"},"affiliations":[],"email":null}],"date":{"year":"2013","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b12":{"title":"Question answering","authors":[{"person_name":{"surname":"Harabagiu","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Moldovan","first_name":"D"},"affiliations":[],"email":null}],"date":{"year":"2003","month":null,"day":null},"ids":null,"target":null,"publisher":"Oxford University Press","journal":null,"series":null,"scope":null},"b13":{"title":"Random decision forests","authors":[{"person_name":{"surname":"Ho","first_name":"T"},"affiliations":[],"email":null}],"date":{"year":"1995","month":null,"day":null},"ids":null,"target":null,"publisher":"IEEE Computer Society","journal":null,"series":null,"scope":{"volume":1,"pages":{"from_page":278,"to_page":282}}},"b14":{"title":"Text summarization","authors":[{"person_name":{"surname":"Hovy","first_name":"E"},"affiliations":[],"email":null}],"date":{"year":"2003","month":null,"day":null},"ids":null,"target":null,"publisher":"Oxford University Press","journal":null,"series":null,"scope":null},"b15":{"title":"Text categorization with support vector machines: learning with many relevant features","authors":[{"person_name":{"surname":"Joachims","first_name":"T"},"affiliations":[],"email":null}],"date":{"year":"1998","month":null,"day":null},"ids":{"DOI":"10.1007/BFb0026683","arXiv":null},"target":"https://doi.org/10.1007/BFb0026683","publisher":"Springer","journal":null,"series":null,"scope":{"volume":1398,"pages":{"from_page":137,"to_page":142}}},"b16":{"title":"Semi-supervised convolutional neural networks for text categorization via region embedding","authors":[{"person_name":{"surname":"Johnson","first_name":"R"},"affiliations":[],"email":null},{"person_name":{"surname":"Zhang","first_name":"T"},"affiliations":[],"email":null}],"date":{"year":"2015","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":919,"to_page":927}}},"b17":{"title":"Bag of tricks for efficient text classification","authors":[{"person_name":{"surname":"Joulin","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Grave","first_name":"E"},"affiliations":[],"email":null},{"person_name":{"surname":"Bojanowski","first_name":"P"},"affiliations":[],"email":null},{"person_name":{"surname":"Mikolov","first_name":"T"},"affiliations":[],"email":null}],"date":{"year":"2017","month":null,"day":null},"ids":null,"target":null,"publisher":"Association for Computational Linguistics","journal":null,"series":"Short Papers","scope":{"volume":2,"pages":{"from_page":427,"to_page":431}}},"b18":{"title":"Recurrent convolutional neural networks for text classification","authors":[{"person_name":{"surname":"Lai","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Xu","first_name":"L"},"affiliations":[],"email":null},{"person_name":{"surname":"Liu","first_name":"K"},"affiliations":[],"email":null},{"person_name":{"surname":"Zhao","first_name":"J"},"affiliations":[],"email":null}],"date":{"year":"2015","month":null,"day":null},"ids":null,"target":null,"publisher":"AAAI Press","journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":2267,"to_page":2273}}},"b19":{"title":"A resource-driven approach for anchoring linguistic resources to conceptual spaces","authors":[{"person_name":{"surname":"Lieto","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Mensa","first_name":"E"},"affiliations":[],"email":null},{"person_name":{"surname":"Radicioni","first_name":"D"},"affiliations":[],"email":null}],"date":{"year":"2016","month":null,"day":null},"ids":{"DOI":"10.1007/978-3-319-49130-1_32","arXiv":null},"target":"https://doi.org/10.1007/978-3-319-49130-132","publisher":"Springer","journal":null,"series":null,"scope":{"volume":10037,"pages":{"from_page":435,"to_page":449}}},"b20":{"title":"Dual peccs: a cognitive system for conceptual representation and categorization","authors":[{"person_name":{"surname":"Lieto","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Radicioni","first_name":"D"},"affiliations":[],"email":null},{"person_name":{"surname":"Rho","first_name":"V"},"affiliations":[],"email":null}],"date":{"year":"2017","month":null,"day":null},"ids":{"DOI":"10.1080/0952813X.2016.1198934","arXiv":null},"target":"https://doi.org/10.1080/0952813X.2016.1198934","publisher":null,"journal":"J. Exp. Theoret. Artif. Intell","series":null,"scope":{"volume":29,"pages":{"from_page":433,"to_page":452}}},"b21":{"title":"Opendial: a toolkit for developing spoken dialogue systems with probabilistic rules","authors":[{"person_name":{"surname":"Lison","first_name":"P"},"affiliations":[],"email":null},{"person_name":{"surname":"Kennington","first_name":"C"},"affiliations":[],"email":null}],"date":{"year":"2016","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":67,"to_page":72}}},"b22":{"title":"Conceptnet-a practical commonsense reasoning tool-kit","authors":[{"person_name":{"surname":"Liu","first_name":"H"},"affiliations":[],"email":null},{"person_name":{"surname":"Singh","first_name":"P"},"affiliations":[],"email":null}],"date":{"year":"2004","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"BT Technol. J","series":null,"scope":{"volume":22,"pages":{"from_page":211,"to_page":226}}},"b23":{"title":"Key phrase extraction lightly filtered broadcast news","authors":[{"person_name":{"surname":"Marujo","first_name":"L"},"affiliations":[],"email":null},{"person_name":{"surname":"Ribeiro","first_name":"R"},"affiliations":[],"email":null},{"person_name":{"surname":"De Matos","first_name":"D"},"affiliations":[],"email":null},{"person_name":{"surname":"Neto","first_name":"J"},"affiliations":[],"email":null},{"person_name":{"surname":"Gershman","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Carbonell","first_name":"J"},"affiliations":[],"email":null}],"date":{"year":"2012","month":null,"day":null},"ids":{"DOI":"10.1007/978-3-642-32790-2_35","arXiv":null},"target":"https://doi.org/10.1007/978-3-642-32790-235","publisher":"Springer","journal":null,"series":null,"scope":{"volume":7499,"pages":{"from_page":290,"to_page":297}}},"b24":{"title":"A comparison of event models for naive bayes text classification","authors":[{"person_name":{"surname":"Mccallum","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Nigam","first_name":"K"},"affiliations":[],"email":null}],"date":{"year":"1998","month":null,"day":null},"ids":null,"target":null,"publisher":"AAAI Press","journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":41,"to_page":48}}},"b25":{"title":"COVER: a linguistic resource combining common sense and lexicographic information","authors":[{"person_name":{"surname":"Mensa","first_name":"E"},"affiliations":[],"email":null},{"person_name":{"surname":"Radicioni","first_name":"D"},"affiliations":[],"email":null},{"person_name":{"surname":"Lieto","first_name":"A"},"affiliations":[],"email":null}],"date":{"year":"2018","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"Lang. Resour. Eval","series":null,"scope":{"volume":52,"pages":{"from_page":921,"to_page":948}}},"b26":{"title":"WordNet: a lexical database for english","authors":[{"person_name":{"surname":"Miller","first_name":"G"},"affiliations":[],"email":null}],"date":{"year":"1995","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"Commun. ACM","series":null,"scope":{"volume":38,"pages":{"from_page":39,"to_page":41}}},"b27":{"title":"BabelNet: building a very large multilingual semantic network","authors":[{"person_name":{"surname":"Navigli","first_name":"R"},"affiliations":[],"email":null},{"person_name":{"surname":"Ponzetto","first_name":"S"},"affiliations":[],"email":null}],"date":{"year":"2010","month":null,"day":null},"ids":null,"target":null,"publisher":"ACL","journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":216,"to_page":225}}},"b28":{"title":"Text classification from labeled and unlabeled documents using EM","authors":[{"person_name":{"surname":"Nigam","first_name":"K"},"affiliations":[],"email":null},{"person_name":{"surname":"Mccallum","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Thrun","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Mitchell","first_name":"T"},"affiliations":[],"email":null}],"date":{"year":"2000","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"Mach. Learn","series":null,"scope":{"volume":39,"pages":{"from_page":103,"to_page":134}}},"b29":{"title":"Distributional structure","authors":[{"person_name":{"surname":"Harris","first_name":"Z"},"affiliations":[],"email":null}],"date":{"year":"1954","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"Word","series":null,"scope":{"volume":10,"pages":{"from_page":146,"to_page":162}}},"b30":{"title":"Enriching the knowledge sources used in a maximum entropy part-of-speech tagger","authors":[{"person_name":{"surname":"Toutanova","first_name":"K"},"affiliations":[],"email":null},{"person_name":{"surname":"Manning","first_name":"C"},"affiliations":[],"email":null}],"date":{"year":"2000","month":null,"day":null},"ids":null,"target":null,"publisher":"Association for Computational Linguistics","journal":null,"series":null,"scope":{"volume":13,"pages":{"from_page":63,"to_page":70}}},"b31":{"title":"A knowledge network constructed by integrating classification, thesaurus, and metadata in digital library","authors":[{"person_name":{"surname":"Wang","first_name":"J"},"affiliations":[],"email":null}],"date":{"year":"2003","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"Int. Inf. Libr. Rev","series":null,"scope":{"volume":35,"pages":{"from_page":383,"to_page":397}}},"b32":{"title":"The dublin core: a simple content description model for electronic resources","authors":[{"person_name":{"surname":"Weibel","first_name":"S"},"affiliations":[],"email":null}],"date":{"year":"1997","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"Bull. Am. Soc. Inf. Sci. Technol","series":null,"scope":{"volume":24,"pages":{"from_page":9,"to_page":11}}},"b33":{"title":"An improved random forest classifier for text categorization","authors":[{"person_name":{"surname":"Xu","first_name":"B"},"affiliations":[],"email":null},{"person_name":{"surname":"Guo","first_name":"X"},"affiliations":[],"email":null},{"person_name":{"surname":"Ye","first_name":"Y"},"affiliations":[],"email":null},{"person_name":{"surname":"Cheng","first_name":"J"},"affiliations":[],"email":null}],"date":{"year":"2012","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"JCP","series":null,"scope":{"volume":7,"pages":{"from_page":2913,"to_page":2920}}}},"sections":[{"title":"Introduction","paragraphs":[{"text":"To date natural language processing (NLP) resources and techniques are being used in many tasks, such as conversational agents applications [22], question answering [13], automatic summarization [15], keywords extraction [24], text categorisation [17]. In this paper we propose a system to automatically annotate metadata related to scholarly records; in particular, we show how lexical resources can be paired to standard categorisation algorithms to obtain accurate categorisation results.","refs":[{"start":140,"end":144,"marker":"bibr","target":"#b21"},{"start":165,"end":169,"marker":"bibr","target":"#b12"},{"start":195,"end":199,"marker":"bibr","target":"#b14"},{"start":221,"end":225,"marker":"bibr","target":"#b23"},{"start":247,"end":251,"marker":"bibr","target":"#b16"}]},{"text":"This work is carried out in the frame of a broader philosophical research project aimed at investigating a set of UK doctoral theses collected by the Electronic Theses Online Service (EThOS). 1 Although we presently consider only the EThOS dataset, a huge amount of such documents have been collected within the project activities from different sources and countries, such as US, Canada, Italy, and other PhD theses are currently being searched to collect further data. Of course, many issues arise when trying to apply a uniform data model to such heterogeneous data; data is noisy, with partly missing information (e.g., abstracts are mostly missing until more recent years), and so on. Amongst the most basic issues, we single out a problem of text categorisation. In fact, when searching for philosophical theses in the EThOS dataset (i.e., those with 'Philosophy' in the dc:subject field) not all retrieved records are actually related to Philosophy, but rather to cognate disciplines such as Sociology, Religion, Psychology, and so forth. Additionally, in some cases the subject field is empty, or it contains numbers, or different sorts of noisy information. The thesis subject may be of little relevance in this setting because in UK there is no clear and univocal administrative classification of PhD titles according to disciplines. We presently focus on the problem of categorising such records in order to further refine the information provided by the dc:subject field, and to individuate philosophical theses. Although the task at hand is a binary classification problem, it is not that simple in that (i) in many cases the thesis disciplines are distinct though not well separated; (ii) the abstract may be lacking (thus very little information is available), and (iii) no labelled data is available to train some learning algorithm.","refs":[]},{"text":"Although the methodology described in the paper has been developed to cope with a specific problem, the proposed solution is general; the whole system basically implements an attempt at integrating domain specific knowledge (acquired by training a learning system) and general knowledge (embodied in the Babel-Net semantic network). Specifically, we show that the output of a state-of-the-art algorithm (Random Forest [14], an ensemble learning technique building on decision trees) trained on a specific dataset can be refined through a search over a semantic network grasping general conceptual knowledge. The obtained results significantly improve on those provided by the two software modules separately. Also, the system enjoys the nice property of providing a concise explanation illustrating why a thesis should be considered as properly philosophical, based uniquely on the information available in the thesis title.","refs":[{"start":418,"end":422,"marker":"bibr","target":"#b13"}]},{"text":"The paper is structured as follows: in Sect. 2 we briefly survey the related work on text categorisation; we then describe the EThOS dataset and provide some descriptive statistics to qualify it (Sect. 3). The System is then illustrated in full detail (Sect. 4). In Sect. 5 we present and discuss the results of the evaluation of the system-which was tested on a dataset handcrafted by two human experts-and conclude by pointing out present weaknesses and future work (Sect. 6).","refs":[]}]},{"title":"Related Work","paragraphs":[{"text":"Classification of textual documents is a task that draws particular interest in the field of natural language processing and is characterised by numerous challenges such as the high dimensionality and sparsity of the data, unbalanced classes, the lack of enough annotated samples and the time and effort required to manually inspect large datasets.","refs":[]},{"text":"During the years, many techniques have been proposed that address one or more of the mentioned challenges trying to reduce their negative impact. Traditional approaches usually feature machine learning algorithms such as decision trees, random forests [3,8], support vector machines (SVM) [2,16], Naïve Bayes [5,25]. Some hybrid approaches have been proposed that combine the result of the classification with other types of information. Wang integrates the classification of documents and the knowledge acquired from Chinese digital archives into a concept network, enhancing the metadata of documents and their organisation in digital libraries [32]. Similarly, Ferilli et al. build a semantic network from the text, where concepts are connected by a set of relationships, consisting in verbs from the input documents [9]. The resulting taxonomy can be used to perform some semantic reasoning or understand the content of the documents, although it needs some refinement. Nigam et al. address the problem of scarcity of annotated data by combining the Expectation-Maximization (EM) and a Naïve Bayes classifier [29]. They show how the classification error can be significantly reduced by an appropriate weighing of the documents and modelling of the classes. Gabrilovich et al. propose a classifier that is able to match documents with Wikipedia articles, then integrate the concepts extracted from such articles into the features of the original document, thus enriching its semantic representation [11].","refs":[{"start":252,"end":255,"marker":"bibr","target":"#b2"},{"start":255,"end":257,"marker":"bibr","target":"#b7"},{"start":289,"end":292,"marker":"bibr","target":"#b1"},{"start":292,"end":295,"marker":"bibr","target":"#b15"},{"start":309,"end":312,"marker":"bibr","target":"#b4"},{"start":312,"end":315,"marker":"bibr","target":"#b24"},{"start":647,"end":651,"marker":"bibr","target":"#b31"},{"start":820,"end":823,"marker":"bibr","target":"#b8"},{"start":1113,"end":1117,"marker":"bibr","target":"#b28"},{"start":1502,"end":1506,"marker":"bibr","target":"#b10"}]},{"text":"Textual data is often represented using bag-of-words (BoW) techniques, in which a given document is mapped onto a high-dimensional vector space [30]. The resulting vector can then be used to train a linear classifier, for example Linear Regression or SVM. There is also a significant volume of research in literature on the use of Random Forests; Cutler et al. show that Random Forests outperform linear methods for three ecology-related tasks [7]; Akinyelu and Adewumi achieve an high accuracy on classification of phishing emails using a combination of meticulously defined features [1], while Xu et al. optimise the classification accuracy by weighing the input features of the individual trees and excluding the ones that negatively affect the performance of the classifier [34].","refs":[{"start":144,"end":148,"marker":"bibr","target":"#b29"},{"start":585,"end":588,"marker":"bibr","target":"#b0"},{"start":778,"end":782,"marker":"bibr","target":"#b33"}]},{"text":"A rather novel language modeling technique consists in word embeddings, that is, dense vector representation of words, that have the property of carrying semantic information about words and their context. Word embeddings gained increasing interest in the latest years and are normally employed in a deep learning context as in [18,19]: documents are transformed using a pre-trained dictionary and are processed by the neural network which ultimately outputs a class label.","refs":[{"start":328,"end":332,"marker":"bibr","target":"#b17"},{"start":332,"end":335,"marker":"bibr","target":"#b18"}]}]},{"title":"Dataset and Gold Standard","paragraphs":[{"text":"The EThOS initiative is aimed at sharing information about UK Doctoral theses and at \"making the full texts openly available for researchers\". 2 The dataset used in this work consists of a corpus of PhD theses whose publication dates range from the second half of the Twentieth Century to the most recent years. Such corpus has been kindly made available by the staff of the EThOS service of the British Library, and consists in nearly half a million bibliographic records (namely 475, 383); records with empty abstract are 57.6% (overall 273, 665), while the abstract is present in 42.4% of such theses (that is, 201, 718). The corpus implements the following metadating schema:","refs":[{"start":143,"end":144,"marker":null,"target":"#foot_0"}]},{"text":"uketdterms:ethosid: the identifier of the record within the EThOS digital library; dc:title: the title of the thesis; dc:creator: the PhD student who authored the thesis; uketdterms:institution: the name of the University; dc:publisher: may differ from institution in some cases; dcterms:issued: year of publication; dcterms:abstract: abstract of the thesis (when available); dc:type: always \"Thesis or Dissertation\"; uketdterms:qualificationname: \"Thesis\", sometimes followed by area of study and University; uketdterms:qualificationlevel: \"Thesis\",\"Doctoral\" or similar; dc:identifier: pointer to the resourse in EThOS digital library; dc:source: pointer to the original location of the resourse (e.g., institutional website); dc:subjectxsi: empty for all records; dc:subject: synthetic description of the area of study. The above schema employs three different vocabularies to define the metadata of a document. Dublin Core Metadata Element Set (dc) and its extension DCMI Metadata Terms (dcterms) are both defined in the Dublin Core Schema [33], which features a number of attributes that can be used to describe a digital or physical resource within a collection (e.g., a book in a library), while the uketd dc namespace (uketdterms) is defined on top of dcterms and describes the core set of metadata for UK theses that are part of the EThOS dataset. 3Some descriptive statistics about the textual content of the records are reported in Table 1 that details total words, total unique words and average number of words per element, before and after preprocessing of the text. Such figures are computed also on the two subsets individuated based on the distinction between empty/valued abstract. We considered three fields: dc:subject, dc:title and dcterms:abstract, when available. We note that the presence vs. absence of the abstract is a key aspect for a record; in fact, records containing abstract information account for almost 95% of the total word count. We note that the preprocessing phase has higher impact on the elements containing abstract information (where we observe 42% reduction of the available words after the preprocessing step) with respect to the second one (27% decrease). This is because titles and subjects tend to be shorter and more synthetic, sometimes consisting only in a list of keywords or concepts, in contrast with abstracts that are more exhaustive and written in fully fledged natural language. This tendency is in fact evident in Table 1; the average number of words increases by a factor of 25 when the abstract is present. Finally, in Fig. 1 we plotted the distribution of the number of theses per year of publication. The graph is computed separately for the two subsets of data (with-without abstract information), and clearly shows that the records with abstract are more recent and concentrated in a smaller time span. Both distributions have their peak after year 2000, and drop right before 2020, in accord with intuition.","refs":[{"start":1044,"end":1048,"marker":"bibr","target":"#b32"},{"start":1357,"end":1358,"marker":null,"target":"#foot_1"},{"start":1449,"end":1450,"marker":"table","target":"#tab_0"},{"start":2480,"end":2481,"marker":"table","target":"#tab_0"},{"start":2586,"end":2587,"marker":"figure","target":"#fig_0"}]}]},{"title":"The System","paragraphs":[{"text":"The goal of our work is to identify as many philosophy theses as possible: as mentioned, the problem at hand is that of individuating the theses that are actually related to philosophy, by discarding all records related to similar though distinct disciplines. The original dataset is not annotated and so it does not contain any explicit information that we could use to pursue our goal. We had two options: (i) to apply an unsupervised learning technique on the structured data, such as clustering or topic modelling, in order to single out philosophy samples among the multitude of subjects; or (ii) to automatically annotate the data (i.e., an educated guess), and subsequently to use such annotated data to train a supervised learning algorithm. We chose the second option, and set up a binary classification framework where a record from the dataset is classified as either philosophical or non-philosophical. We hypothesised that a supervised model would have fit our use case way better that an unsupervised one. In fact, we can train the algorithm by providing specific examples of the two classes, which will result in a better-defined decision boundary.","refs":[]},{"text":"In the following we first describe the data employed and how we built a training set (based on a educated guess) and a test set (annotated by human experts). We then illustrate the training of a binary classifier (hereafter Random Forest Module), and elaborate on the learnt features. Finally, we introduce the Semantic Module, devised to refine the predictions of the Random Forest Module.","refs":[]}]},{"title":"Building the Training Set and the Test Set","paragraphs":[{"text":"The first step was devised to build the training and the test set based on an educated guess. This bootstrapping technique is a key aspect of the whole approach, as it allowed us to adopt supervised machine learning algorithms. In many settings, in fact, a first raw, automatic categorisation can be attempted in order to overcome the limitation due to the lack of labelled data. Given the huge number of documents in the corpus, we did not consider the option of manually annotating the data in order to select a significant sample. We instead employed a text-based extraction method to search for relevant documents in the dataset by using a regular expression. We searched in the field dc:subject of each document and selected all samples matching the keyword philosophy, or a meaningful part of it (i.e., the substring philosop). We consider such documents as positive examples, while we consider all the other ones as negative examples. Of course this strategy is not completely fault-free, in fact it is possible that a given document is philosophical even though there is no philosophy specified in the subject (e.g., Kant's reasoning).   Building the Test Set. To create the test set we randomly selected 500 documents from each of the two groups, thus creating a new set of 1, 000 samples. Such samples were manually annotated by two domain experts. Interestingly enough, even though they had access to the whole record (different from our system, that only sported the dc:title field), in some cases (around 20 out of thousand records) the domain experts could not make a clear decision. All such ambiguous cases were left out from the test set. We did not record the inter-annotator agreement, since only the records where the annotators agreed were retained. 4 The final test set was built by adding further randomly chosen records that were annotated by the experts, finally obtaining a balanced set of 500 philosophical records and 500 non-philosophical records.","refs":[{"start":1771,"end":1772,"marker":null,"target":"#foot_2"}]}]},{"title":"The Random Forest Module","paragraphs":[{"text":"We then trained the classifier to acquire a model for the categorisation problem at hand. 5 In order to train the classifier we considered only the terms in the dc:title field, which is available in all records of the dataset and that in almost all cases suffices also to human experts to classify the record. A preprocessing step was devised, in order to filter out stopwords and to normalise the text elements (please refer to Table 2 reporting the statistics of the dataset after the preprocessing step). We chose not to use abstracts to train the model, since they are not available for most records; nor we used such information at testing time, even if available. By doing so, we adopted a conservative stance, and we are aware that some helpful information is not used, thereby resulting in a lower bound to the performance of the categorisation system. The applied preprocessing steps are:","refs":[{"start":90,"end":91,"marker":null,"target":"#foot_3"},{"start":435,"end":436,"marker":"table","target":"#tab_1"}]},{"text":"1. Conversion to lowercase \"Today I baked 3 apple pies!\" →\"today i baked 3 apple pies!\". 2. Stop-words removal 6 \"today i baked 3 apple pies!\" →\"today baked 3 apple pies!\". 3. Punctuation removal \"today baked 3 apple pies!\" →\"today baked 3 apple pies\". 4. Numbers removal \"today baked 3 apple pies\" →\"today baked apple pies\". 5. Stemming7 \"today baked apple pies\" →\"today bake apple pie\". 6. Short document removal documents with n tokens < 3 are removed.","refs":[{"start":111,"end":112,"marker":null,"target":"#foot_4"},{"start":337,"end":338,"marker":null,"target":"#foot_5"}]},{"text":"After preprocessing, we transformed the documents into vectors using a bagof-words approach that maps each of them to the vector space with the tf-idf transformation. Tf-idf computes the frequency of terms in a document, weighed by the number of documents (within a given collection) that contain such term. This procedure favours important and more discriminative terms rather than common ones. The resulting dictionary vector (containing all meaningful terms in the collection) has been truncated to 60, 000 features, based on the frequency of the terms in the corpus, so to discard highly uncommon ones.","refs":[]},{"text":"The estimator that we employed to acquire a binary classifier is Random Forest [14]. This is an ensemble method that trains a set of decision trees by using random subsets of input features, and assigns to a given sample the class that is predicted more often among the different classifiers. This choice is motivated by the fact that Random Forest can handle a large number of features and provides a measure of their importance; this may be of particular interest to examine the intermediate stages of the computation. The output of the classifier includes the set of terms that are most probably predictive for a sample to be positive or negative for a given class label. However, several algorithms could be used in principle in this step, by plugging a different learner into the overall system. ","refs":[{"start":79,"end":83,"marker":"bibr","target":"#b13"}]}]},{"title":"Max depth none","paragraphs":[{"text":"We preprocessed each document in the training set and extracted the corresponding vector representation along with its label (which was computed based on the educated guess, as illustrated above). Given the binary categorisation setting, labels did encode only two classes: 'philosophy' and 'non-philosophy'.","refs":[]},{"text":"The training set was fed to the estimator, to extract significant patterns in the data and to learn how to exploit them to individuate philosophy theses. Table 4 shows some relevant configuration parameters.","refs":[{"start":160,"end":161,"marker":"table","target":"#tab_4"}]},{"text":"Figure 2 reports the 30 most important features, along with a relevance score, ranging over [0,1]. The score of a feature is computed for a single tree of the forest as the total decrease of node impurity brought by that feature, and is averaged among all trees. We also report the standard deviation of their values. We observe that the majority of such terms is highly predictive of a philosophical context, even though among the most relevant learnt features also terms proper to the Religion class are present (e.g., 'theology', 'church', 'religious', 'biblical'). 8For this reason we further investigated the score acquired for the features, with particular focus to philosophy-related ones:9 in Fig. 3 we show 30 salient philosophical terms, whose score is lower than that learnt for the term 'biblical', which is the rightmost feature portrayed in Fig. 2. Such scores were likely to negatively affect the recall of the Random Forest Module, which acquired inaccurate weights, probably due to the scarcity of philosophical training data and to the noise present in the educated guess. This is why we devised the other module that-independent of the information in the training set-relies on the knowledge available in BabelNet, as described in the following.","refs":[{"start":7,"end":8,"marker":"figure","target":"#fig_1"},{"start":92,"end":95,"marker":"bibr","target":null},{"start":95,"end":97,"marker":"bibr","target":"#b0"},{"start":569,"end":570,"marker":null,"target":"#foot_6"},{"start":696,"end":697,"marker":null,"target":"#foot_7"},{"start":706,"end":707,"marker":"figure","target":"#fig_2"},{"start":860,"end":861,"marker":"figure","target":"#fig_1"}]}]},{"title":"The Semantic Module","paragraphs":[{"text":"The semantic module performs some basic Information Extraction tasks, accessing the lexical conceptual resource of BabelNet [28]. BabelNet is a multilingual semantic network resulting from the integration of WordNet and Wikipedia; it builds on the constructive rationale of WordNet-that is, it relies on sets of synonyms, the Babel synsets-which is extended through the encyclopedic structure of Wikipedia. In particular, the nodes in the network represent concepts and entities (that is, persons, organisations and locations), and the edges intervening between each two nodes represent semantic relations (such as IsA, PartOf, etc.). Although further lexical resource exist containing different sorts of knowledge (such as, e.g., WordNet [27], ConceptNet [23], COVER [20,26], or a hybrid approach proposed by [12,21]), we chose to adopt BabelNet in that it ensures a broad coverage to concepts and entities as well, that in the present domain are particularly relevant. The semantic module aims at searching the terms present in the theses title, to individuate the underlying concept and then at checking whether they are either philosophical concepts (that is, linked to 'philosophy' in Net, and their corresponding synsets (that is, sets of synonyms along with their meaning identifiers) retrieved. At this stage of the computation, we discard the MWEs that are not present in BabelNet, thus implementing a semantic filtering for the terms individuated through the patterns described above. For each sense t or entity t associated to each extracted term t we inspect if it corresponds to philosophy (bn:00061984n) or philosopher (bn:00061979n), and whether it is linked to either concept. In doing so, we basically explore the relations IsA and Occupation, and we retain any sense t and entity t such that -[sense t ,entity t ] IsA philosophy/philosopher; or -[sense t ,entity t ] Occupation philosopher. Building the PHILO-ENTITIES array. Such elements are added to the description of the record for the thesis being classified, in the philosophicalentities set. We note that thanks to the linking of BabelNet synsets with external triple stores (such as Wikidata), such triples can be exploited to perform further analysis of the record, and of the entities herein contained.","refs":[{"start":124,"end":128,"marker":"bibr","target":"#b27"},{"start":739,"end":743,"marker":"bibr","target":"#b26"},{"start":756,"end":760,"marker":"bibr","target":"#b22"},{"start":768,"end":772,"marker":"bibr","target":"#b19"},{"start":772,"end":775,"marker":"bibr","target":"#b25"},{"start":810,"end":814,"marker":"bibr","target":"#b11"},{"start":814,"end":817,"marker":"bibr","target":"#b20"}]},{"text":"The decision rule of the semantic module is very simple: if the array of philosophical entities associated to this record is not empty, we label it as a philosophical one; we label it as a non-philosophical one, otherwise.","refs":[]},{"text":"The set of PHILO-ENTITIES can be used to build simple yet informative explanations of why a given record has been categorised as a philosophical one. This approach, based on simple templates such as the system described in [6] will be extended to build explanations also for non-philosophical records in next future. Let us consider, as an example, a record whose title is \"Dialectic in the philosophy of Ernst Bloch\"; this record has been marked as philosophical by human annotators. While processing this record, the Semantic Module detects the concepts philosophy (bn:00061984n) and dialectic (bn:00026827n) as associated to the concept philosophy, as well as the Named Entity Ernst Bloch (bn:03382194n) as a person whose occupation is that of philosopher.","refs":[{"start":223,"end":226,"marker":"bibr","target":"#b5"}]},{"text":"The semantic module is executed when the first module (implementing the Random Forest-based classifier) returns 0, that is when the record is not recognised as a philosophical one in the first stage of the computation.","refs":[]}]},{"title":"Evaluation","paragraphs":[{"text":"We evaluated the system on a test set composed of 1, 000 records, annotated by human experts, and built as described in Sect. 4.1. All modules of our system were run with only title information in input 12 . In the experimentation we recorded (i ) the results of the Random Forest Module alone (which is a state-ofthe-art algorithm, thus working as our baseline); (ii ) the results of the Semantic Module alone; and (iii ) the results of both modules, where the latter module is executed only in case a record is predicted to be non-philosophical by the former one. The results are presented in Table 5. Discussion. The system obtained encouraging results. First of all, as earlier mentioned, the dataset was strongly unbalanced, with a vast majority of records that were non-philosophical (please refer to Table 2), but with many records coming from closely related research fields. Namely, out of the overall 475K records, those concerned with philosophy were less than 3.5K, thus in the order of 0.7%. Yet, to conduct a thorough experimentation we restricted to considering only title information, thus often exploiting only a fraction of the available information. To consider in how far this limitation can be harmful to categorisation, let us consider that the human experts in some cases were not able to decide (or to decide consistently) the class of the considered records: when available, they had the opportunity to inspect the abstract and any other field of the record. Additionally, the assumption underlying the Semantic Module was rather fragile: just looking for people's occupation and for concepts hypernyms is a crude way to determine whether philosophy (or any other discipline) is mentioned. It was necessary to avoid more noisy relations in BabelNet (such as Semantical-lyRelated), that allow retrieving many more entities connected to the concept at hand, but in less controlled fashion. We observe that the Random Forest Module obtains a high precision, at the expense of a poor recall, caused by a significant number of false negatives, as expected by inspecting the feature weights. The attempt at correcting this behaviour has been at the base of the design of the semantic module; specifically, we strove to reduce the number of false negatives, meantime limiting the growth of the false positives. The key of the improvement in the recall (over 22%) obtained by the whole system with respect to the Random Forest Module is thus easily explained: the whole system incurs in false negatives in less than half cases, with a reduced increase of false positives.","refs":[{"start":601,"end":602,"marker":"table","target":"#tab_5"},{"start":813,"end":814,"marker":"table","target":"#tab_1"}]},{"text":"Provided that the explanatory features of the system were not object of the present experimentation, nonetheless we briefly report on this point, too, as about a preliminary test. An explanation is built only when a record is associated to either some philosopher(s) or to philosophical concept(s): it is thus presently conceived simply as a listing of the elements collected in the PHILO-ENTITIES array. The system generated overall 496 explanations: in 394 cases this correctly happened for a philosophical record (thus in 78.8% of cases), whilst in 102 cases an explanation was wrongly built for non-philosophical records. Interestingly enough, when both modules agree on recognising a record as a philosophical one, the PHILO-ENTITIES array contains on average 1.78 elements; when the Random Forest Module predicts 'non-philosophical' label and Semantic Module (correctly) overwrites this prediction, the PHILO-ENTITIES array contains on average 1.57 elements. Thus less information is available also to the Semantic Module, which can be interpreted as a recognition that records misclassified by the Random Forest Module are objectively more difficult. However, further investigation is required to properly interpret this datum, and to select further semantic relations in BabelNet.","refs":[]}]},{"title":"Conclusions","paragraphs":[{"text":"In this paper we have presented a system for categorising bibliographic records, to automatically the metadata about the subject of the record. The research question underlying this work was basically how to integrate domain specific knowledge (acquired by training a learning system) and general knowledge (embodied in the BabelNet semantic network). As we pointed out, the difficulty of the present task was caused by the unfavourable bootstrapping conditions.","refs":[]},{"text":"We have described the EThOS dataset, and illustrated the methodology adopted: based on the educated guess, we tentatively classified all records. After partitioning the data between training and test set, we trained a Random Forest learner to acquire a classifier for the training set. On the other side, we developed the Semantic Module, which is charged to extract concepts and entities from the title field of the records, exploiting the BabelNet semantic network. The evaluation revealed that the system integrating both modules works better than the individual software modules: we obtained interesting results. Future work will include improving the explanation, exploring additional semantic relations, and considering further knowledge bases.","refs":[]}]}],"tables":{"tab_0":{"heading":"Table 1 .","description":"Statistics about the textual content of the dataset. The values reported are computed on subject, title, and (if present) abstract of the record.","rows":[["Measure","With abstract Without abstract Whole dataset"],["Words","64, 946, 071","3, 480, 858","68, 426, 929"],["Words after preprocessing","37, 875, 285","2, 548, 988","40, 424, 273"],["Unique words","1, 496, 488","258, 045","1, 610, 896"],["Unique words after","435, 825","124, 650","476, 769"],["preprocessing","","",""],["Average words per record","321.96","12.72","143.94"],["Average words per record","187.76","9.31","85.03"],["after preprocessing","","",""]]},"tab_1":{"heading":"Table 2 .","description":"Statistics of the dataset, partitioned into philosophical and non-philosophical records based on the educated guess.","rows":[["Corpus","Philosophy Not Philosophy Total"],["With abstracts","1, 495","200, 223","201, 718"],["Without abstracts 1, 982","271, 683","273, 665"],["Whole dataset","3, 477","471, 906","475, 383"]]},"tab_2":{"heading":"Table 2","description":"illustrates how the records in the dataset have been classified, based on this simple partitioning rule. We observe that only 3, 477 records (that is 0.73% of the whole dataset), were initially recognised as pertaining to philosophy theses, while the remaining 471, 906 are negative examples from all other fields of study.Building the Training Set. We then left out 500 randomly chosen records from the positive examples and as many records from the negative examples that were used at a later time in order to build a test set. The final training set is thus composed of 2, 977 positive and 471, 406 negative examples. Not all negative examples were actually used to train the classifier: the training set has been built by randomly selecting a number of negative samples that outnumbers its positive counterpart by a factor of 10, thereby resulting in 29, 770 records. Some descriptive statistics of the training set are reported in Table3.","rows":[]},"tab_3":{"heading":"Table 3 .","description":"Basic statistics about the textual content of the training set. The values reported are computed only on title.","rows":[["Measure","Value"],["Total words","371,478"],["Total words after preprocessing","249,266"],["Total unique words","53,338"],["Total unique words after preprocessing","34,549"],["Average words per document","11.41"],["Average words per document after preprocessing 7.65"]]},"tab_4":{"heading":"Table 4 .","description":"Configuration parameters used for the Random Forest classifier. Namely, 50 decision trees were trained, each of them assigning either a class label 0 or 1 to a given vector. The final class label will be the most frequent one. Other parameters are kept as their default value.","rows":[["Parameter","Value"],["Number of estimators 50"],["Max features","0.6"],["Random state","none"]]},"tab_5":{"heading":"Table 5 .","description":"Categorisation results obtained on the test set by experimenting with the Random Forest Module, with the Semantic Module, and with their combination.","rows":[["","RF","SEM RF+SEM"],["Precision 0.8227 0.8269 0.7944"],["Recall","0.5660 0.6400 0.7880"],["F1","0.6706 0.7215 0.7912"],["Accuracy 0.7220 0.7530 0.7920"]]}},"abstract":{"title":"Abstract","paragraphs":[{"text":"In this paper we illustrate a system aimed at solving a longstanding and challenging problem: acquiring a classifier to automatically annotate bibliographic records by starting from a huge set of unbalanced and unlabelled data. We illustrate the main features of the dataset, the learning algorithm adopted, and how it was used to discriminate philosophical documents from documents of other disciplines. One strength of our approach lies in the novel combination of a standard learning approach with a semantic one: the results of the acquired classifier are improved by accessing a semantic network containing conceptual information. We illustrate the experimentation by describing the construction rationale of training and test set, we report and discuss the obtained results and conclude by drawing future work.","refs":[]}]}}