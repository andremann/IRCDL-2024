{"bibliography":{"title":"ConNeKTion: A Tool for Handling Conceptual Graphs Automatically Extracted from Text","authors":[{"person_name":{"surname":"Leuzzi","first_name":"Fabio"},"affiliations":[{"department":"Dipartimento di Informatica","institution":"Università di Bari","laboratory":null}],"email":"fabio.leuzzi@uniba.it"},{"person_name":{"surname":"Ferilli","first_name":"Stefano"},"affiliations":[{"department":"Dipartimento di Informatica","institution":"Università di Bari","laboratory":null},{"department":"Centro Interdipartimentale per la Logica e sue Applicazioni","institution":"Università di Bari","laboratory":null}],"email":"stefano.ferilli@uniba.it"},{"person_name":{"surname":"Rotella","first_name":"Fulvio"},"affiliations":[{"department":"Dipartimento di Informatica","institution":"Università di Bari","laboratory":null}],"email":"fulvio.rotella@uniba.it"}],"date":null,"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"keywords":[],"citations":{"b0":{"title":"Style mining of electronic messages for multiple authorship discrimination: first results","authors":[{"person_name":{"surname":"Argamon","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Saric","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Stein","first_name":"S"},"affiliations":[],"email":null}],"date":{"year":"2003","month":null,"day":null},"ids":null,"target":null,"publisher":"ACM","journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":475,"to_page":480}}},"b1":{"title":"Learning concept hierarchies from text corpora using formal concept analysis","authors":[{"person_name":{"surname":"Cimiano","first_name":"P"},"affiliations":[],"email":null},{"person_name":{"surname":"Hotho","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Staab","first_name":"S"},"affiliations":[],"email":null}],"date":{"year":"2005","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"J. Artif. Int. Res","series":null,"scope":{"volume":24,"pages":{"from_page":305,"to_page":339}}},"b2":{"title":"Generating typed dependency parses from phrase structure trees","authors":[{"person_name":{"surname":"De Marneffe","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Maccartney","first_name":"B"},"affiliations":[],"email":null},{"person_name":{"surname":"Manning","first_name":"C"},"affiliations":[],"email":null}],"date":{"year":"2006","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b3":{"title":"Improving Information Retrieval with Latent Semantic Indexing","authors":[{"person_name":{"surname":"Deerwester","first_name":"S"},"affiliations":[],"email":null}],"date":{"year":"1988","month":"10","day":null},"ids":null,"target":null,"publisher":"American Society for Information Science","journal":null,"series":null,"scope":{"volume":25,"pages":null}},"b4":{"title":"Concept decompositions for large sparse text data using clustering","authors":[{"person_name":{"surname":"Dhillon","first_name":"I"},"affiliations":[],"email":null},{"person_name":{"surname":"Modha","first_name":"D"},"affiliations":[],"email":null}],"date":{"year":"2001","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":143,"to_page":175}}},"b5":{"title":"Authorship attribution with support vector machines","authors":[{"person_name":{"surname":"Diederich","first_name":"J"},"affiliations":[],"email":null},{"person_name":{"surname":"Kindermann","first_name":"J"},"affiliations":[],"email":null},{"person_name":{"surname":"Leopold","first_name":"E"},"affiliations":[],"email":null},{"person_name":{"surname":"Paass","first_name":"G"},"affiliations":[],"email":null}],"date":{"year":"2003","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"Applied Intelligence","series":null,"scope":{"volume":19,"pages":{"from_page":109,"to_page":123}}},"b6":{"title":"WordNet: An Electronic Lexical Database","authors":[],"date":{"year":"1998","month":null,"day":null},"ids":null,"target":null,"publisher":"MIT Press","journal":null,"series":null,"scope":null},"b7":{"title":"Combining qualitative and quantitative keyword extraction methods with document layout analysis","authors":[{"person_name":{"surname":"Ferilli","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Biba","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Basile","first_name":"T"},"affiliations":[],"email":null},{"person_name":{"surname":"Esposito","first_name":"F"},"affiliations":[],"email":null}],"date":{"year":"2009","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":22,"to_page":33}}},"b8":{"title":"Plugging taxonomic similarity in first-order logic horn clauses comparison","authors":[{"person_name":{"surname":"Ferilli","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Biba","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Di Mauro","first_name":"N"},"affiliations":[],"email":null},{"person_name":{"surname":"Basile","first_name":"T"},"affiliations":[],"email":null},{"person_name":{"surname":"Esposito","first_name":"F"},"affiliations":[],"email":null}],"date":{"year":"2009","month":null,"day":null},"ids":null,"target":null,"publisher":"Springer","journal":null,"series":null,"scope":{"volume":5883,"pages":{"from_page":131,"to_page":140}}},"b9":{"title":"Cooperating techniques for extracting conceptual taxonomies from text","authors":[{"person_name":{"surname":"Ferilli","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Leuzzi","first_name":"F"},"affiliations":[],"email":null},{"person_name":{"surname":"Rotella","first_name":"F"},"affiliations":[],"email":null}],"date":{"year":"2011","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b10":{"title":"Error detecting and error correcting codes","authors":[{"person_name":{"surname":"Hamming","first_name":"R"},"affiliations":[],"email":null}],"date":{"year":"1950","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"Bell System Technical Journal","series":null,"scope":{"volume":29,"pages":{"from_page":147,"to_page":160}}},"b11":{"title":"Effective Presentation: How to Create and Deliver a Winning Presentation","authors":[{"person_name":{"surname":"Jay","first_name":"R"},"affiliations":[],"email":null},{"person_name":{"surname":"Jay","first_name":"A"},"affiliations":[],"email":null}],"date":{"year":"2004","month":null,"day":null},"ids":null,"target":null,"publisher":"Prentice Hall","journal":null,"series":null,"scope":null},"b12":{"title":"Pictures of relevance: A geometric analysis of similarity measures","authors":[{"person_name":{"surname":"Jones","first_name":"W"},"affiliations":[],"email":null},{"person_name":{"surname":"Furnas","first_name":"G"},"affiliations":[],"email":null}],"date":{"year":"1987","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"Journal of the American Society for Information Science","series":null,"scope":{"volume":38,"pages":{"from_page":420,"to_page":442}}},"b13":{"title":"Concept indexing: A fast dimensionality reduction algorithm with applications to document retrieval and categorization","authors":[{"person_name":{"surname":"Karypis","first_name":"G"},"affiliations":[],"email":null},{"person_name":{"surname":"Han","first_name":"E.-H.(s"},"affiliations":[],"email":null}],"date":{"year":"2000","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":2000,"to_page":2000}}},"b14":{"title":"Fast exact inference with a factored model for natural language parsing","authors":[{"person_name":{"surname":"Klein","first_name":"D"},"affiliations":[],"email":null},{"person_name":{"surname":"Manning","first_name":"C"},"affiliations":[],"email":null}],"date":{"year":"2003","month":null,"day":null},"ids":null,"target":null,"publisher":"MIT Press","journal":null,"series":null,"scope":{"volume":15,"pages":null}},"b15":{"title":"Improving robustness and flexibility of concept taxonomy learning from text","authors":[{"person_name":{"surname":"Leuzzi","first_name":"F"},"affiliations":[],"email":null},{"person_name":{"surname":"Ferilli","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Rotella","first_name":"F"},"affiliations":[],"email":null}],"date":{"year":"2013","month":null,"day":null},"ids":null,"target":null,"publisher":"Springer","journal":null,"series":null,"scope":{"volume":7765,"pages":{"from_page":170,"to_page":184}}},"b16":{"title":"Mining ontologies from text","authors":[{"person_name":{"surname":"Maedche","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Staab","first_name":"S"},"affiliations":[],"email":null}],"date":{"year":"2000","month":null,"day":null},"ids":null,"target":null,"publisher":"Springer","journal":null,"series":null,"scope":{"volume":1937,"pages":{"from_page":189,"to_page":202}}},"b17":{"title":"The text-to-onto ontology learning environment","authors":[{"person_name":{"surname":"Maedche","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Staab","first_name":"S"},"affiliations":[],"email":null}],"date":{"year":"2000","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b18":{"title":"Keyword extraction from a single document using word co-occurrence statistical information","authors":[{"person_name":{"surname":"Matsuo","first_name":"Y"},"affiliations":[],"email":null},{"person_name":{"surname":"Ishizuka","first_name":"M"},"affiliations":[],"email":null}],"date":{"year":"2003","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"International Journal on Artificial Intelligence Tools","series":null,"scope":{"volume":13,"pages":{"from_page":2004,"to_page":2004}}},"b19":{"title":"A formal ontology discovery from web documents","authors":[{"person_name":{"surname":"Ogata","first_name":"N"},"affiliations":[],"email":null}],"date":{"year":"2001","month":null,"day":null},"ids":null,"target":null,"publisher":"Springer","journal":"LNCS (LNAI","series":null,"scope":{"volume":2198,"pages":{"from_page":514,"to_page":519}}},"b20":{"title":"Evaluation of OntoLearn, a methodology for automatic population of domain ontologies","authors":[{"person_name":{"surname":"Cucchiarelli","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Velardi","first_name":"P"},"affiliations":[],"email":null},{"person_name":{"surname":"Navigli","first_name":"R"},"affiliations":[],"email":null},{"person_name":{"surname":"Neri","first_name":"F"},"affiliations":[],"email":null}],"date":{"year":"2006","month":null,"day":null},"ids":null,"target":null,"publisher":"IOS Press","journal":null,"series":null,"scope":null},"b21":{"title":"Problog: a probabilistic prolog and its application in link discovery","authors":[{"person_name":{"surname":"De Raedt","first_name":"L"},"affiliations":[],"email":null},{"person_name":{"surname":"Kimmig","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Toivonen","first_name":"H"},"affiliations":[],"email":null}],"date":{"year":"2007","month":null,"day":null},"ids":null,"target":null,"publisher":"AAAI Press","journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":2468,"to_page":2473}}},"b22":{"title":"","authors":[{"person_name":{"surname":"Robertson","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Walker","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Jones","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Hancock-Beaulieu","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Gatford","first_name":"M"},"affiliations":[],"email":null}],"date":{"year":"1996","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"Okapi at trec","series":null,"scope":{"volume":3,"pages":{"from_page":109,"to_page":126}}},"b23":{"title":"A domain based approach to information retrieval in digital libraries","authors":[{"person_name":{"surname":"Rotella","first_name":"F"},"affiliations":[],"email":null},{"person_name":{"surname":"Ferilli","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Leuzzi","first_name":"F"},"affiliations":[],"email":null}],"date":{"year":"2013","month":null,"day":null},"ids":null,"target":null,"publisher":"Springer","journal":null,"series":null,"scope":{"volume":354,"pages":{"from_page":129,"to_page":140}}},"b24":{"title":"The SMART Retrieval System-Experiments in Automatic Document Processing","authors":[{"person_name":{"surname":"Salton","first_name":"G"},"affiliations":[],"email":null}],"date":{"year":"1971","month":null,"day":null},"ids":null,"target":null,"publisher":"Prentice-Hall, Inc","journal":null,"series":null,"scope":null},"b25":{"title":"Introduction to Modern Information Retrieval","authors":[{"person_name":{"surname":"Salton","first_name":"G"},"affiliations":[],"email":null},{"person_name":{"surname":"Mcgill","first_name":"M"},"affiliations":[],"email":null}],"date":{"year":"1984","month":null,"day":null},"ids":null,"target":null,"publisher":"McGraw-Hill Book Company","journal":null,"series":null,"scope":null},"b26":{"title":"A vector space model for automatic indexing","authors":[{"person_name":{"surname":"Salton","first_name":"G"},"affiliations":[],"email":null},{"person_name":{"surname":"Wong","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Yang","first_name":"C"},"affiliations":[],"email":null}],"date":{"year":"1975","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"Commun. ACM","series":null,"scope":{"volume":18,"pages":{"from_page":613,"to_page":620}}},"b27":{"title":"A statistical learning method for logic programs with distribution semantics","authors":[{"person_name":{"surname":"Sato","first_name":"T"},"affiliations":[],"email":null}],"date":{"year":"1995","month":null,"day":null},"ids":null,"target":null,"publisher":"MIT Press","journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":715,"to_page":729}}},"b28":{"title":"Pivoted document length normalization","authors":[{"person_name":{"surname":"Singhal","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Buckley","first_name":"C"},"affiliations":[],"email":null},{"person_name":{"surname":"Mitra","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Mitra","first_name":"A"},"affiliations":[],"email":null}],"date":{"year":"1996","month":null,"day":null},"ids":null,"target":null,"publisher":"ACM Press","journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":21,"to_page":29}}},"b29":{"title":"Neural network applications in stylometry: The federalist papers","authors":[{"person_name":{"surname":"Tweedie","first_name":"F"},"affiliations":[],"email":null},{"person_name":{"surname":"Singh","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Holmes","first_name":"D"},"affiliations":[],"email":null}],"date":{"year":"1996","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"Computers and the Humanities","series":null,"scope":{"volume":30,"pages":{"from_page":1,"to_page":10}}},"b30":{"title":"Verbs semantics and lexical selection","authors":[{"person_name":{"surname":"Wu","first_name":"Z"},"affiliations":[],"email":null},{"person_name":{"surname":"Palmer","first_name":"M"},"affiliations":[],"email":null}],"date":{"year":"1994","month":null,"day":null},"ids":null,"target":null,"publisher":"ACL","journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":133,"to_page":138}}}},"sections":[{"title":"Introduction","paragraphs":[{"text":"The spread of the electronic technology has had a dramatic impact on the production of documents in all fields of knowledge, and has led to the flourishing of document repositories aimed at supporting scholars and non-technically aware users in carrying out their tasks and satisfying their information needs. However the study, understanding and exploitation of the content of a digital library, and the extraction of useful information thereof, are complex activities requiring automatic techniques that can effectively support the users. In this landscape, a relevant role can be played by concept taxonomies that express both common sense and domain-specific information, including implicit relationships among the concepts underlying the collection. Unfortunately, the availability of such a kind of resources is limited, and their manual building and maintenance are costly and error-prone. A possible solution is the exploitation of Natural Language Processing (NLP) techniques, that work on the textual parts of the documents to extract the concepts and relationships expressed by words. Although the task is not trivial, due to the intrinsic ambiguity of natural language and to the huge amount of required common sense and linguistic/conceptual background knowledge, even small portions of such a knowledge may significantly improve understanding performance, at least in limited domains. This work presents Con-NeKTion (acronym for 'CONcept NEtwork for Knowledge representaTION'), a tool for conceptual graph learning and exploitation. It allows to learn conceptual graphs 1 from plain text and to enrich them by finding concept generalizations. The resulting graph can be used for several purposes: finding relationships between concepts (if any), filtering the concepts from a particular perspective, keyword extraction, information retrieval and author identification. Specifically, this paper focuses on the graphical control panel provided to the user for exploiting the various functionalities, while technical details and evaluation of the single functionalities have been already presented in [10,16,24].","refs":[{"start":1584,"end":1585,"marker":null,"target":"#foot_1"},{"start":2112,"end":2116,"marker":"bibr","target":"#b9"},{"start":2116,"end":2119,"marker":"bibr","target":"#b15"},{"start":2119,"end":2122,"marker":"bibr","target":"#b23"}]},{"text":"The paper is organized as follows: the next section describes related works that have some connection to the present proposal, described in Section 3; then, Section 4 describes the tool aimed at supporting end users in managing and exploiting the learned conceptual graph. In the last section, some considerations and future work issues are proposed.","refs":[]}]},{"title":"Related Work","paragraphs":[{"text":"A primary task in this work is the construction of a conceptual graph starting from the analysis of the plain text contained in the documents that make up a collection. Several techniques are present in the literature generally aimed at building some kind of graph-like structure, that have made the basis on which the state of the art specifically aimed at building taxonomies and ontologies from text has built its approaches. [2] builds concept hierarchies using Formal Concept Analysis: objects are grouped using algebraic techniques based on their descriptive attributes, which are determined from text linking terms with verbs. Different approaches are also available. [18,17] build ontologies by labeling taxonomic relations only; in our opinion, also non-taxonomic relationships are very important to improve text understanding, such as those associated to actions (and expressed by verbs). [21] builds taxonomies considering only concepts that are present in a domain but do not appear in others; however, one might be interested in collecting and organizing all concepts that can be recognized in a collection, because generic ones may help to frame and connect domain-specific ones. [20] defines a language to build formal ontologies, but this level is very hard to be effectively reached, so a sensible trade-off between expressive power and practical feasibility might better focus on working at the lexical level (at least in the current state of the art).","refs":[{"start":429,"end":432,"marker":"bibr","target":"#b1"},{"start":675,"end":679,"marker":"bibr","target":"#b17"},{"start":679,"end":682,"marker":"bibr","target":"#b16"},{"start":899,"end":903,"marker":"bibr","target":"#b20"},{"start":1194,"end":1198,"marker":"bibr","target":"#b19"}]},{"text":"Our proposal to learning conceptual graphs from text relies on pre-processing techniques taken from the field of NLP, that provide a formal and structured representation of the sentences on which the actual graph construction and reasoning operators can be applied. As regards the syntactic analysis of the input text, the Stanford Parser and Stanford Dependencies [15,3] are two well-known tools that can be trained for any language for the identification of the most likely syntactic structure of sentences (including active/passive and positive/negative forms), and specifically their 'subject' or '(direct/indirect) object' components.","refs":[{"start":365,"end":369,"marker":"bibr","target":"#b14"},{"start":369,"end":371,"marker":"bibr","target":"#b2"}]},{"text":"They also normalize the words in the input text using lemmatization instead of stemming in order to preserve the grammatical role of the original word (and improve readability by humans). Due to the wide range of tools available for English, compared to other languages, we will focus on this language in the following.","refs":[]},{"text":"Also, we need in some steps of our technique to assess the similarity among concepts in a given conceptual taxonomy. A classical, general measure, is the Hamming distance [11], that works on pairs of equal-length vectorial descriptions and counts the number of changes required to turn one into the other. Other measures, specific for conceptual taxonomies, are [9] (that adopts a global approach based on the whole set of super-concepts) and [31] (that focuses on a particular path between the nodes to be compared).","refs":[{"start":171,"end":175,"marker":"bibr","target":"#b10"},{"start":362,"end":365,"marker":"bibr","target":"#b8"},{"start":443,"end":447,"marker":"bibr","target":"#b30"}]},{"text":"Another technology we use is ProbLog [22] to apply probabilistic reasoning on the extracted knowledge. It is essentially Prolog where all clauses are labeled with the probability that they are true, that in turn can be extracted from large databases by various techniques. A ProbLog program T = {p1 : c1, ..., pn : cn} specifies a probability distribution over all its possible non-probabilistic subprograms according to the theoretical basis in [28]. The semantics of ProbLog is then defined by the success probability of a query, which corresponds to the probability that it succeeds in a randomly sampled program. Indeed, the program can be split into a set of labeled facts p i :: f i , meaning that f i is a fact with probability of occurrence p i , and a Prolog program using those facts, which encodes the background knowledge (BK ). Probabilistic facts correspond to mutually independent random variables, which together define a probability distribution over all ground logic programs L ⊆ L T (where L T is the set of all f i 's):","refs":[{"start":37,"end":41,"marker":"bibr","target":"#b21"},{"start":446,"end":450,"marker":"bibr","target":"#b27"}]},{"text":"In this setting we will use the term possible world to denote the least Herbrand model of a subprogram L together with BK, and we will denote by L both the set of sampled facts and the corresponding world.","refs":[]},{"text":"A possible exploitation of the learned conceptual graph is for Information Retrieval (IR) purposes, so a quick overview of this field of research may be useful as well. Most existing works that tackle the IR problem are based on the so-called Vector Space Model (VSM), originally proposed in [27]. This approach represents a corpus of documents D, and the set of terms T appearing therein, as a T × D matrix, in which the (i, j)-th cell reports a weight representing the importance of the i-th term in the j-th document (usually computed according to both the number of its occurrences in that document and its distribution in the whole collection). Many similarity approaches [13,26] and weighting schemes [25,23,29] have been proposed. Based on this representation, the degree of similarity of a user query to any document in the collection can be computed, simply using any geometric distance measure (e.g., the cosine measure) on that space. One limitation of these approaches is their considering a document only from a lexical point of view, which is typically affected by several kinds of linguistic tricks, such as synonymy and polysemy. More recently, techniques based on dimensionality reduction have been explored with the aim to map both the documents in the corpus and the queries into a lower dimensional space that explicitly takes into account the dependencies between terms, in order to improve the retrieval or categorization performance. Prominent examples are Latent Semantic Indexing [4] (a statistical method based on Singular Value Decomposition that is capable of retrieving texts based on the concepts they contain, not just by matching terms) and Concept Indexing [14] (that exploits Concept Decomposition [5] instead of Singular Value Decomposition).","refs":[{"start":292,"end":296,"marker":"bibr","target":"#b26"},{"start":677,"end":681,"marker":"bibr","target":"#b12"},{"start":681,"end":684,"marker":"bibr","target":"#b25"},{"start":707,"end":711,"marker":"bibr","target":"#b24"},{"start":711,"end":714,"marker":"bibr","target":"#b22"},{"start":714,"end":717,"marker":"bibr","target":"#b28"},{"start":1505,"end":1508,"marker":"bibr","target":"#b3"},{"start":1690,"end":1694,"marker":"bibr","target":"#b13"},{"start":1732,"end":1735,"marker":"bibr","target":"#b4"}]}]},{"title":"Provided Functionalities","paragraphs":[{"text":"ConNeKTion aims at partially simulating some human abilities in the text understanding and concept formation activity, such as: extracting the concepts expressed in given texts and assessing their relevance; obtaining a practical description of the concepts underlying the terms, which in turn would allow to generalize concepts having similar descriptions; applying some kind of reasoning 'by association', that looks for possible indirect connections between two identified concepts; identifying relevant keywords that are present in the text and helping the user in the retrieval of useful information; building a model of the author's writing style through a relational description of the syntactical structure of the sentences, in order to understand whether two documents have been written by the same author or not. The system takes as input texts in natural language, and process them to build a conceptual network that supports the above objectives. The resulting network can be considered as an intensional representation of a collection of documents. Translating it into a suitable First-Order Logic (FOL) formalism allows the subsequent exploitation of logic inference engines in applications that use that knowledge.","refs":[]}]},{"title":"Graph Learning","paragraphs":[{"text":"ConNeKTion exploits a mix of existing tools and techniques, that are brought to cooperation in order to reach the above objectives, extended and supported by novel techniques when needed.","refs":[]},{"text":"Natural language texts are processed by the Stanford Parser in order to extract triples of the form subject, verb, complement , that will represent the concepts (the subject s and complements) and relationships (verbs) for the graph. Some representational tricks are adopted: indirect complements are treated as direct ones by embedding the corresponding preposition into the verb; sentences involving verb 'to be' or nouns with adjectives contributed in building the sub-class structure of the taxonomy (e.g., \"the penguin is a bird\" yields is a(penguin,bird )). Specifically, 'is a' relationships are exploited to build the taxonomy. The representation formalism was enriched by including the sentence's positive or negative form based on the absence or presence (respectively) of a negation modifier for the verb in the corresponding syntactic tree. The frequency of each arc between the concepts in positive and negative sentences were taken into account separately. This made our solution more robust, laying the basis for a statistical approach that inspects the obtained taxonomy by filtering out all portions that do not pass a given level of reliability.","refs":[]},{"text":"The graph so built embeds formal descriptions of concepts, on which the use of generalizations provides many opportunities of enrichment and/or manipulations on the graph. It can be used to build taxonomic structures, also after the addition of new text (possibly causing the presence of new nodes in the graph); to shift the representation, by removing the generalized nodes from the graph and leaving just their generalization (that inherits all their relationships to other concepts); to extend the amount of relationships between concepts belonging to the same connected component of the graph, or to build bridges between disjoint components that open new reasoning paths (which improves the effectiveness of reasoning 'by association'). Given two concepts G and C, G generalizes C if anything that can be labeled as C can be labeled as G as well, but not vice-versa [16]. The generalization procedure is made up of three steps: Concept Grouping, in which all concepts are grossly partitioned to obtain subsets of concepts (we group similar concepts if the aim is to enrich the relationships, or dissimilar ones in the bridging perspective); Word Sense Disambiguation, that associates a single meaning to each term by solving possible ambiguities using the domain of discourse; Computation of taxonomic similarity, in which WordNet [7] is exploited in order to further filter with an external source the groups found in step 1, and to choose an appropriate subsumer.","refs":[{"start":872,"end":876,"marker":"bibr","target":"#b15"},{"start":1337,"end":1340,"marker":"bibr","target":"#b6"}]}]},{"title":"Reasoning by Association","paragraphs":[{"text":"We intend 'reasoning by association' in a given conceptual graph as the task of finding a path of pairwise related concepts that establishes an indirect interaction between two concepts [16]. Our tool provides two different strategies for doing this: one works in breadth and returns the minimal path (in the number of traversed edges) between concepts, also specifying all involved relations; the other works in depth and allows to answer probabilistic queries on the conceptual graph.","refs":[{"start":186,"end":190,"marker":"bibr","target":"#b15"}]},{"text":"In more details, the former strategy looks for a minimal path starting two Breadth-First Search (BFS) procedures, one for each concept under consideration, until their boundaries meet. It also provides the number of positive/negative instances, and the corresponding ratios over the total, in order to express different gradations (such as permitted, prohibited, typical, rare, etc.) of actions between two objects. While this value does not affect the reasoning strategy, it allows to distinguish which reasoning path is more suitable for a given task. Note that this is different than the standard spreading activation algorithm, in that (1) we do not impose weights on arcs (we just associate arcs with symbolic labels expressing their semantics) nor any threshold for graph traversal, (2) we focus on paths rather than nodes, and specifically we are interested in the path(s) between two particular nodes rather than in the whole graph activation, hence (3) it makes no sense in our approach setting the initial activation weight of start nodes, and (4) this allows us to exploit a bi-directional partial search rather than a mono-directional complete graph traversal.","refs":[]},{"text":"Since real world data are typically noisy and uncertain, the latter strategy was included, that softens the classical rigid logical reasoning. This is obtained by suitably weighting the arcs/relationships among concepts to represent their likelihood among all possible worlds, and using these weights to prefer some paths over others. ProbLog [22] is exploited for this purpose, whose descriptions are based on the formalism p i :: f i where f i is a ground literal having probability p i . In our case, f i is of the form link (subject, verb, complement) and p i is the ratio between the sum of all examples for which f i holds and the sum of all possible links between subject and complement. Again, this is different than spreading activation because the ProbLog strategy is adopted.","refs":[{"start":343,"end":347,"marker":"bibr","target":"#b21"}]}]},{"title":"Keyword Extraction","paragraphs":[{"text":"The identification of relevant nodes in the graph may in some sense correspond to selecting keywords that provide indications on the main topics treated in the collection. As a first step, the frequency of each term is computed (its spread through the collection is ignored, to allow the incremental addition of new texts without the need of recomputing this statistics). Then, the EM clustering approach provided by Weka based on the Euclidean distance is applied to row vectors (representing concepts in the graph). Finally, various Keyword Extraction techniques, based on different (and complementary) aspects, perspectives and theoretical principles, are applied on the input texts to identify relevant concepts. We mixed a quantitative approach based on co-occurrences [19], a qualitative one based on WordNet [8] and a novel psychological one based on word positions. Assuming that humans tend to place relevant terms/concepts toward the start and end of sentences and discourses, where the attention of the reader/listener is higher [12], this approach determines the chance of a term being a keyword based on its position in the sentence/discourse. In particular, a mixture model determined by two Gaussian curves, whose peaks are placed around the extremes of the portion of text to be examined, is used. The outcomes of these techniques are exploited to compute a compound Relevance Weight for each node in the network. Then, nodes are ranked by decreasing Relevance Weight, and a suitable cut-point in the ranking is determined to distinguish relevant concepts from irrelevant ones. We cut the list at the first item in the ranking such that the difference in relevance weight from the next item is greater or equal than the maximum difference between all pairs of adjacent items, smoothed by a user-defined parameter p ∈ [0, 1] [10].","refs":[{"start":774,"end":778,"marker":"bibr","target":"#b18"},{"start":815,"end":818,"marker":"bibr","target":"#b7"},{"start":1040,"end":1044,"marker":"bibr","target":"#b11"}]}]},{"title":"Information Retrieval","paragraphs":[{"text":"The set of obtained representative keywords for each document can be considered as a higher-level representation of the digital library's content, and hence keyword extraction also work as a pre-processing step toward Information Retrieval in the library itself [24]. Indeed, to each keyword a corresponding meaning can be associated as follows: each keyword in the document is mapped to a corresponding synset (i.e., the code of a concept) in WordNet, that is taken as its semantic representative, using Word Sense Disambiguation techniques [10]. The output such a step, for each document, is a list of pairs, consisting of keywords and their associated synsets. All these synsets are partitioned into different groups using pairwise clustering. Then, each document is considered in turn, and each of its keywords 'votes' for the cluster to which the associated synset has been assigned. The aim is finding groups of similar synsets that might be usefully exploited as a kind of 'glue' binding together subsets of documents that are consistent with each other. In this perspective, the obtained clusters can be interpreted as intensional representations of specific domains, and thus they can be exploited to retrieve the sub-collection they are associated to. In this setting, a query in natural language is processed in order to recognize the relevant terms, and consequently find the corresponding synsets. At this point, a similarity evaluation (using the function in [8]) is performed against each cluster (that has a list of associated documents). The best result is used to obtain the list of documents by descending relevance, that can be used as an answer to the user's search.","refs":[{"start":262,"end":266,"marker":"bibr","target":"#b23"},{"start":542,"end":546,"marker":"bibr","target":"#b9"},{"start":1473,"end":1476,"marker":"bibr","target":"#b7"}]}]},{"title":"Author Identification","paragraphs":[{"text":"This functionality wants to face a well-known problem [1,6,30]: given a set of documents by a single author and a questioned document, determine whether the questioned document was written by that particular author or not.","refs":[{"start":54,"end":57,"marker":"bibr","target":"#b0"},{"start":57,"end":59,"marker":"bibr","target":"#b5"},{"start":59,"end":62,"marker":"bibr","target":"#b29"}]},{"text":"This technique is based on First-Order Logic. It is motivated by the assumption that making explicit the typed syntactical dependencies in the text one may obtain significant features on which basing the predictions. Thus, this approach translates the complex data represented by natural language text to complex (relational) patterns that allow to model the writing style of an author.","refs":[]},{"text":"Our approach consists in translating the sentences into relational descriptions, then clustering these descriptions (using an automatically computed threshold to stop the clustering procedure). The resulting clusters represent our model of an author. So, after building the models of the base (known) author and the target (unknown) one, the comparison of these models suggests a classification (i.e., whether the target author is the same as the base one or not). The underlying idea is that the model describes a set of ways in which an author composes the sentences in its writings. If we can bring back such writing habits from the target model to the base model, we can conclude that the author is the same.","refs":[]}]},{"title":"Exploitation Tool","paragraphs":[{"text":"The above functionalities are delivered to the users through a graphical tool that provides a set of controls allowing to explore and analyze the conceptual graph. The learned net is represented through an XML file. The tool can load a file in this format, and draw the corresponding net (automatically organizing the nodes in the best possible way). The semantic network can be built incrementally, avoiding too long times of unavailability. Different colors are used for nodes depending on their type: subjects and complements have a different color than verbs. Also the relations are filled with a different color depending on the positive or negative valence of the corresponding phrase. Figure 1 shows two screenshots of the main interface of the tool, showing two different perspectives on the same net (a complete overview and a selection thereof, respectively). The main area, on the left, contains the net.","refs":[{"start":699,"end":700,"marker":"figure","target":"#fig_0"}]},{"text":"After loading a conceptual graph, the tool allows to explore it in a graphical intuitive way, using classical mouse-based controls. Since the compound view of the whole graph is typically cluttered and very dense of (often overlapping) nodes and edges (but still useful to grasp the overall shape of the net), scroll, pan and zoom in/out controls allow to focus on specific parts thereof and to have a better insight on them. Single nodes can be dragged as well, and the entire net is automatically rearranged accordingly to best fit the available space. Finally, by selecting a specific node, it is possible to set a neighborhood limit such that all the nodes whose shortest path to the selected node are outside the selected level are filtered out.","refs":[]},{"text":"All the controls, settings and results are placed in a panel standing on the right of the graph visualization window. Such a panel is in turn divided into several sub-parts (shown in Figures 2, 3 and4). Let us examine the single sub-areas of the control panel in more details.","refs":[{"start":191,"end":195,"marker":"figure","target":null},{"start":199,"end":200,"marker":"figure","target":null}]},{"text":"Distance hops is in the top part of the panel (shown in Figure 3) and containing a text field in which the user can enter the desired level up to which nodes are to be shown, starting from the selected one (i.e. the user can set the maximum neighborhood level). Relevance filtering contains two sliding bars: the former is Distance, it is aimed at providing the same functionality as the Distance hops area, but bound in [0, maxHops(net)], where maxHops(•) is a function that returns the diameter of a given net; the latter is Relevance, it allows to set the relevance parameter that determines which relevant nodes are to be highlighted. Highlight nodes is a radio button, it allows to select a visualization that highlights relevant nodes or a classical one. Choosing the relevant nodes perspective enables access to the advanced functionality of relevant node recognition, useful for a deeper analysis of the collection. Moreover, using the Hibernation check box the study of the net is made more comfortable for the human reader. This issue may require further clarification. In standard mode, the research for a balanced placement of nodes within the used space is always 'alive', so that the nodes automatically rearrange their position in the screen after the perturbations introduced by the user when he moves some elements to study them more comfortably. Since the continuous movement of the nodes makes the visual analysis of the net difficult, the possibility to stop the net in order to explore it (through reading and manual rearrangements of single nodes) was introduced. Network embeds four options, and specifically: Show taxonomy, that adds taxonomic relations to the network; Show object-verb, that adds verbs as nodes, and edges < subject, verb > and < verb, complement >; Show object-complement, that adds direct relations < subject, complement > (regardless of the verbs connecting them); Show tagged object-complement, that enables the tagging of the relations < subject, complement > with verbs and associated (positive or negative) valence as reported in the XML file (so, the visual outcome is the same as for Show object-complement, but the tagged relations in the XML can be used for further functionalities). Reasoning is devoted to the reasoning operators (shown in Figure 2). In particular, it contains two text fields in each of which a concept (label of a node) can be entered, so that pressing the Search path button starts the Reasoning by Association functionality in order to obtain a plausible complex relation between the specified concepts. This sub-area also contains a button (named Search generalizations) that starts the search for Generalization; its behavior can be modified by acting on two checkboxes, Get clusters from XML (that avoids computing the clusters if they have already been computed and stored in suitable XML files), and Run anti-clustering (that starts the technique to build bridges between different components of the net [16]). Results appears in the bottom area in the panel (shown in Figure 4). It is dedicated to textual results, consisting of paths where each row reports in square brackets (the labels of) the relations that exist between two nodes. In particular, each relation (verb) is associated to the number of positive and negative instances in which it occurred, expressing its valence. This also provides an indication of the degree of reliability of the path sub-parts. As an example, the screenshot in Figure 4 shows the resulting path between nodes 'symbol' and 'literature': sentence(symbol, humanity, [relate P: 1/1 (100.0%), N: 0/1 (0.0%)]) sentence(god, humanity, [reveal to P: 1/1 (100.0%), N: 0/1 (0.0%)]) sentence(teaching, god, [lead to P: 2/2 (100.0%), N: 0/2 (0.0%)]) sentence(teaching, literature, [include including P: 4/4 (100.0%), N: 0/4 (0.0%)]) which can be interpreted as: \"Humanity can relate by means of symbols. God reveals to humanity. Teaching (or education) leads to God, and includes the use of literature.\". Here only one relation per row is present, and there are no sentences with negative valence.","refs":[{"start":63,"end":64,"marker":"figure","target":null},{"start":2302,"end":2303,"marker":"figure","target":null},{"start":2985,"end":2989,"marker":"bibr","target":"#b15"},{"start":3057,"end":3058,"marker":"figure","target":null},{"start":3489,"end":3490,"marker":"figure","target":null}]},{"text":"Finally, an additional functionality concerns the possibility of querying the ProLog knowledge base expressing the content of the net, which allows more complex kinds of reasoning than simple reasoning by association on the graph. It can be accessed from menu Tools in the main window, using the PROLOG user interface item. A window like that in Figure 5 is opened, that allows to enter a ProLog query to be answered using the knowledge base (e.g., \"what does a dog eat?\" might be asked in the form eat(dog,X) ). The ProLog representation of the net can be obtained and saved from the same window, by choosing the Create new K.B. item in the Options menu.","refs":[{"start":353,"end":354,"marker":"figure","target":null}]}]},{"title":"Conclusions","paragraphs":[{"text":"Studying, understanding and exploiting the content of a digital library, and extracting useful information thereof, are complex and knowledge-intensive activities for which the user needs the support of effective automatic techniques. To this aim, a relevant role can be played by concept taxonomies. Unfortunately, the availability of such a kind of resources is limited, and their manual building and maintenance are costly and error-prone. ConNeKTion is a tool that allows to learn conceptual graphs from plain text and to enrich them by finding concept generalizations. The resulting graph can be used for several purposes: finding relationships between concepts (if any), filtering the concepts from a particular perspective, keyword extraction, information retrieval and author identification. A suitable control panel is provided for the user to comfortably carry out these activities.","refs":[]},{"text":"As future work, we plan to improve the natural language text pre-processing using anaphora resolution in order to replace, where possible, pronouns with the explicit concept they express. We also wish to extend the reasoning operators by adding an argumentation operator, that could exploit probabilistic weights, intended as a rate of reliability, to provide support or attack to a given statement.","refs":[]}]}],"tables":{},"abstract":{"title":"Abstract","paragraphs":[{"text":"Studying, understanding and exploiting the content of a digital library, and extracting useful information thereof, require automatic techniques that can effectively support the users. To this aim, a relevant role can be played by concept taxonomies. Unfortunately, the availability of such a kind of resources is limited, and their manual building and maintenance are costly and error-prone. This work presents ConNeK-Tion, a tool for conceptual graph learning and exploitation. It allows to learn conceptual graphs from plain text and to enrich them by finding concept generalizations. The resulting graph can be used for several purposes: finding relationships between concepts (if any), filtering the concepts from a particular perspective, extracting keyword, retrieving information and identifying the author. ConNeKTion provides also a suitable control panel, to comfortably carry out these activities.","refs":[]}]}}