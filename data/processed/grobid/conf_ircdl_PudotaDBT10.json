{"bibliography":{"title":"A New Domain Independent Keyphrase Extraction System","authors":[{"person_name":{"surname":"Pudota","first_name":"Nirmala"},"affiliations":[{"department":"Department of Mathematics and Computer Science","institution":"University of Udine","laboratory":"Artificial Intelligence Lab"}],"email":"nirmala.pudota@uniud.it"},{"person_name":{"surname":"Dattolo","first_name":"Antonina"},"affiliations":[{"department":"Department of Mathematics and Computer Science","institution":"University of Udine","laboratory":"Artificial Intelligence Lab"}],"email":"antonina.dattolo@uniud.it"},{"person_name":{"surname":"Baruzzo","first_name":"Andrea"},"affiliations":[{"department":"Department of Mathematics and Computer Science","institution":"University of Udine","laboratory":"Artificial Intelligence Lab"}],"email":"andrea.baruzzo@uniud.it"},{"person_name":{"surname":"Tasso","first_name":"Carlo"},"affiliations":[{"department":"Department of Mathematics and Computer Science","institution":"University of Udine","laboratory":"Artificial Intelligence Lab"}],"email":"carlo.tasso@uniud.it"}],"date":null,"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"keywords":[],"citations":{"b0":{"title":"Using noun phrase heads to extract document keyphrases","authors":[{"person_name":{"surname":"Barker","first_name":"K"},"affiliations":[],"email":null},{"person_name":{"surname":"Cornacchia","first_name":"N"},"affiliations":[],"email":null}],"date":{"year":"2000","month":null,"day":null},"ids":null,"target":null,"publisher":"Springer","journal":null,"series":null,"scope":{"volume":1822,"pages":{"from_page":40,"to_page":52}}},"b1":{"title":"A general framework for personalized text classification and annotation","authors":[{"person_name":{"surname":"Baruzzo","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Dattolo","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Pudota","first_name":"N"},"affiliations":[],"email":null},{"person_name":{"surname":"Tasso","first_name":"C"},"affiliations":[],"email":null}],"date":{"year":"2009","month":null,"day":null},"ids":null,"target":null,"publisher":"Springer","journal":null,"series":null,"scope":{"volume":5535,"pages":{"from_page":31,"to_page":39}}},"b2":{"title":"Recommending new tags using domain-ontologies","authors":[{"person_name":{"surname":"Baruzzo","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Dattolo","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Pudota","first_name":"N"},"affiliations":[],"email":null},{"person_name":{"surname":"Tasso","first_name":"C"},"affiliations":[],"email":null}],"date":{"year":"2009","month":null,"day":null},"ids":null,"target":null,"publisher":"IEEE","journal":null,"series":null,"scope":{"volume":3,"pages":{"from_page":409,"to_page":412}}},"b3":{"title":"Ocelot: a system for summarizing web pages","authors":[{"person_name":{"surname":"Berger","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Mittal","first_name":"V"},"affiliations":[],"email":null}],"date":{"year":"2000","month":null,"day":null},"ids":null,"target":null,"publisher":"ACM","journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":144,"to_page":151}}},"b4":{"title":"Multilingual single document keyword extraction for information retrieval","authors":[{"person_name":{"surname":"Bracewell","first_name":"D"},"affiliations":[],"email":null},{"person_name":{"surname":"Ren","first_name":"F"},"affiliations":[],"email":null},{"person_name":{"surname":"Kuroiwa","first_name":"S"},"affiliations":[],"email":null}],"date":{"year":"2005","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":517,"to_page":522}}},"b5":{"title":"The anatomy of a large-scale hypertextual web search engine","authors":[{"person_name":{"surname":"Brin","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Page","first_name":"L"},"affiliations":[],"email":null}],"date":{"year":"1998","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"Computer Networks","series":null,"scope":{"volume":30,"pages":{"from_page":107,"to_page":117}}},"b6":{"title":"Supporting personalized user concept spaces and recommendations for a publication sharing system","authors":[{"person_name":{"surname":"Dattolo","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Ferrara","first_name":"F"},"affiliations":[],"email":null},{"person_name":{"surname":"Tasso","first_name":"C"},"affiliations":[],"email":null}],"date":{"year":"2009","month":null,"day":null},"ids":null,"target":null,"publisher":"Springer","journal":null,"series":null,"scope":{"volume":5535,"pages":{"from_page":325,"to_page":330}}},"b7":{"title":"Keyphrase extraction for summarization purposes: the lake system at duc2004","authors":[{"person_name":{"surname":"D'avanzo","first_name":"E"},"affiliations":[],"email":null},{"person_name":{"surname":"Magnini","first_name":"B"},"affiliations":[],"email":null},{"person_name":{"surname":"Vallin","first_name":"A"},"affiliations":[],"email":null}],"date":{"year":"2004","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b8":{"title":"Domain-specific keyphrase extraction","authors":[{"person_name":{"surname":"Frank","first_name":"E"},"affiliations":[],"email":null},{"person_name":{"surname":"Paynter","first_name":"G"},"affiliations":[],"email":null},{"person_name":{"surname":"Witten","first_name":"I"},"affiliations":[],"email":null},{"person_name":{"surname":"Gutwin","first_name":"C"},"affiliations":[],"email":null},{"person_name":{"surname":"Nevill-Manning","first_name":"C"},"affiliations":[],"email":null}],"date":null,"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":668,"to_page":673}}},"b9":{"title":"Corephrase: Keyphrase extraction for document clustering","authors":[{"person_name":{"surname":"Hammouda","first_name":"K"},"affiliations":[],"email":null},{"person_name":{"surname":"Matute","first_name":"D"},"affiliations":[],"email":null},{"person_name":{"surname":"Kamel","first_name":"M"},"affiliations":[],"email":null}],"date":{"year":"2005","month":null,"day":null},"ids":null,"target":null,"publisher":"Springer","journal":null,"series":null,"scope":{"volume":3587,"pages":{"from_page":265,"to_page":274}}},"b10":{"title":"Improved automatic keyword extraction given more linguistic knowledge","authors":[{"person_name":{"surname":"Hulth","first_name":"A"},"affiliations":[],"email":null}],"date":{"year":"2003","month":null,"day":null},"ids":null,"target":null,"publisher":"Association for Computational Linguistics","journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":216,"to_page":223}}},"b11":{"title":"Technical terminology: some linguistic properties and an algorithm for identification in text","authors":[{"person_name":{"surname":"Justeson","first_name":"J"},"affiliations":[],"email":null},{"person_name":{"surname":"Katz","first_name":"S"},"affiliations":[],"email":null}],"date":{"year":"1995","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"Natural Language Engineering","series":null,"scope":{"volume":1,"pages":{"from_page":9,"to_page":27}}},"b12":{"title":"Use of keyphrase extraction software for creation of an AEC/FM thesaurus","authors":[{"person_name":{"surname":"Kosovac","first_name":"B"},"affiliations":[],"email":null},{"person_name":{"surname":"Vanier","first_name":"D"},"affiliations":[],"email":null},{"person_name":{"surname":"Froese","first_name":"T"},"affiliations":[],"email":null}],"date":{"year":"2000","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"Electronic Journal of Information Technology in Construction","series":null,"scope":{"volume":5,"pages":{"from_page":25,"to_page":36}}},"b13":{"title":"Learning user information interests through the extraction of semantically significant phrases","authors":[{"person_name":{"surname":"Krulwich","first_name":"B"},"affiliations":[],"email":null},{"person_name":{"surname":"Burkey","first_name":"C"},"affiliations":[],"email":null}],"date":{"year":"1996","month":null,"day":null},"ids":null,"target":null,"publisher":"AAAI Press","journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":110,"to_page":112}}},"b14":{"title":"Automatic keyphrase extraction from scientific documents using n-gram filtration technique","authors":[{"person_name":{"surname":"Kumar","first_name":"N"},"affiliations":[],"email":null},{"person_name":{"surname":"Srinathan","first_name":"K"},"affiliations":[],"email":null}],"date":{"year":"2008","month":null,"day":null},"ids":null,"target":null,"publisher":"ACM","journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":199,"to_page":208}}},"b15":{"title":"Graph-based keyword extraction for single-document summarization","authors":[{"person_name":{"surname":"Litvak","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Last","first_name":"M"},"affiliations":[],"email":null}],"date":{"year":"2008","month":null,"day":null},"ids":null,"target":null,"publisher":"ACL","journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":17,"to_page":24}}},"b16":{"title":"Clustering to find exemplar terms for keyphrase extraction","authors":[{"person_name":{"surname":"Liu","first_name":"Z"},"affiliations":[],"email":null},{"person_name":{"surname":"Li","first_name":"P"},"affiliations":[],"email":null},{"person_name":{"surname":"Zheng","first_name":"Y"},"affiliations":[],"email":null},{"person_name":{"surname":"Sun","first_name":"M"},"affiliations":[],"email":null}],"date":{"year":"2009","month":null,"day":null},"ids":null,"target":null,"publisher":"ACL","journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":257,"to_page":266}}},"b17":{"title":"Human-competitive tagging using automatic keyphrase extraction","authors":[{"person_name":{"surname":"Medelyan","first_name":"O"},"affiliations":[],"email":null},{"person_name":{"surname":"Frank","first_name":"E"},"affiliations":[],"email":null},{"person_name":{"surname":"Witten","first_name":"I"},"affiliations":[],"email":null}],"date":{"year":"2009","month":null,"day":null},"ids":null,"target":null,"publisher":"ACL","journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":1318,"to_page":1327}}},"b18":{"title":"Keyphrase extraction in scientific publications","authors":[{"person_name":{"surname":"Nguyen","first_name":"T"},"affiliations":[],"email":null},{"person_name":{"surname":"Kan","first_name":"M"},"affiliations":[],"email":null}],"date":{"year":"2007","month":null,"day":null},"ids":null,"target":null,"publisher":"Springer","journal":null,"series":null,"scope":{"volume":4822,"pages":{"from_page":317,"to_page":326}}},"b19":{"title":"An algorithm for suffix stripping","authors":[{"person_name":{"surname":"Porter","first_name":"M"},"affiliations":[],"email":null}],"date":{"year":"1997","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"Readings in information retrieval","series":null,"scope":{"volume":null,"pages":{"from_page":313,"to_page":316}}},"b20":{"title":"Keyphrase extraction-based query expansion in digital libraries","authors":[{"person_name":{"surname":"Song","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Song","first_name":"I"},"affiliations":[],"email":null},{"person_name":{"surname":"Allen","first_name":"R"},"affiliations":[],"email":null},{"person_name":{"surname":"Obradovic","first_name":"Z"},"affiliations":[],"email":null}],"date":{"year":"2006","month":null,"day":null},"ids":null,"target":null,"publisher":"ACM","journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":202,"to_page":209}}},"b21":{"title":"Learning algorithms for keyphrase extraction","authors":[{"person_name":{"surname":"Turney","first_name":"P"},"affiliations":[],"email":null}],"date":{"year":"2000","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"Information Retrieval","series":null,"scope":{"volume":2,"pages":{"from_page":303,"to_page":336}}},"b22":{"title":"Single document keyphrase extraction using neighborhood knowledge","authors":[{"person_name":{"surname":"Wan","first_name":"X"},"affiliations":[],"email":null},{"person_name":{"surname":"Xiao","first_name":"J"},"affiliations":[],"email":null}],"date":{"year":"2008","month":null,"day":null},"ids":null,"target":null,"publisher":"AAAI Press","journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":855,"to_page":860}}},"b23":{"title":"Kea: practical automatic keyphrase extraction","authors":[{"person_name":{"surname":"Witten","first_name":"I"},"affiliations":[],"email":null},{"person_name":{"surname":"Paynter","first_name":"G"},"affiliations":[],"email":null},{"person_name":{"surname":"Frank","first_name":"E"},"affiliations":[],"email":null},{"person_name":{"surname":"Gutwin","first_name":"C"},"affiliations":[],"email":null},{"person_name":{"surname":"Nevill-Manning","first_name":"C"},"affiliations":[],"email":null}],"date":{"year":"1999","month":null,"day":null},"ids":null,"target":null,"publisher":"ACM","journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":254,"to_page":255}}},"b24":{"title":"Document keyphrases as subject metadata: incorporating document key concepts in search results","authors":[{"person_name":{"surname":"Wu","first_name":"Y"},"affiliations":[],"email":null},{"person_name":{"surname":"Li","first_name":"Q"},"affiliations":[],"email":null}],"date":{"year":"2008","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"Information Retrieval","series":null,"scope":{"volume":11,"pages":{"from_page":229,"to_page":249}}}},"sections":[{"title":"Introduction","paragraphs":[{"text":"A keyphrase is a short phrase (typically it contains one to three words) that provides a key idea of a document. A keyphrase list is a short list of keyphrases (typically five to fifteen phrases) that reflects the content of a single document, capturing the main topics discussed and providing a brief summary of its content. If every document is attached with keyphrases, a user can choose easily which documents to read and/or understand the relationships among documents. Document keyphrases are used successfully in Information Retrieval (IR) and Natural Language Processing (NLP) tasks, such as document indexing [9], clustering [10], classification [14], and summarization [4]. Among all of them, document indexing is one important application of automatic keyphrase generation in digital libraries, where the major part of publications usually are not associated with keyphrases. Furthermore, keyphrases are well exploited for other tasks such as thesaurus creation [13], subject metadata enrichment [25], query expansion [21], and automatic tagging [18].","refs":[{"start":618,"end":621,"marker":"bibr","target":"#b8"},{"start":634,"end":638,"marker":"bibr","target":"#b9"},{"start":655,"end":659,"marker":"bibr","target":"#b13"},{"start":679,"end":682,"marker":"bibr","target":"#b3"},{"start":973,"end":977,"marker":"bibr","target":"#b12"},{"start":1007,"end":1011,"marker":"bibr","target":"#b24"},{"start":1029,"end":1033,"marker":"bibr","target":"#b20"},{"start":1057,"end":1061,"marker":"bibr","target":"#b17"}]},{"text":"Despite of having many applications, only a small percent of documents have keyphrases assigned to them. For instance, in digital libraries, authors assign keyphrases to their documents when they are instructed to do so [9], other digital content, like news or magazine articles, usually do not have keyphrases since it is neither mandatory nor necessary for the document authors to provide keyphrases. Manually assigning keyphrases to documents is tedious, timeconsuming, and as well as expensive. Therefore, automatic methods that generate keyphrases for a given document are beneficial.","refs":[{"start":220,"end":223,"marker":"bibr","target":"#b8"}]},{"text":"Witten et al. [24] defined two fundamental approaches for automatic keyphrase generation:","refs":[{"start":14,"end":18,"marker":"bibr","target":"#b23"}]},{"text":"1. Keyphrase assignment : in this case, the set of possible keyphrases is limited to a predefined vocabulary of terms (e.g., subject headings, classification schemes, thesaurus). The task is to classify documents based on the content into different keyphrase classes that correspond to the terms of a pre-defined list. In this process, the document can be associated with keyphrases constituted by words (or n-grams) that are not contained in the document. 2. Keyphrase extraction: in contrast to the previous case, keyphrase extraction selects the most indicative phrases present in the input document. In this process, selection of keyphrases does not depend on any vocabulary and such phrases are supposed to be available in the document itself.","refs":[]},{"text":"In this paper, we concentrate on the keyphrase extraction problem leaving the more general task of keyphrase assignment. The work presented here is part of a wide research project PIRATES (Personalized Intelligent tag Recommendation and Annotation TEStbed) [2,3,7], a framework for personalized content retrieval, annotation, and classification. Using an integrated set of tools, PIRATES framework lets the users experiment, customize, and personalize the way they retrieve, filter, and organize the large amount of information available on the Web. Furthermore, the framework undertakes a novel approach that automates typical manual tasks such as content annotation and tagging, by means of personalized tag recommendations and other forms of textual annotations (e.g., keyphrases). The rest of this paper is organized as follows: Section 2 introduces the related work. The proposed domain independent keyphrase extraction system is described in detail in Section 3. Empirical evaluation is presented in Section 4 and finally we conclude the paper in Section 5.","refs":[{"start":257,"end":260,"marker":"bibr","target":"#b1"},{"start":260,"end":262,"marker":"bibr","target":"#b2"},{"start":262,"end":264,"marker":"bibr","target":"#b6"}]}]},{"title":"Related Work","paragraphs":[{"text":"Keyphrase extraction methods usually work in two stages: (i) a candidate identification stage, identifies all possible phrases from the document and (ii) a selection stage, selects only few candidate phrases as keyphrases. Existing methods for keyphrase extraction can be divided into supervised and unsupervised approaches, illustrated in the following:","refs":[]},{"text":"A. The supervised approach treats the problem as a classification task. In this approach, a model is constructed by using training documents, that have already keyphrases assigned (by humans) to them. This model is applied in order to select keyphrases from previously unseen documents. All the above systems need a training data in small or large extent in order to construct an extraction system. However, acquiring training data with known keyphrases is not always feasible and human assignment is timeconsuming. Furthermore, a model that is trained on a specific domain, does not always yield to better classification results in other domains. B. The unsupervised approach2 eliminates the need of training data. It selects a general set of candidate phrases from the given document, and it uses some ranking strategy to select the most important candidates as keyphrases for the document. Barker and Cornacchia [1] extract noun phrases from a document and ranks them by using simple heuristics based on their length, frequency, and the frequency of their head noun.","refs":[{"start":676,"end":677,"marker":null,"target":"#foot_1"},{"start":915,"end":918,"marker":"bibr","target":"#b0"}]},{"text":"In [5], Bracewell et al. extract noun phrases from a document, and then cluster the terms which share the same noun term. The clusters are ranked based on term and noun phrase frequencies. Finally, top-n ranked clusters are selected as keyphrases for the document.","refs":[{"start":3,"end":6,"marker":"bibr","target":"#b4"}]},{"text":"In [17], Liu et al. propose another unsupervised method, that extracts keyphrases by using clustering techniques which assure that the document is semantically covered by these terms. Another unsupervised method that utilizes document cluster information to extract keyphrases from a single document is presented in [23].","refs":[{"start":3,"end":7,"marker":"bibr","target":"#b16"},{"start":316,"end":320,"marker":"bibr","target":"#b22"}]},{"text":"Employing graph-based ranking methods for keyphrase extraction is another widely used unsupervised approach, exploited for example in [16]. In such methods, a document is represented as a term graph based on term relatedness, and then a graph-based ranking model algorithm (similar to the PageRank algorithm [6]) is applied to assign scores to each term. Term relatedness is approximated in between terms that co-occur each other within a pre-defined window size.","refs":[{"start":134,"end":138,"marker":"bibr","target":"#b15"},{"start":308,"end":311,"marker":"bibr","target":"#b5"}]},{"text":"Keyphrase extraction systems that are developed by following unsupervised approach are in general domain independent since they are not constrained by any specific training documents.","refs":[]}]},{"title":"Domain Independent Keyphrase Extraction (DIKpE) System Description","paragraphs":[{"text":"Domain independent keyphrase extraction approach, which doesn't enforce any training data has many applications. For instance, it can be useful for a user who wants to know quickly the content of a new Web page, or who wants to know the main claim of a paper at hand. In such cases, keyphrase extraction approach that can be applied without a corpus3 of the same kind of documents is very useful. Simple term frequency is sometimes sufficient to know the document overview; however, more powerful techniques are desirable. Our approach is applied to any document without the need of a corpus. It is solely based on a single document. In the following, we provide a detailed description of our approach. The general workflow in DIKpE system is shown in Fig. 1 and is illustrated in detail in the following subsections 3.1, 3.2, and 3.3. We follow three main steps: (i) extract candidate phrases from the document (ii) calculate feature values for candidates (iii) compute a score for each candidate phrase from its feature values and rank the candidate phrases based on their respective scores, in such a way, highest ranked phrases being assigned as keyphrases.","refs":[{"start":349,"end":350,"marker":null,"target":"#foot_2"},{"start":757,"end":758,"marker":"figure","target":"#fig_0"}]}]},{"title":"Step1: Candidate Phrase Extraction","paragraphs":[{"text":"This step is divided in the following substeps:","refs":[]},{"text":"-Format conversion. We assume that the input document can be in any format (e.g., pdf ), and as our approach only deals with textual input, our system first exploits document converters to extract the text from the given input document. -Cleaning and Sentence delimiting. The plain text form is then processed to delimit sentences, following the assumption that no keyphrase parts are located simultaneously in two sentences. Separating sentences by inserting ). The assigned pos tags are later utilized for filtering candidate phrases and in calculating pos value feature. The next step in our procedure is to extract n-grams. We have observed that in the dataset utilized for the experimentation, phrases that are constituted by more than 3 words are rarely assigned as keyphrases, so, in our process, we set the value of 'n' to the maximum value 3. We extract all possible subsequences of phrases up to 3 words (uni-grams, bi-grams, and tri-grams). -Stemming and Stopword removing. From the extracted n-grams, we remove all phrases 7 that start and/or end with a stopword and phrases containing the sentence delimiter. Partial stemming (i.e., unifying the plural forms and singular forms which mean essentially the same thing) is performed using the first step of Porter stemmer algorithm [20]. To reduce the size of the candidate phrase set, we have filtered out some candidate phrases by using their pos tagging information. Uni-grams that are not labeled as noun, adjective, and verb are filtered out. For bi-grams and tri-grams, only pos-patterns defined by Justeson and Katz [12] and other patterns that include adjective and verb forms are considered. -Separating n-gram lists. Generally, in a document, uni-grams are more frequent than bi-grams, and bi-grams are more frequent than tri-grams and so on. In the calculation of phrase frequency (explained in Section 3.2) feature, this shows a bias towards n-grams which are having small value of 'n'.","refs":[{"start":1292,"end":1296,"marker":"bibr","target":"#b19"},{"start":1583,"end":1587,"marker":"bibr","target":"#b11"}]},{"text":"In order to solve this problem, we have separated n-grams of different lengths (n=1, n=2, and n=3) and arranged them in three different lists. These lists are treated separately in calculation of feature sets and in final keyphrase selection. As a result of step 1, we obtain a separate list of uni-gram, bi-gram, and tri-gram candidate phrases (with corresponding pos tags) per document after the proper stemming and stopword removal explained above.","refs":[]}]},{"title":"Step2: Feature Calculation","paragraphs":[{"text":"The candidate phrase extraction step is followed by a feature calculation step that characterizes each candidate phrase by statistical and linguistic properties.","refs":[]},{"text":"Five features for each candidate phrase are computed; these are: phrase frequency, pos value, phrase depth, phrase last occurrence, and phrase lifespan, illustrated in the following.","refs":[]},{"text":"phrase frequency: this feature is same as the classical term frequency (tf) metric. But instead of calculating it with respect to the whole length of the document, we compute it with respect to each n-gram list. With a separate list for each n-gram in hand, the phrase frequency for phrase P in a list L is:","refs":[]}]},{"title":"req(P, L) size(L) ,","paragraphs":[{"text":"where:","refs":[]},{"text":"• f req(P, L) is the number of times P occurs in L;","refs":[]},{"text":"• size(L) is the total number of phrases included in L.","refs":[]},{"text":"pos value: as described in [1], most author-assigned keyphrases for a document turn out to be noun phrases. For this reason, in our approach, we stress the presence of a noun in a candidate phrase while computing a pos value for the phrase. A pos value is assigned to each phrase by calculating the number of nouns (singular or plural) normalizing it by the total number of terms in the phrase. For instance, in a tri-gram phrase, if all tokens are noun forms, then the pos value of the phrase is 1, if two tokens are noun forms, then the pos value is 0.66, and if one noun is present, the value is 0.33. All remaining phrases which do not include at least one noun form are assigned the pos value 0.25. The same strategy is followed for bi-gram and uni-gram phrases. -phrase depth: this feature reflects the belief that important phrases often appear in the initial part of the document especially in news articles and scientific publications (e.g., abstract, introduction). We compute the position in the document where the phrase first appears. The phrase depth value for phrase P in a document D is:","refs":[{"start":27,"end":30,"marker":"bibr","target":"#b0"}]},{"text":"where f irst index(P ) is the number of words preceding the phrase's first appearance; size(D) is the total number of words in D.","refs":[]},{"text":"The result is a number between 0 and 1. Highest values represent the presence of a phrase at the very beginning of the document. For instance, if a phrase appears at 16th position, while the whole document contains 700 words, the phrase depth value is 0.97, indicating the first appearance at the beginning of the document.","refs":[]},{"text":"phrase last occurrence: we give also importance to phrases that appear at the end of the document, since keyphrases may also appear in the last parts of a document, as in the case of scientific articles (i.e., in the conclusion and discussion parts). The last occurrence value of a phrase is calculated as the number of words preceding the last occurrence of the phrase normalized with the total number of words in the document. The last occurrence value for phrase P in a document D is:","refs":[]},{"text":"where last index(P ) is the number of words preceding the phrase's last appearance; size(D) is the total number of words in D.","refs":[]},{"text":"For instance, if a phrase appears for the last time at 500th position last time in a document that contains 700 words, then the phrase last occurrence value is 0.71.","refs":[]},{"text":"phrase lifespan: the span value of a phrase depends on the portion of the text that is covered by the phrase. The covered portion of the text is the distance between the first occurrence position and last occurrence position of the phrase in the document. The lifespan value is computed by calculating the difference between the phrase last occurrence and the phrase first occurrence. The lifespan value for phrase P in a document D is:","refs":[]},{"text":"where last index(P ) is the number of words preceding the phrase's last appearance; f irst index(P ) is the number of words preceding the phrase's first appearance; size(D) is the total number of words in D.","refs":[]},{"text":"The result is a number between 0 and 1. Highest values mean that the phrase is introduced at the beginning of the document and carried until the end of the document. Phrases that appear only once through out the document have the lifespan value 0.","refs":[]},{"text":"As a result of step 2, we get a feature vector for each candidate phrase in the three n-gram lists.","refs":[]}]},{"title":"Step3: Scoring and Ranking","paragraphs":[{"text":"In this step a score is assigned to each candidate phrase which is later exploited for the selection of the most appropriate phrases as representatives of the document. The score of each candidate phrase is calculated as a linear combination of the 5 features. We call the resulting score value keyphraseness of the candidate phrase. The keyphraseness of a phrase P with non empty feature set {f 1 ,f 2 ,...f 5 }, with non-negative weights {w 1 ,w 2 ,..w 5 } is:","refs":[]},{"text":"In this initial stage of the research, we assign equal weights to all features, yielding to the computation of the average. Therefore:","refs":[]},{"text":"where:","refs":[]},{"text":"n is the total number of features (i.e., 5 in our case); f 1 is the phrase frequency; f 2 is the phrase depth; f 3 is the phrase pos value; f 4 is the phrase last occurrence; f 5 is the phrase lifespan.","refs":[]},{"text":"Producing Final Keyphrases. The scoring process produces three separate lists L 1 , L 2 , and L 3 containing respectively all the uni-grams, bi-grams and trigrams with their keyphraseness values. We then select some keyphrases, which are considered to be the most important from each list. In order to produce the 'k' final keyphrases, we have followed the same strategy that was utilized in [15]. In every list, the candidate phrases are ranked in descending order based on the keyphraseness values. Top 20% (i.e., 20% of 'k' ) keyphrases are selected from \"L 3 \", Top 40% (i.e., 40% of 'k' ) are selected from \"L 2 \", and remaining 40% of rest of 'k' keyphrases are selected from \"L 1 '. In this way top k keyphrases for the given document are extracted.","refs":[{"start":392,"end":396,"marker":"bibr","target":"#b14"}]}]},{"title":"Evaluation","paragraphs":[{"text":"The effectiveness and efficiency of our system has been tested on a publicly available keyphrase extraction dataset [19] which contains 215 full length documents from different computer science subjects. Each document in the dataset contains a first set of keyphrases assigned by the paper's authors and a second set of keyphrases assigned by volunteers, familiar with computer science papers. DIKpE is evaluated by computing the number of matches between the keyphrases attached to the document and the keyphrases extracted automatically. The same partial stemming strategy exploited in candidate phrase selection (see section 3.1) is used also in matching keyphrases. For instance, given the following keyphrase sets S 1 {component library, facet-based component retrieval, ranking algorithm, component rank, retrieval system} and S 2 {component library system, web search engine, component library, component ranks, retrieval systems, software components} suggested by our system, the number of exact matches is 3: {component library, component rank, retrieval system}.","refs":[{"start":116,"end":120,"marker":"bibr","target":"#b18"}]},{"text":"We have carried out two experiments in order to test our system's performance. For the first experiment, we have considered keyphrase extraction works presented by Nguyen&Kan [19] and KEA [24] as baseline systems. From the available 215 documents, Nguyen&Kan has taken 120 documents to compare these with KEA. The maximum number of keyphrases for each document (i.e., 'k' ) is set to ten in Nguyen&Kan. We have taken their results [19] as reference, and in the first experiment we have worked on 120 documents randomly selected from the 215 documents. In both the experiments, we removed the bibliography section from each document in the dataset in order to better utilize the phrase last occurrence feature.","refs":[{"start":175,"end":179,"marker":"bibr","target":"#b18"},{"start":188,"end":192,"marker":"bibr","target":"#b23"},{"start":431,"end":435,"marker":"bibr","target":"#b18"}]},{"text":"Table-1 shows the average number of exact matches of three algorithms when 10 keyphrases are extracted from each document: our system significantly outperforms the other two. For the second experiment, we have extracted keyphrases for all 215 documents and compared our approach exclusively with the results provided by KEA. We have utilized a total of 70 documents (with keyphrases assigned by authors) extracted from the 215 documents dataset to train the KEA algorithm. For each document, we extracted 7, 15 and 20 top keyphrases using both our approach and KEA.","refs":[]},{"text":"The results are shown in Table-2: it is clear that even though our system does not undertake any training activity, it greatly outperforms KEA performance.","refs":[]},{"text":"A sample output of the DIKpE system for three sample documents is shown Table - ","refs":[]}]},{"title":"Conclusion and Future Work","paragraphs":[{"text":"In this paper, we have presented an innovative and hybrid approach to keyphrase extraction that works on a single document without any previous parameter tuning. Our current work focuses on the integration of DIKpE system with other tools presented in the PIRATES framework, in order to exploit keyphrase extraction method for the automatic tagging task. Further work will focus on the evaluation procedure. We assumed here that a keyphrase extraction system is optimal, if it provides the same keyphrases that an author defines for his documents. However, in general there may exist many other keyphrases (different than those pre-assigned by authors) that are also appropriate for summarizing a given document. Thus, a further aspect to consider is to take into account the human subjectivity in assigning keyphrases, considering also adaptive personalization techniques for tuning the extraction process to the specific user's interests. In this paper, we evaluated DIKpE performance on scientific publications which are well structured and lengthy in general; in future, we are planning to test the effectiveness of the system on short documents such as news or blog entries. Finally, for the future work, we plan to investigate different ways to compute the coefficients of linear combination of features. We also need to concentrate on a better way to decide the number of keyphrases to be extracted by the system, instead of using a fixed number.","refs":[]}]}],"tables":{"tab_2":{"heading":"Table 1 .","description":"3. For each document top seven keyphrases extracted by DIKpE are Overall Performances","rows":[["System","Average # of exact matches"],["KEA","3.03"],["Nguyen&Kan","3.25"],["DIKpE","4.75"]]},"tab_3":{"heading":"Table 2 .","description":"Performance of DIKpE compared to KEA","rows":[["Keyphrases Average number of exact matches"],["Extracted","KEA","DIKpE"],["7","2.05","3.52"],["15","2.95","4.93"],["20","3.08","5.02"]]},"tab_4":{"heading":"Table 3 .","description":"Top seven keyphrases extracted by DIKpE system to three sample documents : keyphrases that are assigned by the document authors are shown in normal font, Italics indicates keyphrases that are assigned by volunteers, and boldface in DIKpE's row (third row) shows keyphrases that have been automatically extracted and matched with author or volunteer assigned keyphrases. Even if some keyphrases of DIKpE do not match with any of the keyphrases, they are still correctly related to the main theme of the document.","rows":[["Document #26.","Accelerating","#57. Contour-based","#136.","Measuring","e-"],["","3D","Convolution","Partial","Object","Government","Impact:"],["","using","Graphics","Recognition","using","Existing","practices","and"],["","Hardware.","Symmetry in Image","shortcomings."],["","","","Databases.","","",""],["keyphrases convolution","object","","e-government"],["assigned by","hardware accelera-","image","","law",""],["the","tion","","","","",""],["document","volume visualization contour","","interoperability"],["authors","","","","","",""],["","","","recognition","","architectures"],["","","","symmetry","","measurement"],["","","","","","evaluation",""],["keyphrases 3D convolution","occlusion","","benchmark",""],["assigned by filtering","object recognition measurement"],["volunteers visualization","symmetry","","e-government"],["","volume rendering contour","","public administration"],["","","","estimation","","business process"],["keyphrases high pass filters","partial object recog-","measuring e-government im-"],["","","","nition","","pact",""],["assigned by volume rendering object recognition business process"],["DIKpE sys-","filter kernels","objects in images","e-governmental services"],["tem","","","","","",""],["","3d convolution","occlusion of objects public administration"],["","convolution","objects","","e-government"],["","visualization","symmetry","","measurement"],["","filtering","contours","","business",""]]}},"abstract":{"title":"Abstract","paragraphs":[{"text":"In this paper we present a keyphrase extraction system that can extract potential phrases from a single document in an unsupervised, domain-independent way. We extract word n-grams from input document. We incorporate linguistic knowledge (i.e., part-of-speech tags), and statistical information (i.e., frequency, position, lifespan) of each n-gram in defining candidate phrases and their respective feature sets. The proposed approach can be applied to any document, however, in order to know the effectiveness of the system for digital libraries, we have carried out the evaluation on a set of scientific documents, and compared our results with current keyphrase extraction systems.","refs":[]}]}}