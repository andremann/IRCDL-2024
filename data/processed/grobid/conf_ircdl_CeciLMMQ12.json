{"bibliography":{"title":"Document Image Understanding through Iterative Transductive Learning","authors":[{"person_name":{"surname":"Ceci","first_name":"Michelangelo"},"affiliations":[{"department":"Dipartimento di Informatica","institution":"Università degli Studi di Bari \"Aldo Moro\"","laboratory":null}],"email":null},{"person_name":{"surname":"Loglisci","first_name":"Corrado"},"affiliations":[{"department":"Dipartimento di Informatica","institution":"Università degli Studi di Bari \"Aldo Moro\"","laboratory":null}],"email":"loglisci@di.uniba.it"},{"person_name":{"surname":"Macchia","first_name":"Lucrezia"},"affiliations":[{"department":"Dipartimento di Informatica","institution":"Università degli Studi di Bari \"Aldo Moro\"","laboratory":null}],"email":"lucrezia.macchia@uniba.it"},{"person_name":{"surname":"Malerba","first_name":"Donato"},"affiliations":[{"department":"Dipartimento di Informatica","institution":"Università degli Studi di Bari \"Aldo Moro\"","laboratory":null}],"email":null},{"person_name":{"surname":"Quercia","first_name":"Luciano"},"affiliations":[{"department":"Dipartimento di Informatica","institution":"Università degli Studi di Bari \"Aldo Moro\"","laboratory":null}],"email":"luciano.quercia@gmail.com"}],"date":null,"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"keywords":[],"citations":{"b0":{"title":"Towards Versatile Document Analysis Systems","authors":[{"person_name":{"surname":"Baird","first_name":"H"},"affiliations":[],"email":null},{"person_name":{"surname":"Casey","first_name":"M"},"affiliations":[],"email":null}],"date":{"year":"2006","month":null,"day":null},"ids":null,"target":null,"publisher":"Springer","journal":null,"series":null,"scope":{"volume":3872,"pages":{"from_page":280,"to_page":290}}},"b1":{"title":"Spatial associative classification: Propositional vs structural approach","authors":[{"person_name":{"surname":"Ceci","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Appice","first_name":"A"},"affiliations":[],"email":null}],"date":{"year":"2006","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"Journal of Intelligent Information Systems","series":null,"scope":{"volume":27,"pages":{"from_page":191,"to_page":213}}},"b2":{"title":"Discovering Emerging Patterns in Spatial Databases: A Multi-relational Approach","authors":[{"person_name":{"surname":"Ceci","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Appice","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Malerba","first_name":"D"},"affiliations":[],"email":null}],"date":{"year":"2007","month":null,"day":null},"ids":null,"target":null,"publisher":"Springer","journal":null,"series":null,"scope":{"volume":4702,"pages":{"from_page":390,"to_page":397}}},"b3":{"title":"Transductive Learning for Spatial Data Classification","authors":[{"person_name":{"surname":"Ceci","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Appice","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Malerba","first_name":"D"},"affiliations":[],"email":null}],"date":{"year":"2010","month":null,"day":null},"ids":null,"target":null,"publisher":"Springer","journal":null,"series":"Advances in Machine Learning I. SCI","scope":{"volume":262,"pages":{"from_page":189,"to_page":207}}},"b4":{"title":"Relational Data Mining and ILP for Document Image Understanding","authors":[{"person_name":{"surname":"Ceci","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Berardi","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Malerba","first_name":"D"},"affiliations":[],"email":null}],"date":{"year":"2007","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"Applied Artificial Intelligence","series":null,"scope":{"volume":21,"pages":{"from_page":317,"to_page":342}}},"b5":{"title":"Transductive Learning of Logical Structures from Document Images","authors":[{"person_name":{"surname":"Ceci","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Loglisci","first_name":"C"},"affiliations":[],"email":null},{"person_name":{"surname":"Malerba","first_name":"D"},"affiliations":[],"email":null}],"date":{"year":"2011","month":null,"day":null},"ids":null,"target":null,"publisher":"Springer","journal":null,"series":null,"scope":{"volume":375,"pages":{"from_page":121,"to_page":142}}},"b6":{"title":"Classifying web documents in a hierarchy of categories: a comprehensive study","authors":[{"person_name":{"surname":"Ceci","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Malerba","first_name":"D"},"affiliations":[],"email":null}],"date":{"year":"2007","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"Journal of Intelligent Information Systems","series":null,"scope":{"volume":28,"pages":{"from_page":37,"to_page":78}}},"b7":{"title":"Efficient mining of emerging patterns: Discovering trends and differences","authors":[{"person_name":{"surname":"Dong","first_name":"G"},"affiliations":[],"email":null},{"person_name":{"surname":"Li","first_name":"J"},"affiliations":[],"email":null}],"date":{"year":"1999","month":null,"day":null},"ids":null,"target":null,"publisher":"ACM Press","journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":43,"to_page":52}}},"b8":{"title":"CAEP: Classification by Aggregating Emerging Patterns","authors":[{"person_name":{"surname":"Dong","first_name":"G"},"affiliations":[],"email":null},{"person_name":{"surname":"Zhang","first_name":"X"},"affiliations":[],"email":null},{"person_name":{"surname":"Wong","first_name":"L"},"affiliations":[],"email":null},{"person_name":{"surname":"Li","first_name":"J"},"affiliations":[],"email":null}],"date":{"year":"1999","month":null,"day":null},"ids":null,"target":null,"publisher":"Springer","journal":null,"series":null,"scope":{"volume":1721,"pages":{"from_page":30,"to_page":42}}},"b9":{"title":"Multistrategy learning for document recognition","authors":[{"person_name":{"surname":"Esposito","first_name":"F"},"affiliations":[],"email":null},{"person_name":{"surname":"Malerba","first_name":"D"},"affiliations":[],"email":null},{"person_name":{"surname":"Semeraro","first_name":"G"},"affiliations":[],"email":null}],"date":{"year":"1994","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"Applied Artificial Intelligence","series":null,"scope":{"volume":8,"pages":{"from_page":33,"to_page":84}}},"b10":{"title":"Multi-relational learning, text mining, and semisupervised learning for functional genomics","authors":[{"person_name":{"surname":"Krogel","first_name":"M.-A"},"affiliations":[],"email":null},{"person_name":{"surname":"Scheffer","first_name":"T"},"affiliations":[],"email":null}],"date":{"year":"2004","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"Mach. Lear","series":null,"scope":{"volume":57,"pages":{"from_page":61,"to_page":81}}},"b11":{"title":"Inducing multi-level association rules from multiple relations","authors":[{"person_name":{"surname":"Lisi","first_name":"F"},"affiliations":[],"email":null},{"person_name":{"surname":"Malerba","first_name":"D"},"affiliations":[],"email":null}],"date":{"year":"2004","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"Machine Learning","series":null,"scope":{"volume":55,"pages":{"from_page":175,"to_page":210}}},"b12":{"title":"A relational approach to probabilistic classification in a transductive setting","authors":[{"person_name":{"surname":"Malerba","first_name":"D"},"affiliations":[],"email":null},{"person_name":{"surname":"Ceci","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Appice","first_name":"A"},"affiliations":[],"email":null}],"date":{"year":"2009","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"Engineering Applications of Artificial Intelligence","series":null,"scope":{"volume":22,"pages":{"from_page":109,"to_page":116}}},"b13":{"title":"Knowledge-based derivation of document logical structure","authors":[{"person_name":{"surname":"Niyogi","first_name":"D"},"affiliations":[],"email":null},{"person_name":{"surname":"Srihari","first_name":"S"},"affiliations":[],"email":null}],"date":{"year":"1995","month":null,"day":null},"ids":null,"target":null,"publisher":"IEEE Computer Society","journal":null,"series":null,"scope":{"volume":1,"pages":{"from_page":472,"to_page":472}}},"b14":{"title":"Learning with labeled and unlabeled data","authors":[{"person_name":{"surname":"Seeger","first_name":"M"},"affiliations":[],"email":null}],"date":{"year":"2001","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b15":{"title":"Exploring constraints to efficiently mine emerging patterns from large high-dimensional datasets","authors":[{"person_name":{"surname":"Zhang","first_name":"X"},"affiliations":[],"email":null},{"person_name":{"surname":"Dong","first_name":"G"},"affiliations":[],"email":null},{"person_name":{"surname":"Ramamohanarao","first_name":"K"},"affiliations":[],"email":null}],"date":{"year":"2000","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":310,"to_page":314}}}},"sections":[{"title":"Introduction","paragraphs":[{"text":"The recognition of semantically relevant components in the layout extracted from a document image is based on domain-specific knowledge, which is represented in very different forms (e.g. formal grammars or production rules). Several prototypical document image understanding systems have been developed by manually encoding the required knowledge (e.g., DeLoS [14]). However, the layout of documents, even for the same publisher, may change considerably. To prevent obsolescence of the developed systems, it is necessary to continuously update the required knowledge, which is unfeasible if based only on manual encoding.","refs":[{"start":361,"end":365,"marker":"bibr","target":"#b13"}]},{"text":"In order to guarantee versatility of Document Image Analysis Systems [1], that is, guarantee competence over a broad and precisely specified class of document images, the application of machine learning methods has been investigated for almost two decades [10] [5]. Operatively, a human operator provides a document image analysis system with images of documents and then detects and labels semantically relevant layout components from which document structures are induced. This supervised learning approach, though providing some flexibility, still does not ensure the key requirement of versatility. Indeed, to acquire the necessary knowledge on a really broad class of documents, supervised learning methods may require a large set of labeled documents. This contrasts with the common situation in which only few labeled training documents are available due to the significant cost of manual annotation. Therefore, it is important to exploit the large amount of information potentially conveyed by unlabeled documents.","refs":[{"start":69,"end":72,"marker":"bibr","target":"#b0"},{"start":256,"end":260,"marker":"bibr","target":"#b9"},{"start":261,"end":264,"marker":"bibr","target":"#b4"}]},{"text":"Two main settings have been proposed in the literature to exploit information contained in both labeled and unlabeled data: the semi-supervised setting and the transductive setting [15]. The former is a type of inductive learning: the learned function is used to make predictions on any possible example. The latter is only interested in making predictions for the given set of unlabeled data. When the set of documents to label is known a priori, the transductive setting is more suitable, since it is an easier problem than (semi-supervised) induction. In this paper, we propose a transductive approach where unlabeled documents are used to reprioritize models learned from labeled documents alone. Indeed, while discriminative learning methods base their decisions on the posterior probability p(y|x), the transductive learning method uses unlabeled documents to improve the estimate of the prior probability p(x), and hence correct the posterior probability p(y|x) by assuming some form of dependence with p(x).","refs":[{"start":181,"end":185,"marker":"bibr","target":"#b14"}]},{"text":"The proposed learning method follows a logic-based approach in which models are represented by a set of rules expressed in relational logic and documents are represented as facts in the same formalism. So, to \"understand\" the layout structure of an unlabeled document, rules are matched against the relational description of the document layout. The relational representation of document layout and rules is motivated by the fact that layout objects can be related by a number of spatial relationships, such as distance, directional or topological relationships. The study of relational learning in a transductive setting has received little attention (see [4], [11], [13]) while the application of transductive relational learning to bootstrap the labeling process of document image collections remains still unexplored. This work extends the research reported in [6], by introducing an iterative bootstrapping framework and by extending empirical evaluation to additional datasets. In the iterative bootstrapping framework, at each iteration, the algorithm expands the training set by including (originally unlabeled) examples for which the classification is considered to be reliable.","refs":[{"start":657,"end":660,"marker":"bibr","target":"#b3"},{"start":662,"end":666,"marker":"bibr","target":"#b10"},{"start":668,"end":672,"marker":"bibr","target":"#b12"},{"start":865,"end":868,"marker":"bibr","target":"#b5"}]},{"text":"The paper is organized as follows. In Section 2, we define the problem to be solved. Sections 3 and 4 are devoted to the presentation of the method. Finally, experimental results are reported in Section 5 and some conclusions are drawn.","refs":[]}]},{"title":"Motivations and Problem Definition","paragraphs":[{"text":"The recognition of semantically relevant layout components in document images is part of a complex transformation process of document images into a structured symbolic form. This transformation is articulated into several steps. Initial processing steps include binarization, skew detection, and noise filtering. Then, the document image is segmented into several layout components, such as text lines, half-tone images, line drawings or graphics (this step is called layout analysis). The interpretation or understanding of document images follows layout analysis. It aims to associate a logical label (e.g. title, abstract of a scientific paper, picture of a newspaper) to semantically relevant layout components, as well as to extract relevant relationships between logical components (e.g., reading order). Document image understanding is typically based on layout information, such as the relative positioning of layout components or the size of layout components, as well as on content information (e.g., textual, graphical). This is the case of the work reported in this paper, where the association of logical labels to layout components is based on both layout information and textual information. However, the novelty here is mainly in the strategy applied to learn a classifier which can be used to recognize semantically relevant components.","refs":[]},{"text":"In this work we investigate this issue and propose a trasductive method for learning classifiers from training data represented in relational formalism. In a formal way, the problem is defined as follows: Given:","refs":[]},{"text":"a database schema SC which consists of a set of h relational tables {T 0 , . . . , T h-1 }, a set PK of primary keys on the tables in SC, and a set FK of foreign key constraints on the tables in SC, -a target relation T ∈ SC (that represents layout components) and a target discrete attribute Y in T , different from the primary key of T , whose domain is the finite set {C In this work, the classification of Y is based on an approach that exploits both the relational data mining setting and the classical Naïve Bayesian framework.","refs":[]},{"text":"More precisely, given an object E ∈ W S to be classified, a classical naïve Bayes classifier assigns E to the class C i that maximizes the posterior probability P (C i |E). By applying the Bayes theorem, P (C i |E) is expressed as follows:","refs":[]},{"text":"In fact, the decision on the class that maximizes the posterior probability can be made only on the basis of the numerator, that is P (C i ) • P (E|C i ), since P (E) is independent of the class C i . The probability P (C i |E) can then be used to identify examples E for which the classification is reliable. This property can be used to iteratively extend the training data by propagating the most reliable decisions when bootstrapping the labeling process.","refs":[]},{"text":"In (1), the main problem is in the computation of P (E|C i ). By following the main intuition in [2], it is possible to consider a set of association rules to define a suitable decomposition of the likelihood P (E|C i ) à la naive Bayes in order to simplify the probability estimation problem. In particular, if (E) ⊆ is the set of first order association rules whose antecedent covers E, P (E|C i ) is:","refs":[{"start":3,"end":6,"marker":"bibr","target":"#b0"},{"start":97,"end":100,"marker":"bibr","target":"#b1"}]},{"text":"(","refs":[]},{"text":"The straightforward application of the naïve Bayes independence assumption to all literals in Rj ∈ (E) antecedent(R j ) is not correct, since it may lead to underestimating P (E|C i ) when several similar clauses in (E) are considered for the class C i . To prevent this problem the authors resort to the logical notion of factorization. Details are reported in [2].","refs":[{"start":362,"end":365,"marker":"bibr","target":"#b1"}]},{"text":"Although this approach would potentially be used in this application, two main limitations could prevent its actual applicability: i) It does not exploit the transductive learning setting. ii) As in most associative classifiers, extracted association rules do not permit to adequately characterize classes.","refs":[]},{"text":"To overcome these limitations, in this paper, we use Emerging Patterns (EPs) instead of association rules in order to discover a characterization of classes and we use this characterization in a transductive classifier. In fact, emerging patterns discovery is a descriptive data mining task which aims at detecting significant differences between objects of distinct classes. EPs are introduced in [8] as a particular kind of patterns (or multi-variate features) whose support significantly changes from one data class to another: the larger the difference of pattern support, the more interesting the pattern. Change in pattern support is estimated in terms of the support ratio (or growth rate). EPs with sharp change in support (high growth rate) can be used to characterize classes.","refs":[{"start":398,"end":401,"marker":"bibr","target":"#b7"}]}]},{"title":"Mining Emerging Patterns with SPADA","paragraphs":[{"text":"Data mining research has provided several solutions (e.g. [8]) for the task of emerging patterns discovery but only one attempt [3] has been done to deal with relational data. In this work, we exploit the system SPADA [12], originally designed for relational frequent patterns discovery, for mining emerging patterns.","refs":[{"start":58,"end":61,"marker":"bibr","target":"#b7"},{"start":128,"end":131,"marker":"bibr","target":"#b2"},{"start":218,"end":222,"marker":"bibr","target":"#b11"}]},{"text":"SPADA represents relational data à la Datalog, a logic programming language with no function symbols specifically designed to implement deductive databases. SPADA distinguishes between the set S of reference (or target) objects, which are the main subject of analysis, and the sets R k , 1 ≤ k ≤ m, of task-relevant (or non-target) objects, which are related to the former and can contribute to account for the variation. From a database viewpoint, S corresponds to the target table T ∈ SC and each R k corresponds to a different relational table T i ∈ SC. A unit of analysis corresponds to a tuple in t ∈ T and to all tuples in the database related to t according to foreign key constraints.","refs":[]},{"text":"In the following sub-sections, the document description and the learning strategy are described, as it has been modified to mine emerging patterns.","refs":[]}]},{"title":"Document Description.","paragraphs":[{"text":"In the logic framework adopted by SPADA, a relational database is boiled down into a deductive database where properties of In this example, b1 and b2 are two constants which denote as many distinct layout components (reference objects), while p1 denotes a document page (taskrelevant object). Predicate block defines a layout component, part of associates a block to a document page, height and width describe geometrical properties of layout components, on top expresses a topological relationship between layout components, page f irst(p1) refers to the position of the page in the document, abstract and title associate b1 and b2 with a logical label, text in abstract and text in title describe the textual content of the logical components. The complete list of predicates is reported in Table 1. The aspatial feature type of specifies the content type of a layout component (e.g. image, text, horizontal line). Logical features are used to associate a logical label to a layout object and depend on the specific domain. In the case of scientific papers (considered in this work), possible logical labels are: affiliation, page number, figure, caption, index term, running head, author, title, abstract, formulae, subsection title, section title, biography, references, paragraph, table. Textual content is represented by means of another class of predicates, which are true when the term reported as second argument occurs in the layout component denoted by the first argument. Terms are automatically extracted by means of a text-processing module [7].","refs":[{"start":800,"end":801,"marker":"table","target":"#tab_0"},{"start":1556,"end":1559,"marker":"bibr","target":"#b6"}]}]},{"title":"The Mining","paragraphs":[{"text":"Step. The original algorithm of SPADA mines frequent patterns at multiple levels l of granularity in order to properly deal with hierarchies H k of objects. When these are available, it is important to take them into account since patterns involving more abstract objects are better supported (although less precise). SPADA operates in two steps for each granularity level: i) pattern generation; ii) pattern evaluation. It takes advantage of statistics computed at granularity level l when computing the supports of patterns at the granularity level l + 1. To discover emerging patterns, SPADA has been modified to mine patterns which characterize classes by detecting significant differences between the objects of these classes. This problem requires the following formulation: Given:","refs":[]},{"text":"a set S of reference objects, -a label value y ∈ Y = {C 1 , C 2 , . . . , C L } associated to each reference object, -some sets R k , 1 ≤ k ≤ m, of task-relevant objects, -a background knowledge BK including hierarchies H k on objects in R k , -M granularity levels in the descriptions, -a set of granularity assignments Ψ k which associate each object in H k with a granularity level, -a couple of sets of thresholds minSup[l], minGR [l] for each granularity level, -a language bias LB that constrains the search space;","refs":[{"start":435,"end":438,"marker":"bibr","target":null}]},{"text":"In this formulation, supp Ci (F ) represents the support of the pattern F in the subset of reference objects labeled with C i while the growth rate GR Ci (F ) is defined as: GR Ci (F ) = suppC i (F ) supp¬C i (F ) where supp ¬Ci (F ) is the support of the pattern F in the subset of reference objects labeled with c ∈ {C 1 , . . . , C i-1 , C i+1 , . . . C L }.","refs":[]},{"text":"To efficiently mine frequent patterns, SPADA prunes the search space by exploiting the monotonicity of the support. Let F be a refinement of a pattern F (i.e. F is more specific that F ). If F is an infrequent pattern for the class C i (i.e. supp Ci (F ) < minSup), then also supp Ci (F ) < minSup. This means that F cannot be an emerging pattern that distinguishes C i from ¬C i . Hence, SPADA does not refine patterns which are infrequent in C i .","refs":[]},{"text":"Unluckily, the monotonicity property does not hold for the growth rate: a refinement of an emerging pattern whose growth rate is lower than the threshold minGR may or may not be an EP. However, also in this case, it is possible to prune the search space. According to [16], we modified the mining algorithm originally developed in SPADA in order to avoid to generate the refinements of a pattern F in the case that GR Ci (F ) = ∞ (i.e., supp Ci (F ) > 0 and supp ¬Ci (F ) = 0). Indeed, due to the monotonicity of support, for each pattern F obtained as refinement of F : supp Ci (F ) ≥ supp Ci (F ) then supp Ci (F ) = 0. Thereby, GR Ci (F ) = 0 in the case that supp Ci (F ) = 0, while GR Ci (F ) = ∞ in the case that supp Ci (F ) > 0. In the former case, F is not worth to be considered. In the latter case, we prefer F to F based on the Occams razor principle, according to which all things being equal, the simplest solution tends to be the best one (F has the same discriminating ability than F ).","refs":[{"start":268,"end":272,"marker":"bibr","target":"#b15"}]},{"text":"In our application domain, reference objects are all the logical components for which a logical label is specified. Task relevant objects are all the logical components (including undefined components) as well as pages and documents. The BK is used to specify the hierarchy of logical components (Figure 1). The BK also allows us to automatically associate information on page order to layout components, since the presence of some logical components may depend on the page order (e.g. author is in the first page). ","refs":[{"start":304,"end":305,"marker":"figure","target":null}]}]},{"title":"Transductive Classification","paragraphs":[{"text":"The transductive classifier implemented in our proposal is described in Algorithm 1, where at each iteration of the cycle at line 3, the algorithm labels objects belonging to the working set W S and uses a subset of them of size |W S|/k as training objects in the subsequent iteration, where k is a user defined parameter1 . The subset is created according to the function score T S∪H (o j , C i ) which represents a membership score of an object o j to the class C i . This score is a growth rate based function which is estimated on the current training set T S ∪ H and is computed by adapting the EP-based classifier CAEP [9] to the relational setting. The largest score determines the object's class.","refs":[{"start":321,"end":322,"marker":null,"target":"#foot_1"},{"start":625,"end":628,"marker":"bibr","target":"#b8"}]},{"text":"In our case, it is computed on the basis of the subset of relational emerging patterns that cover the object to be classified. Formally, let o j be the description of the object to be classified (an object is represented by a tuple in the target table and all the tuples related to it according to foreign key constraints), (o j ) = {F ∈ |∃θ F θ ⊆ o j } is the set of emerging patterns that cover the object o j .","refs":[]},{"text":"The score of o j on the class C i is computed as follows:","refs":[]},{"text":"where GR Ci (F ) and sup Ci (F ) are computed on the current training set T S ∪ H. This measure may result in an inaccurate classifier in the case of unbalanced datasets that is, when training objects are not uniformly distributed over the classes. In order to mitigate this problem the authors in [9] proposed to normalize this score on the basis of the median of the scores obtained from training objects belonging to C i . This results in the following classification function:","refs":[{"start":298,"end":301,"marker":"bibr","target":"#b8"}]},{"text":"where TS ∪ H represents the training set. However, in our case, the main problem comes from the different number of EPs that are extracted from different classes. This means that, in our case a different normalization that weights the number of EPs is necessary:","refs":[]},{"text":"Since sup Ci (F ) represents the probability that a reference object belonging to class C i is covered by F , Equation ( 5) can be transformed as follows:","refs":[{"start":121,"end":122,"marker":"formula","target":"#formula_6"}]},{"text":"By applying the Bayes theorem:","refs":[]},{"text":"where P (C i |F ) can be estimated as the percentage of objects covering F in T S ∪ H that belong to C i . P (C i ) can be estimated as the percentage of objects in T S ∪ H that belong to C i . Finally, P (F ) is the percentage of objects covering F . According to the transductive learning setting, this factor is estimated by considering the whole set of objects (T S ∪ W S). This would provide a more reliable estimation of P (F ) (since obtained from a larger population of objects potentially coming from the same distribution).","refs":[]}]},{"title":"Experiments","paragraphs":[{"text":"The proposed approach has been applied to three different real-world datasets consisting of articles published in two international journals, namely IEEE TPAMI and Behavior Genetics (BG), and in the proceedings of the International Conference on Machine Learning (ICML The iterative transductive classification algorithm is evaluated considering the following experimental setups: 4-fold cross-validation in the case of TPAMI, 6-fold cross-validation in the case of BG and 5-fold cross-validation for ICML. Unlike the standard cross-validation, here one fold at a time is set aside to be used as the training set (and not as the test set ). Small training set sizes allow us to validate the transductive approach, but may result in high error rates.","refs":[]},{"text":"In the step of mining emerging patterns, three experimental schemes of the thresholds minGR, minSup have been set: in the case of TPAMI minGR = {1, 2, 8, 64} and minSup = {30%, 40%, 50%}, in the case of BG minGR = {1, 2, 8, 64} and minSup = {10%, 20%, 30%}, while in the case of ICML minGR = {1, 2, 8, 64} and minSup = {10%, 20%, 30%}. In Table 2 the average number of emerging patterns mined with different parameter values is reported. As expected, by increasing minSup and minGR values, the total number of EPs (sum of the number of EPs in the folds) is reduced. In particular, the number of EPs is more drastically reduced when increasing minSup than when increasing minGR. This means that there are several patterns which characterize a class (a specific layout component) and therefore present a high discriminative power with respect to components belonging to other classes.","refs":[{"start":345,"end":346,"marker":"table","target":null}]},{"text":"Another consideration can be done on the number of EPs mined for each specific class (Table 3). We note that the layout components, for which the descriptions are more heterogeneous or which can be misclassified, are characterized by an higher number of EPs. Indeed, the components which present strong regularities (e.g., described with the same set of features) are those which can be more easily identified and which therefore generate a smaller set of EPs for the classification. Differently, the components which present low regularities can be erroneously labeled and therefore require an higher number of EPs to be discriminated from the others2 . For instance, a figure can be more easily identified than an abstract layout component.","refs":[{"start":92,"end":93,"marker":"table","target":null},{"start":651,"end":652,"marker":null,"target":"#foot_2"}]}]}],"tables":{"tab_0":{"heading":"Table 1 .","description":"The complete list of used predicates both reference objects (which are the main subject of the analysis) and taskrelevant objects (which are relevant for the task at hand, but not necessarely the main subjects of the analysis) are represented in the extensional part D E , while the domain knowledge is expressed as a normal logic program which defines the intensional part D I . As an example, we report a fragment of the extensional part of a deductive database D which describes multimodal information which can be extracted from any document image:","rows":[["","Locational","x pos center/2"],["","features","y pos center/2"],["Layout","Geometrical","height/2"],["structure","features","width/2"],["","Topological","on top/2"],["","features","to right/2"],["","Aspatial feature","type of /2"],["Logical structure","Logical features","application dependent (e.g., abstract/1 )"],["Text","Textual features","application dependent (e.g., text in abstract/2 )"]]}},"abstract":{"title":"Abstract","paragraphs":[{"text":"In Document Image Understanding, one of the fundamental tasks is that of recognizing semantically relevant components in the layout extracted from a document image. This process can be automatized by learning classifiers able to automatically label such components. However, the learning process assumes the availability of a huge set of documents whose layout components have been previously manually labeled. Indeed, this contrasts with the more common situation in which we have only few labeled documents and abundance of unlabeled ones. In addition, labeling layout documents introduces further complexity aspects due to multi-modal nature of the components (textual and spatial information may coexist). In this work, we investigate the application of a relational classifier that works in the transductive setting. The relational setting is justified by the multi-modal nature of the data we are dealing with, while transduction is justified by the possibility of exploiting the large amount of information conveyed in the unlabeled layout components. The classifier bootstraps the labeling process in an iterative way: reliable classifications are used in subsequent iterative steps as training examples. The proposed computational solution has been evaluated on document images of scientific literature.","refs":[]}]}}