{"bibliography":{"title":"Automatic analysis of artistic heritage through Artificial Intelligence","authors":[{"person_name":{"surname":"Castellano","first_name":"Giovanna"},"affiliations":[{"department":"Department of Computer Science","institution":"University of Bari Aldo Moro","laboratory":null}],"email":"giovanna.castellano@uniba.it"},{"person_name":{"surname":"Scaringi","first_name":"Raffaele"},"affiliations":[{"department":"Department of Computer Science","institution":"University of Bari Aldo Moro","laboratory":null}],"email":"raffaele.scaringi@uniba.it"},{"person_name":{"surname":"Vessio","first_name":"Gennaro"},"affiliations":[{"department":"Department of Computer Science","institution":"University of Bari Aldo Moro","laboratory":null}],"email":"gennaro.vessio@uniba.it"}],"date":null,"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"keywords":["Graph representation learning 1. motivations and objectives","Deep learning","Digital humanities","Computer vision"],"citations":{"b0":{"title":"The met dataset: Instance-level recognition for artworks","authors":[{"person_name":{"surname":"Ypsilantis","first_name":"N.-A"},"affiliations":[],"email":null},{"person_name":{"surname":"Garcia","first_name":"N"},"affiliations":[],"email":null},{"person_name":{"surname":"Han","first_name":"G"},"affiliations":[],"email":null},{"person_name":{"surname":"Ibrahimi","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Van Noord","first_name":"N"},"affiliations":[],"email":null},{"person_name":{"surname":"Tolias","first_name":"G"},"affiliations":[],"email":null}],"date":{"year":"2021","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b1":{"title":"DEArt: Dataset of European Art","authors":[{"person_name":{"surname":"Reshetnikov","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Marinescu","first_name":"M.-C"},"affiliations":[],"email":null},{"person_name":{"surname":"Lopez","first_name":"J"},"affiliations":[],"email":null}],"date":{"year":"2022","month":null,"day":null},"ids":{"DOI":null,"arXiv":"arXiv:2211.01226"},"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b2":{"title":"Leveraging Knowledge Graphs and Deep Learning for automatic art analysis","authors":[{"person_name":{"surname":"Castellano","first_name":"G"},"affiliations":[],"email":null},{"person_name":{"surname":"Digeno","first_name":"V"},"affiliations":[],"email":null},{"person_name":{"surname":"Sansaro","first_name":"G"},"affiliations":[],"email":null},{"person_name":{"surname":"Vessio","first_name":"G"},"affiliations":[],"email":null}],"date":{"year":"2022","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"Knowledge-Based Systems","series":null,"scope":{"volume":248,"pages":{"from_page":108859,"to_page":108859}}},"b3":{"title":"Text-Guided Synthesis of Artistic Images with Retrieval-Augmented Diffusion Models","authors":[{"person_name":{"surname":"Rombach","first_name":"R"},"affiliations":[],"email":null},{"person_name":{"surname":"Blattmann","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Ommer","first_name":"B"},"affiliations":[],"email":null}],"date":{"year":"2022","month":null,"day":null},"ids":{"DOI":null,"arXiv":"arXiv:2207.13038"},"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b4":{"title":"Artistic image classification: An analysis on the printart database","authors":[{"person_name":{"surname":"Carneiro","first_name":"G"},"affiliations":[],"email":null},{"person_name":{"surname":"Silva","first_name":"N"},"affiliations":[],"email":null},{"person_name":{"surname":"Bue","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Costeira","first_name":"J"},"affiliations":[],"email":null}],"date":{"year":"2012","month":null,"day":null},"ids":null,"target":null,"publisher":"Springer","journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":143,"to_page":157}}},"b5":{"title":"Deep learning","authors":[{"person_name":{"surname":"Lecun","first_name":"Y"},"affiliations":[],"email":null},{"person_name":{"surname":"Bengio","first_name":"Y"},"affiliations":[],"email":null},{"person_name":{"surname":"Hinton","first_name":"G"},"affiliations":[],"email":null}],"date":{"year":"2015","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"nature","series":null,"scope":{"volume":521,"pages":{"from_page":436,"to_page":444}}},"b6":{"title":"Recognizing Image Style","authors":[{"person_name":{"surname":"Karayev","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Trentacoste","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Han","first_name":"H"},"affiliations":[],"email":null},{"person_name":{"surname":"Agarwala","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Darrell","first_name":"T"},"affiliations":[],"email":null},{"person_name":{"surname":"Hertzmann","first_name":"A"},"affiliations":[],"email":null}],"date":{"year":"2014","month":null,"day":null},"ids":null,"target":null,"publisher":"BMVA Press","journal":null,"series":null,"scope":null},"b7":{"title":"Fine-tuning Convolutional Neural Networks for fine art classification","authors":[{"person_name":{"surname":"Cetinic","first_name":"E"},"affiliations":[],"email":null},{"person_name":{"surname":"Lipic","first_name":"T"},"affiliations":[],"email":null},{"person_name":{"surname":"Grgic","first_name":"S"},"affiliations":[],"email":null}],"date":{"year":"2018","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"Expert Systems with Applications","series":null,"scope":{"volume":114,"pages":{"from_page":107,"to_page":118}}},"b8":{"title":"Representation learning on graphs: Methods and applications","authors":[{"person_name":{"surname":"Hamilton","first_name":"W"},"affiliations":[],"email":null},{"person_name":{"surname":"Ying","first_name":"R"},"affiliations":[],"email":null},{"person_name":{"surname":"Leskovec","first_name":"J"},"affiliations":[],"email":null}],"date":{"year":"2017","month":null,"day":null},"ids":{"DOI":null,"arXiv":"arXiv:1709.05584"},"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b9":{"title":"Understanding of Emotion Perception from Art","authors":[{"person_name":{"surname":"Bose","first_name":"D"},"affiliations":[],"email":null},{"person_name":{"surname":"Somandepalli","first_name":"K"},"affiliations":[],"email":null},{"person_name":{"surname":"Kundu","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Lahiri","first_name":"R"},"affiliations":[],"email":null},{"person_name":{"surname":"Gratch","first_name":"J"},"affiliations":[],"email":null},{"person_name":{"surname":"Narayanan","first_name":"S"},"affiliations":[],"email":null}],"date":{"year":"2021","month":null,"day":null},"ids":{"DOI":null,"arXiv":"arXiv:2110.06486"},"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b10":{"title":"ArtEmis: Affective Language for Visual Art","authors":[{"person_name":{"surname":"Achlioptas","first_name":"P"},"affiliations":[],"email":null},{"person_name":{"surname":"Ovsjanikov","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Haydarov","first_name":"K"},"affiliations":[],"email":null},{"person_name":{"surname":"Elhoseiny","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Guibas","first_name":"L"},"affiliations":[],"email":null}],"date":{"year":"2021","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":11569,"to_page":11579}}},"b11":{"title":"A survey on generative diffusion model","authors":[{"person_name":{"surname":"Cao","first_name":"H"},"affiliations":[],"email":null},{"person_name":{"surname":"Tan","first_name":"C"},"affiliations":[],"email":null},{"person_name":{"surname":"Gao","first_name":"Z"},"affiliations":[],"email":null},{"person_name":{"surname":"Chen","first_name":"G"},"affiliations":[],"email":null},{"person_name":{"surname":"Heng","first_name":"P.-A"},"affiliations":[],"email":null},{"person_name":{"surname":"Li","first_name":"S"},"affiliations":[],"email":null}],"date":{"year":"2022","month":null,"day":null},"ids":{"DOI":null,"arXiv":"arXiv:2209.02646"},"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b12":{"title":"Gen-erative adversarial networks: An overview","authors":[{"person_name":{"surname":"Creswell","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"White","first_name":"T"},"affiliations":[],"email":null},{"person_name":{"surname":"Dumoulin","first_name":"V"},"affiliations":[],"email":null},{"person_name":{"surname":"Arulkumaran","first_name":"K"},"affiliations":[],"email":null},{"person_name":{"surname":"Sengupta","first_name":"B"},"affiliations":[],"email":null},{"person_name":{"surname":"Bharath","first_name":"A"},"affiliations":[],"email":null}],"date":{"year":"2018","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"IEEE signal processing magazine","series":null,"scope":{"volume":35,"pages":{"from_page":53,"to_page":65}}},"b13":{"title":"Towards Generating and Evaluating Iconographic Image Captions of Artworks","authors":[{"person_name":{"surname":"Cetinic","first_name":"E"},"affiliations":[],"email":null}],"date":{"year":"2021","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"Journal of Imaging","series":null,"scope":{"volume":7,"pages":null}},"b14":{"title":"Recognizing the Emotions Evoked by Artworks Through Visual Features and Knowledge Graph-Embeddings","authors":[{"person_name":{"surname":"Aslan","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Castellano","first_name":"G"},"affiliations":[],"email":null},{"person_name":{"surname":"Digeno","first_name":"V"},"affiliations":[],"email":null},{"person_name":{"surname":"Migailo","first_name":"G"},"affiliations":[],"email":null},{"person_name":{"surname":"Scaringi","first_name":"R"},"affiliations":[],"email":null},{"person_name":{"surname":"Vessio","first_name":"G"},"affiliations":[],"email":null}],"date":{"year":"2022","month":null,"day":null},"ids":null,"target":null,"publisher":"Springer","journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":129,"to_page":140}}},"b15":{"title":"Graph attention networks","authors":[{"person_name":{"surname":"Veličković","first_name":"P"},"affiliations":[],"email":null},{"person_name":{"surname":"Cucurull","first_name":"G"},"affiliations":[],"email":null},{"person_name":{"surname":"Casanova","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Romero","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Lio","first_name":"P"},"affiliations":[],"email":null},{"person_name":{"surname":"Bengio","first_name":"Y"},"affiliations":[],"email":null}],"date":{"year":"2017","month":null,"day":null},"ids":{"DOI":null,"arXiv":"arXiv:1710.10903"},"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b16":{"title":"An image is worth 16x16 words: Transformers for image recognition at scale","authors":[{"person_name":{"surname":"Dosovitskiy","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Beyer","first_name":"L"},"affiliations":[],"email":null},{"person_name":{"surname":"Kolesnikov","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Weissenborn","first_name":"D"},"affiliations":[],"email":null},{"person_name":{"surname":"Zhai","first_name":"X"},"affiliations":[],"email":null},{"person_name":{"surname":"Unterthiner","first_name":"T"},"affiliations":[],"email":null},{"person_name":{"surname":"Dehghani","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Minderer","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Heigold","first_name":"G"},"affiliations":[],"email":null},{"person_name":{"surname":"Gelly","first_name":"S"},"affiliations":[],"email":null}],"date":{"year":"2020","month":null,"day":null},"ids":{"DOI":null,"arXiv":"arXiv:2010.11929"},"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b17":{"title":"Comparison of CoModGans, LaMa and GLIDE for Art Inpainting Completing M.C Escher's Print Gallery","authors":[{"person_name":{"surname":"Cipolina-Kun","first_name":"L"},"affiliations":[],"email":null},{"person_name":{"surname":"Caenazzo","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Mazzei","first_name":"G"},"affiliations":[],"email":null}],"date":{"year":"2022","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":716,"to_page":724}}}},"sections":[{"title":"Problem approach","paragraphs":[{"text":"As always, in the machine and deep learning community, the first step in creating effective models is to have a large and representative dataset available. A solid starting point is 𝒜𝑟𝑡𝒢𝑟𝑎𝑝ℎ [3], which consists of a large Knowledge Graph (KG) regarding cultural heritage, including information about artworks and their authors from different perspectives. However, it is extensible to include other data, such as textual descriptions, to enrich the encoded knowledge. 𝒜𝑟𝑡𝒢𝑟𝑎𝑝ℎ is saved in Neo4j, which already provides information retrieval and knowledge discovery capabilities even without training learning algorithms, using the Cypher query language. For example, Fig. 1 shows the subgraph related to \"The Last Supper\" by Leonardo da Vinci: all the metadata directly associated with the artwork include many different information, from the materials with which the artwork was made to the people depicted.","refs":[{"start":191,"end":194,"marker":"bibr","target":"#b2"},{"start":672,"end":673,"marker":"figure","target":"#fig_1"}]},{"text":"Once the data are available, some research problems can be addressed. As a first step toward a system that can understand art, we are interested in artwork attribute recognition and, more specifically, neuro-symbolic models. This model is suitable for the project's goal because it can jointly exploit different modes of information related to the images and metadata stored in the KG. In particular, Graph Neural Networks can be used to exploit graph features, a state-of-the-art approach to extract meaningful features from the graph and use them for downstream tasks [9]. Intimately related to attribute recognition is recognizing the emotion an image evokes in the observer. In [10], the authors have presented a full transformer-like architecture; in particular, they used the ArtEmis dataset [11], which provides utterances describing the motivation behind an elicited emotion.","refs":[{"start":570,"end":573,"marker":"bibr","target":"#b8"},{"start":682,"end":686,"marker":"bibr","target":"#b9"},{"start":798,"end":802,"marker":"bibr","target":"#b10"}]},{"text":"Another relevant task, which could be helpful for historians and art experts, is the recognition of influences among artists. To this end, it is interesting to construct a method for obtaining such information by reconstructing the history of artistic influences. In particular, from a methodological point of view, also this task can be approached using the KG, in which all these relationships are stored, and solving a link prediction task.","refs":[]},{"text":"Finally, generative algorithms can automate the generation of images and sequences. In this area, Diffusion Models [12] and Generative Adversarial Networks [13] represent the current state-of-the-art and can be considered to solve the task of artwork in-painting, the purpose of which is to reconstruct damaged artworks. Some generative methods are even multimodal. One example is DALL•E-2, 1 developed by OpenAI. This model can generate an image in many  ways based on text prompts. On the other hand, image captioning can generate meaningful descriptions of artworks. For this purpose, methods based on natural language processing are crucial and will be explored. Unfortunately, image captioning systems that work well with natural images often fail when asked to generate output from an art image because they lack the richness and depth that a historical background would provide. This is confirmed by the study conducted by Cetinic et al. [14]. Developing a system that can mimic a human expert is a long-term goal of this community research line.","refs":[{"start":115,"end":119,"marker":"bibr","target":"#b11"},{"start":156,"end":160,"marker":"bibr","target":"#b12"},{"start":945,"end":949,"marker":"bibr","target":"#b13"}]}]},{"title":"Current research and expected results","paragraphs":[{"text":"We are currently developing a tool for solving artwork attribute prediction, a preliminary version of which was presented in [15]. The main idea is to exploit \"contextual\" information provided by 𝒜𝑟𝑡𝒢𝑟𝑎𝑝ℎ, in combination with visual information of the given artwork, extracted respectively by a Graph Attention Network [16] and a Vision Transformer [17]. More specifically, we are trying to address the problem of predicting style, genre, and the evoked emotion of a painting. In this way, we are figuring out a practical approach to recognize those attributes, extending pure computer vision methods.","refs":[{"start":125,"end":129,"marker":"bibr","target":"#b14"},{"start":319,"end":323,"marker":"bibr","target":"#b15"},{"start":349,"end":353,"marker":"bibr","target":"#b16"}]},{"text":"Regarding long-term objectives, we would like to develop a recommender system that could represent a digitized art gallery where general users can purchase a specific artwork based on what they like most. Alternatively, the same system can be used to provide customized tours in a museum. In fact, in this way, the tool can return and rank for the user a subset of the artworks, basing the decision on meta information. Along with artwork recommendation, there is another essential task: artwork captioning. In fact, when returning a selected artwork, a significant description is needed. To this end, our goal is to develop a system capable of generating a caption that includes a description based on the visual content and adds some other information, such as the hidden message the artist wants to communicate to the observer.","refs":[]},{"text":"Finally, we would like to develop an end-to-end method for in-painting to support art experts consistently. In particular, the tool should be able to reconstruct damaged images, varying the generation based on metadata or text prompts in which the experts specify some constraints for the reconstruction. For example, the user requires that the presence of a specific object or person in the reconstructed area is compulsory. Alternatively, the experts could be interested in a reconstruction based on a specific painting style. A promising starting point in this direction is the work of Cipolina-Kun et al. [18].","refs":[{"start":609,"end":613,"marker":"bibr","target":"#b17"}]}]}],"tables":{},"abstract":{"title":"Abstract","paragraphs":[{"text":"Technological improvements have resulted in a large-scale digitization effort in recent years, leading to the increasing availability of large digitized art collections. This provides an opportunity to develop AI systems capable of understanding art, thus supporting art historians and enjoying culture more generally. This paper briefly reviews our ongoing project on automatic art heritage analysis through AI. In particular, new graph representation learning approaches, combined with computer vision, are investigated to handle the complexity of visual arts.","refs":[]}]}}