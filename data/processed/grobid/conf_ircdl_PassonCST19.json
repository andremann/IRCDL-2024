{"bibliography":{"title":"Keyphrase Extraction via an Attentive Model","authors":[{"person_name":{"surname":"Passon","first_name":"Marco"},"affiliations":[{"department":null,"institution":"University of Udine","laboratory":"Artificial Intelligence Laboratory"}],"email":null},{"person_name":{"surname":"Comuzzo","first_name":"Massimo"},"affiliations":[{"department":null,"institution":"University of Udine","laboratory":"Artificial Intelligence Laboratory"}],"email":"comuzzo.massimo@spes.uniud.it"},{"person_name":{"surname":"Serra","first_name":"Giuseppe"},"affiliations":[{"department":null,"institution":"University of Udine","laboratory":"Artificial Intelligence Laboratory"}],"email":null},{"person_name":{"surname":"Tasso","first_name":"Carlo"},"affiliations":[{"department":null,"institution":"University of Udine","laboratory":"Artificial Intelligence Laboratory"}],"email":"carlo.tasso@uniud.it"}],"date":null,"ids":{"DOI":"10.1007/978-3-030-11226-4_24","arXiv":null},"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"keywords":[],"citations":{"b0":{"title":"Neural machine translation by jointly learning to align and translate","authors":[{"person_name":{"surname":"Bahdanau","first_name":"D"},"affiliations":[],"email":null},{"person_name":{"surname":"Cho","first_name":"K"},"affiliations":[],"email":null},{"person_name":{"surname":"Bengio","first_name":"Y"},"affiliations":[],"email":null}],"date":{"year":"2014","month":null,"day":null},"ids":{"DOI":null,"arXiv":"arXiv:1409.0473"},"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b1":{"title":"Bidirectional LSTM recurrent neural network for keyphrase extraction","authors":[{"person_name":{"surname":"Basaldella","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Antolli","first_name":"E"},"affiliations":[],"email":null},{"person_name":{"surname":"Serra","first_name":"G"},"affiliations":[],"email":null},{"person_name":{"surname":"Tasso","first_name":"C"},"affiliations":[],"email":null}],"date":{"year":"2018","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":180,"to_page":187}}},"b2":{"title":"Evaluating anaphora and coreference resolution to improve automatic keyphrase extraction","authors":[{"person_name":{"surname":"Basaldella","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Chiaradia","first_name":"G"},"affiliations":[],"email":null},{"person_name":{"surname":"Tasso","first_name":"C"},"affiliations":[],"email":null}],"date":{"year":"2016","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":804,"to_page":814}}},"b3":{"title":"Natural Language Processing with Python","authors":[{"person_name":{"surname":"Bird","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Klein","first_name":"E"},"affiliations":[],"email":null},{"person_name":{"surname":"Loper","first_name":"E"},"affiliations":[],"email":null}],"date":{"year":"2009","month":null,"day":null},"ids":null,"target":null,"publisher":"OReilly Media Inc","journal":null,"series":null,"scope":null},"b4":{"title":"TopicRank: graph-based topic ranking for keyphrase extraction","authors":[{"person_name":{"surname":"Bougouin","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Boudin","first_name":"F"},"affiliations":[],"email":null},{"person_name":{"surname":"Daille","first_name":"B"},"affiliations":[],"email":null}],"date":{"year":"2013","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":543,"to_page":551}}},"b5":{"title":"Natural language processing (almost) from scratch","authors":[{"person_name":{"surname":"Collobert","first_name":"R"},"affiliations":[],"email":null},{"person_name":{"surname":"Weston","first_name":"J"},"affiliations":[],"email":null},{"person_name":{"surname":"Bottou","first_name":"L"},"affiliations":[],"email":null},{"person_name":{"surname":"Karlen","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Kavukcuoglu","first_name":"K"},"affiliations":[],"email":null},{"person_name":{"surname":"Kuksa","first_name":"P"},"affiliations":[],"email":null}],"date":{"year":"2011","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"J. Mach. Learn. Res","series":null,"scope":{"volume":12,"pages":{"from_page":2498,"to_page":2537}}},"b6":{"title":"A new multi-lingual knowledge-base approach to keyphrase extraction for the Italian language","authors":[{"person_name":{"surname":"Degl'innocenti","first_name":"D"},"affiliations":[],"email":null},{"person_name":{"surname":"De Nart","first_name":"D"},"affiliations":[],"email":null},{"person_name":{"surname":"Tasso","first_name":"C"},"affiliations":[],"email":null}],"date":{"year":"2014","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":78,"to_page":85}}},"b7":{"title":"Neural mechanisms of selective visual attention","authors":[{"person_name":{"surname":"Desimone","first_name":"R"},"affiliations":[],"email":null},{"person_name":{"surname":"Duncan","first_name":"J"},"affiliations":[],"email":null}],"date":{"year":"1995","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"Ann. Rev. Neurosci","series":null,"scope":{"volume":18,"pages":{"from_page":193,"to_page":222}}},"b8":{"title":"Learning precise timing with LSTM recurrent networks","authors":[{"person_name":{"surname":"Gers","first_name":"F"},"affiliations":[],"email":null},{"person_name":{"surname":"Schraudolph","first_name":"N"},"affiliations":[],"email":null},{"person_name":{"surname":"Schmidhuber","first_name":"J"},"affiliations":[],"email":null}],"date":{"year":"2002","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"J. Mach. Learn. Res","series":null,"scope":{"volume":3,"pages":{"from_page":115,"to_page":143}}},"b9":{"title":"Framewise phoneme classification with bidirectional LSTM and other neural network architectures","authors":[{"person_name":{"surname":"Graves","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Schmidhuber","first_name":"J"},"affiliations":[],"email":null}],"date":{"year":"2005","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"Neural Netw","series":null,"scope":{"volume":18,"pages":{"from_page":602,"to_page":610}}},"b10":{"title":"Accurate keyphrase extraction by discriminating overlapping phrases","authors":[{"person_name":{"surname":"Haddoud","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Abdedda√Øm","first_name":"S"},"affiliations":[],"email":null}],"date":{"year":"2014","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"J. Inf. Sci","series":null,"scope":{"volume":40,"pages":{"from_page":488,"to_page":500}}},"b11":{"title":"CorePhrase: keyphrase extraction for document clustering","authors":[{"person_name":{"surname":"Hammouda","first_name":"K"},"affiliations":[],"email":null},{"person_name":{"surname":"Matute","first_name":"D"},"affiliations":[],"email":null},{"person_name":{"surname":"Kamel","first_name":"M"},"affiliations":[],"email":null}],"date":{"year":"2005","month":null,"day":null},"ids":{"DOI":"10.1007/11510888_26","arXiv":null},"target":"https://doi.org/10.1007/1151088826","publisher":"Springer","journal":null,"series":null,"scope":{"volume":3587,"pages":{"from_page":265,"to_page":274}}},"b12":{"title":"Automatic keyphrase extraction: a survey of the state of the art","authors":[{"person_name":{"surname":"Hasan","first_name":"K"},"affiliations":[],"email":null},{"person_name":{"surname":"Ng","first_name":"V"},"affiliations":[],"email":null}],"date":{"year":"2014","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":1,"pages":{"from_page":1262,"to_page":1273}}},"b13":{"title":"Gradient flow in recurrent nets: the difficulty of learning long-term dependencies","authors":[{"person_name":{"surname":"Hochreiter","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Bengio","first_name":"Y"},"affiliations":[],"email":null},{"person_name":{"surname":"Frasconi","first_name":"P"},"affiliations":[],"email":null},{"person_name":{"surname":"Schmidhuber","first_name":"J"},"affiliations":[],"email":null}],"date":{"year":"2001","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b14":{"title":"Long short-term memory","authors":[{"person_name":{"surname":"Hochreiter","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Schmidhuber","first_name":"J"},"affiliations":[],"email":null}],"date":{"year":"1997","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"Neural Comput","series":null,"scope":{"volume":9,"pages":{"from_page":1735,"to_page":1780}}},"b15":{"title":"Improved automatic keyword extraction given more linguistic knowledge","authors":[{"person_name":{"surname":"Hulth","first_name":"A"},"affiliations":[],"email":null}],"date":{"year":"2003","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":216,"to_page":223}}},"b16":{"title":"A model of saliency-based visual attention for rapid scene analysis","authors":[{"person_name":{"surname":"Itti","first_name":"L"},"affiliations":[],"email":null},{"person_name":{"surname":"Koch","first_name":"C"},"affiliations":[],"email":null},{"person_name":{"surname":"Niebur","first_name":"E"},"affiliations":[],"email":null}],"date":{"year":"1998","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"IEEE Trans. Patt. Anal. Mach. Intell","series":null,"scope":{"volume":20,"pages":{"from_page":1254,"to_page":1259}}},"b17":{"title":"SemEval-2010 task 5: automatic keyphrase extraction from scientific articles","authors":[{"person_name":{"surname":"Kim","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Medelyan","first_name":"O"},"affiliations":[],"email":null},{"person_name":{"surname":"Kan","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Baldwin","first_name":"T"},"affiliations":[],"email":null}],"date":{"year":"2010","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":21,"to_page":26}}},"b18":{"title":"Adam: a method for stochastic optimization","authors":[{"person_name":{"surname":"Kingma","first_name":"D"},"affiliations":[],"email":null},{"person_name":{"surname":"Ba","first_name":"J"},"affiliations":[],"email":null}],"date":{"year":"2014","month":null,"day":null},"ids":{"DOI":null,"arXiv":"arXiv:1412.6980"},"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b19":{"title":"HUMB: automatic key term extraction from scientific articles in GROBID","authors":[{"person_name":{"surname":"Lopez","first_name":"P"},"affiliations":[],"email":null},{"person_name":{"surname":"Romary","first_name":"L"},"affiliations":[],"email":null}],"date":{"year":"2010","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":248,"to_page":251}}},"b20":{"title":"Effective approaches to attention-based neural machine translation","authors":[{"person_name":{"surname":"Luong","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Pham","first_name":"H"},"affiliations":[],"email":null},{"person_name":{"surname":"Manning","first_name":"C"},"affiliations":[],"email":null}],"date":{"year":"2015","month":null,"day":null},"ids":{"DOI":null,"arXiv":"arXiv:1508.04025"},"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b21":{"title":"Deep keyphrase generation","authors":[{"person_name":{"surname":"Meng","first_name":"R"},"affiliations":[],"email":null},{"person_name":{"surname":"Zhao","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Han","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"He","first_name":"D"},"affiliations":[],"email":null},{"person_name":{"surname":"Brusilovsky","first_name":"P"},"affiliations":[],"email":null},{"person_name":{"surname":"Chi","first_name":"Y"},"affiliations":[],"email":null}],"date":{"year":"2017","month":null,"day":null},"ids":{"DOI":null,"arXiv":"arXiv:1704.06879"},"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b22":{"title":"TextRank: bringing order into texts","authors":[{"person_name":{"surname":"Mihalcea","first_name":"R"},"affiliations":[],"email":null},{"person_name":{"surname":"Tarau","first_name":"P"},"affiliations":[],"email":null}],"date":{"year":"2004","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b23":{"title":"Distributed representations of words and phrases and their compositionality","authors":[{"person_name":{"surname":"Mikolov","first_name":"T"},"affiliations":[],"email":null},{"person_name":{"surname":"Sutskever","first_name":"I"},"affiliations":[],"email":null},{"person_name":{"surname":"Chen","first_name":"K"},"affiliations":[],"email":null},{"person_name":{"surname":"Corrado","first_name":"G"},"affiliations":[],"email":null},{"person_name":{"surname":"Dean","first_name":"J"},"affiliations":[],"email":null}],"date":{"year":"2013","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":3111,"to_page":3119}}},"b24":{"title":"Abstractive text summarization using sequence-to-sequence RNNs and beyond","authors":[{"person_name":{"surname":"Nallapati","first_name":"R"},"affiliations":[],"email":null},{"person_name":{"surname":"Zhou","first_name":"B"},"affiliations":[],"email":null},{"person_name":{"surname":"Gulcehre","first_name":"C"},"affiliations":[],"email":null},{"person_name":{"surname":"Xiang","first_name":"B"},"affiliations":[],"email":null}],"date":{"year":"2016","month":null,"day":null},"ids":{"DOI":null,"arXiv":"arXiv:1602.06023"},"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b25":{"title":"A contentbased approach to social network analysis: a case study on research communities","authors":[{"person_name":{"surname":"De Nart","first_name":"D"},"affiliations":[],"email":null},{"person_name":{"surname":"Degl'innocenti","first_name":"D"},"affiliations":[],"email":null},{"person_name":{"surname":"Basaldella","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Agosti","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Tasso","first_name":"C"},"affiliations":[],"email":null}],"date":{"year":"2016","month":null,"day":null},"ids":{"DOI":"10.1007/978-3-319-41938-1_15","arXiv":null},"target":"https://doi.org/10.1007/978-3-319-41938-115","publisher":"Springer","journal":null,"series":null,"scope":{"volume":612,"pages":{"from_page":142,"to_page":154}}},"b26":{"title":"Modelling the user modelling community (and other communities as well)","authors":[{"person_name":{"surname":"De Nart","first_name":"D"},"affiliations":[],"email":null},{"person_name":{"surname":"Degl'innocenti","first_name":"D"},"affiliations":[],"email":null},{"person_name":{"surname":"Pavan","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Basaldella","first_name":"M"},"affiliations":[],"email":null},{"person_name":{"surname":"Tasso","first_name":"C"},"affiliations":[],"email":null}],"date":{"year":"2015","month":null,"day":null},"ids":{"DOI":"10.1007/978-3-319-20267-9_31","arXiv":null},"target":"https://doi.org/10.1007/978-3-319-20267-931","publisher":"Springer","journal":null,"series":null,"scope":{"volume":9146,"pages":{"from_page":357,"to_page":363}}},"b27":{"title":"Automatic differentiation in PyTorch","authors":[{"person_name":{"surname":"Paszke","first_name":"A"},"affiliations":[],"email":null}],"date":{"year":"2017","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b28":{"title":"GloVe: global vectors for word representation","authors":[{"person_name":{"surname":"Pennington","first_name":"J"},"affiliations":[],"email":null},{"person_name":{"surname":"Socher","first_name":"R"},"affiliations":[],"email":null},{"person_name":{"surname":"Manning","first_name":"C"},"affiliations":[],"email":null}],"date":{"year":"2014","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":1532,"to_page":1543}}},"b29":{"title":"A neural attention model for abstractive sentence summarization","authors":[{"person_name":{"surname":"Rush","first_name":"A"},"affiliations":[],"email":null},{"person_name":{"surname":"Chopra","first_name":"S"},"affiliations":[],"email":null},{"person_name":{"surname":"Weston","first_name":"J"},"affiliations":[],"email":null}],"date":{"year":"2015","month":null,"day":null},"ids":{"DOI":null,"arXiv":"arXiv:1509.00685"},"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b30":{"title":"Sequence to sequence learning with neural networks","authors":[{"person_name":{"surname":"Sutskever","first_name":"I"},"affiliations":[],"email":null},{"person_name":{"surname":"Vinyals","first_name":"O"},"affiliations":[],"email":null},{"person_name":{"surname":"Le","first_name":"Q"},"affiliations":[],"email":null}],"date":{"year":"2014","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":3104,"to_page":3112}}},"b31":{"title":"A language model approach to keyphrase extraction","authors":[{"person_name":{"surname":"Tomokiyo","first_name":"T"},"affiliations":[],"email":null},{"person_name":{"surname":"Hurst","first_name":"M"},"affiliations":[],"email":null}],"date":{"year":"2003","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":18,"pages":{"from_page":33,"to_page":40}}},"b32":{"title":"Learning algorithms for keyphrase extraction","authors":[{"person_name":{"surname":"Turney","first_name":"P"},"affiliations":[],"email":null}],"date":{"year":"2000","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"Inf. Retrieval","series":null,"scope":{"volume":2,"pages":{"from_page":303,"to_page":336}}},"b33":{"title":"KEA: practical automatic keyphrase extraction","authors":[{"person_name":{"surname":"Witten","first_name":"I"},"affiliations":[],"email":null},{"person_name":{"surname":"Paynter","first_name":"G"},"affiliations":[],"email":null},{"person_name":{"surname":"Frank","first_name":"E"},"affiliations":[],"email":null},{"person_name":{"surname":"Gutwin","first_name":"C"},"affiliations":[],"email":null},{"person_name":{"surname":"Nevill-Manning","first_name":"C"},"affiliations":[],"email":null}],"date":{"year":"1999","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":null},"b34":{"title":"Show, attend and tell: neural image caption generation with visual attention","authors":[{"person_name":{"surname":"Xu","first_name":"K"},"affiliations":[],"email":null}],"date":{"year":"2015","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":2048,"to_page":2057}}},"b35":{"title":"Keyphrase extraction using deep recurrent neural networks on Twitter","authors":[{"person_name":{"surname":"Zhang","first_name":"Q"},"affiliations":[],"email":null},{"person_name":{"surname":"Wang","first_name":"Y"},"affiliations":[],"email":null},{"person_name":{"surname":"Gong","first_name":"Y"},"affiliations":[],"email":null},{"person_name":{"surname":"Huang","first_name":"X"},"affiliations":[],"email":null}],"date":{"year":"2016","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":null,"series":null,"scope":{"volume":null,"pages":{"from_page":836,"to_page":845}}},"b36":{"title":"World wide web site summarization","authors":[{"person_name":{"surname":"Zhang","first_name":"Y"},"affiliations":[],"email":null},{"person_name":{"surname":"Zincir-Heywood","first_name":"N"},"affiliations":[],"email":null},{"person_name":{"surname":"Milios","first_name":"E"},"affiliations":[],"email":null}],"date":{"year":"2004","month":null,"day":null},"ids":null,"target":null,"publisher":null,"journal":"Web Intell. Agent Syst. Int. J","series":null,"scope":{"volume":2,"pages":{"from_page":39,"to_page":53}}}},"sections":[{"title":"Introduction","paragraphs":[{"text":"The continuous growth of textual digital libraries, in terms of both importance and size, urgently requires advanced and effective tools to extract automatically, for each document, the most relevant content. To achieve this goal, the Natural Language Processing Community exploits the concept of \"Keyphrases\" (KPs), which are phrases that \"capture the main topics discussed on a given document\" [33].","refs":[{"start":396,"end":400,"marker":"bibr","target":"#b32"}]},{"text":"Extracting KPs from a document can be done manually, employing human judges, or automatically; in the latter case we talk about Automatic Keyphrase Extraction (AKE), a task whose importance has been growing for the last two decades [13]. In fact, the ability to extract automatically keyphrases from documents will make it possible to build more effective information retrieval systems or to summarize [37] or cluster [12] textual documents. Other fields worth mentioning where AKE can be applied are social network analysis [26] and user modeling [27].","refs":[{"start":232,"end":236,"marker":"bibr","target":"#b12"},{"start":402,"end":406,"marker":"bibr","target":"#b36"},{"start":418,"end":422,"marker":"bibr","target":"#b11"},{"start":525,"end":529,"marker":"bibr","target":"#b25"},{"start":548,"end":552,"marker":"bibr","target":"#b26"}]},{"text":"Classic AKE approaches rely on Machine Learning algorithms. More specifically, supervised techniques have been used for this task: Naive Bayes [34], C4.5 decision trees [33], Multilayer Perceptrons [3,20], Support Vector Machines [20], Logistic Regression [3,11], and Bagging [16]. Relevant works that investigated the unsupervised extraction of Keyphrases used a language model approach [32] or a graph-based ranking algorithm [23]. However, these approaches achieved lower performance than the one obtained in the supervised case.","refs":[{"start":143,"end":147,"marker":"bibr","target":"#b33"},{"start":169,"end":173,"marker":"bibr","target":"#b32"},{"start":198,"end":201,"marker":"bibr","target":"#b2"},{"start":201,"end":204,"marker":"bibr","target":"#b19"},{"start":230,"end":234,"marker":"bibr","target":"#b19"},{"start":256,"end":259,"marker":"bibr","target":"#b2"},{"start":259,"end":262,"marker":"bibr","target":"#b10"},{"start":276,"end":280,"marker":"bibr","target":"#b15"},{"start":388,"end":392,"marker":"bibr","target":"#b31"},{"start":428,"end":432,"marker":"bibr","target":"#b22"}]},{"text":"Due to this difference in performance, in the last years research focus shifted towards the features exploited by supervised algorithms. The kind of knowledge encoded in the model can be used to discriminate between different families of approaches: statistical knowledge (number of appearances of KPs in the document, TF-IDF, number of sentences containing KPs, etc.), positional knowledge (first position of the KP in the document, position of the last occurrence, appearance in the title or in specific sections, etc.), linguistic knowledge (part-of-speech tags of the KP [16], anaphoras pointing to the KP [3], etc.), external knowledge (presence of the KP as a page on Wikipedia [7] or in specialized domain ontologies [20], etc.). Despite being a subject on several studies, AKE is still an open problem in the NLP field: in fact, even the best techniques for this task reach at best an average performance F1-Score around 50% [16,18].","refs":[{"start":575,"end":579,"marker":"bibr","target":"#b15"},{"start":610,"end":613,"marker":"bibr","target":"#b2"},{"start":684,"end":687,"marker":"bibr","target":"#b6"},{"start":724,"end":728,"marker":"bibr","target":"#b19"},{"start":933,"end":937,"marker":"bibr","target":"#b15"},{"start":937,"end":940,"marker":"bibr","target":"#b17"}]},{"text":"Although Deep Learning techniques have been recently established as stateof-the-art approaches in many NLP tasks (i.e. sentiment classification, machine translation, etc.), to the best of our knowledge, only a few Deep Learning models addressed the AKE task. Zhang et al. [36] proposed a deep Recurrent Neural Network (RNN) model that combines keywords and context information to be exploited in the AKE task in Twitter domain. In particular, their model consists of a RNN model with two hidden layers: the first captures the keyword information, the second extracts the keyphrases according to the keyword information. Meng et al. [22] addressed the challenge of generating keyphrases that are not present in the text, investigating Encoder-Decoder Neural architecture [31]: the underlying idea is to compress the text content into an hidden representation using an encoder and generate the corresponding keyphrases with a decoder. Basaldella et al. [2] proposed an architecture for AKE based on Bidirectional Long Short-Term Memory (BLSTM) RNN, which is able to exploit both previous and future context of a specific word, differently from simple RNNs that can exploit only the previous context.","refs":[{"start":272,"end":276,"marker":"bibr","target":"#b35"},{"start":632,"end":636,"marker":"bibr","target":"#b21"},{"start":770,"end":774,"marker":"bibr","target":"#b30"},{"start":951,"end":954,"marker":"bibr","target":"#b1"}]},{"text":"In parallel with these initiatives, the class of Attentive Models [31,35] started gaining more and more interest in NLP community, because they have been successfully applied in various text understanding tasks, like neural machine translation from a language to another [1,21,31], abstracting text [25] and sentence summarization [30]. To our knowledge, however, these Models have not been employed in AKE tasks.","refs":[{"start":66,"end":70,"marker":"bibr","target":"#b30"},{"start":70,"end":73,"marker":"bibr","target":"#b34"},{"start":271,"end":274,"marker":"bibr","target":"#b0"},{"start":274,"end":277,"marker":"bibr","target":"#b20"},{"start":277,"end":280,"marker":"bibr","target":"#b30"},{"start":299,"end":303,"marker":"bibr","target":"#b24"},{"start":331,"end":335,"marker":"bibr","target":"#b29"}]},{"text":"In this paper, we investigate the usage of Attentive models in the Keyphrase Extraction domain. The rationale behind this choice is that Attentive Models provide weights that indicate the relevance of a word with respect to its context [1,35] and thus can help in extracting keyphrases. Preliminary experimental results on the widely used INSPEC dataset [16] confirm our hypothesis and show that our approach outperforms the competitors.","refs":[{"start":236,"end":239,"marker":"bibr","target":"#b0"},{"start":239,"end":242,"marker":"bibr","target":"#b34"},{"start":354,"end":358,"marker":"bibr","target":"#b15"}]}]},{"title":"Keyphrase Extraction Approach","paragraphs":[{"text":"We aim at developing a system that, given a text document, is able to automatically extract its keyphrases. Our solution consists of a neural network architecture that combines Recurrent Neural models with an Attentive component. The proposed model takes as input a text and returns as prediction an annotated text (see Fig. 1).","refs":[{"start":325,"end":326,"marker":"figure","target":"#fig_0"}]},{"text":"First, text is split into sentences, and then tokenized in words using the library NLTK [4]. Each word is then mapped into a continuous vector representation, called word embedding, that according to recent studies [6,24] represents the semantics of words better than the \"one hot\" encoding word representation. For our work we used Stanford's GloVe Embeddings [29], since the common datasets adopted for AKE task are rather small, making it difficult to build custom embeddings.","refs":[{"start":88,"end":91,"marker":"bibr","target":"#b3"},{"start":215,"end":218,"marker":"bibr","target":"#b5"},{"start":218,"end":221,"marker":"bibr","target":"#b23"},{"start":361,"end":365,"marker":"bibr","target":"#b28"}]},{"text":"However, when dealing with the keyphrase extraction, words cannot be treated with the same importance (for example, a stop word is less important than a noun, adjectives and adverbs enrich a speech but add almost nothing to the core meaning of it, etc.).","refs":[]},{"text":"To encode this core feature, we propose to integrate in our pipeline an attentive neural component. In fact, attention models are inspired by human attention mechanisms, that do not use all the available information at a given time, but select the most pertinent piece of information, leaving out the parts considered irrelevant [8,17].","refs":[{"start":329,"end":332,"marker":"bibr","target":"#b7"},{"start":332,"end":335,"marker":"bibr","target":"#b16"}]},{"text":"The goal of our Attentive Model is to associate to each word of the text an attentive value, i.e. a weight representing the attention level of the word: this information is then exploited in the subsequent processing phase and we claim it can have a significant role for a more effective and precise identification of KPs. To compute such attentive values, the Attentive Module needs, for each word w in the text, (i) the corresponding word embedding and (ii) the so called context, that is a representation of the semantics of the words appearing in the text before and after the word w. The context is computed by a specific module, the Context Encoder, which exploits a BLSTM network capable of producing a non-linear combination of the word embeddings belonging to the previous and future surroundings of w.","refs":[]},{"text":"Finally, to extract keyphrases, the Extractor module combines word embeddings and the attentive values (concatenating their results) by means of a BLSTM neural model which is able to analyze word embeddings and their sequential features and to effectively deal with the variable lengths of sentences. For each word, the output consists of three possible classes: NO KP for words that are not keyphrases, BEGIN KP for words corresponding to the first token of a keyphrase, INSIDE KP for a token, other than the first one, belonging to a keyphrase (see Fig. 2 for more details). ","refs":[{"start":556,"end":557,"marker":"figure","target":"#fig_1"}]}]},{"title":"Attentive Module and Context Encoder","paragraphs":[{"text":"To extract the importance of a word in a given sentence we propose to use an attentive model. Our attentive model exploits the semantic representation of each word (the word embeddings) and the context in which the word is located. The main idea is to identify words that are more related to the context. In our architecture the context is defined as the output results of a BLSTM that takes in input the word embeddings of the text.","refs":[]},{"text":"Let S be the matrix formed by the word embeddings w 1 , w 2 , . . . , w s and C the matrix formed by the context vectors c 1 , c 2 , . . . , c s (see Fig. 2). The size of each context vector is equal to size of the word embedding vector. Therefore, S and C are both s√ód matrices, where d is the dimension of the word embeddings.","refs":[{"start":155,"end":156,"marker":"figure","target":"#fig_1"}]},{"text":"Our attentive model first performs the matrix multiplication between C and the transpose of S, resulting in a new matrix M . More formally, each element of M , m i,j (where i is the row and j is the column) is computed as follows:","refs":[]},{"text":"In other words, each element of m i,j is given by the sum, for each word embedding dimension k, of the product between the element in the context matrix C i,k and the element in the word embedding matrix S j,k .","refs":[]},{"text":"Then, we compute the normalization of the matrix M using a softmax layer (that behaves almost like a argmax, but is differentiable) as follows:","refs":[]},{"text":"Every item of M is now the range of 0 and 1 and the sum of each row is equal to 1. The element m i,j represents the attention score of the word w j in the classification context of the word w i . Through matrix multiplication between the matrices M and S we compute the matrix A, where each row represents the output of the attentive model.","refs":[]},{"text":"The attentive output is then used as input in a Dense layer in order to manage the contribution of the attentive model in the final word representation. The output vectors coming out from this Dense layer are finally concatenated with the initial embeddings and fed into the classifier BLSTM.","refs":[]},{"text":"Figure 3 illustrates a single iteration of the attentive model that we just outlined. Specifically, we represented the case where the importance of the single words is computed against the context vector c3 and saved into the vector a3 (in our case, this represents the third row of A). It is important to point out that there are as many context vectors as there are word embeddings and each context vector is different from the others, thus each subsequent iteration will use a different context vector and will consequently compute a different vector of weights.","refs":[{"start":7,"end":8,"marker":"figure","target":"#fig_2"}]}]},{"title":"Bidirectional Long Short-Term Memory (BLSTM)","paragraphs":[{"text":"Differently from feedforward neural networks, where the inputs are independent of each other, Recurrent Neural Networks (RNNs) keep an internal state that allows them to process sequences of inputs, with each input related to each other, thus granting the persistence of the information. RNNs are often employed in NLP tasks where the context is an important component needed to compute the predictions. As Recurrent Neural Network, we adopt the Long Short-Term Memory (LSTM) architecture [15], a common and effective solution employed to reduce the vanishing gradient problem [14] that typically affect plain RNNs. In particular an LSTM is defined as follows [9]:","refs":[{"start":489,"end":493,"marker":"bibr","target":"#b14"},{"start":577,"end":581,"marker":"bibr","target":"#b13"},{"start":660,"end":663,"marker":"bibr","target":"#b8"}]},{"text":"where œÉ is the logistic sigmoid function, i, f , o, and c are the input gate, forget gate, output gate and cell activation vectors, and all b are learned biases. The first step (Eq. 3) deletes the information coming from the previous element in the input sequence. Next (Eqs. 4 and 5) comes the decision of what information is going to be stored in the cell's state, replacing the one previously forgot: this is done having an input gate i deciding the values to update and then creating a candidate value c. In the last step (Eqs. 6 and 7) the output is computed: the cell's state passes through a sigmoid layer œÉ and finally through the tanh function in order to push the values between -1 and 1.","refs":[]},{"text":"This kind of architecture, however, contemplates only previous information, but in our case, dealing with the AKE task, future information can support the identification of a possible keyphrase. For this reason a variant of the LSTM architecture is used, namely the Bidirectional LSTM architecture [10], that allows us to employ both past and future information. In a BLSTM the output y t is obtained combining the forward hidden sequence -‚Üí h t and the backward hidden sequence ‚Üêh t . A BLSTM is then defined as follows:","refs":[{"start":298,"end":302,"marker":"bibr","target":"#b9"}]}]},{"title":"Experimental Results","paragraphs":[{"text":"In order to validate our solution, we used the well-known INSPEC dataset [16], which consists by 2000 abstract papers written in English extracted from journal papers from the disciplines Computer and Control, Information Technology. The dataset is split in: 1000 documents as training set, 500 documents as validation set, 500 documents as test set.","refs":[{"start":73,"end":77,"marker":"bibr","target":"#b15"}]},{"text":"To write the implementation of our approach we used Pytorch [28]. The GPU employed in our experiments is a GeForce GTX 660 Ti. We train our network aiming at minimizing the Crossentropy Loss and the training is done using the Root Square Mean Propagation optimization algorithm [19]. The data is loaded into the network in batches, where each batch has a size of 32 input items. The experiments have been run with different configurations of the network's parameters, finally obtaining the best results with a size of 30 neurons for the Attentive Model, 150 neurons for the BLSTM used for classification, 150 neurons for its hidden dense layer and a value of 0.5 for the Dropout layer before the final Dense layer. The Pytorch framework does not implement a early-stopping mechanism; for this reason, we empirically set the number of epochs to 14. The first of our experiments aimed at reproducing Basaldella et al. [2] results (an approach based on word embeddings and BLSTM) in order to create a solid baseline against which we can compare the results obtained by our approach based on an attentive module. Baseline results presented in Table 1 are slightly better than the original ones [2], because here we adopted a different value of the dropout layer.","refs":[{"start":60,"end":64,"marker":"bibr","target":"#b27"},{"start":278,"end":282,"marker":"bibr","target":"#b18"},{"start":916,"end":919,"marker":"bibr","target":"#b1"},{"start":1145,"end":1146,"marker":"table","target":"#tab_0"},{"start":1190,"end":1193,"marker":"bibr","target":"#b1"}]},{"text":"Table 1 also presents performance varying the dimension of the attentive module: the size of the Dense layer that allows us to weight the attention effect. Note that a small attentive layer does not bring any benefit, on the contrary the performances are lower compared to the baseline where no attentive module is present. The reason behind this behavior may be the attentive layer focusing on a part of information that is too small and leaving other important parts out. Further increasing the dimension of the attentive module causes an improvement to the performances achieving the best score (in all metrics) with an attentive layer of dimension 30. An additional increase of the attentive layer makes it focus on a part of information that is too big, actually not focusing on anything in particular thus not making use of the attention mechanism. In fact, performance are similar to the ones obtained without attentive model.","refs":[{"start":6,"end":7,"marker":"table","target":"#tab_0"}]},{"text":"Finally, we compare our results with state-of-the-art systems, which rely on supervised and unsupervised machine learning techniques (see Table 2). The first system is the one proposed in [2] that uses a BiLSTM architecture (our baseline); the second technique proposed an approach based on Encoder-Decoder Neural architecture [22]; the next three are works presented in [16] that use three different techniques, respectively: n-grams, Noun Phrases (NP) chunking and patterns; the last one [5] relies on a topical representation of a document, making use of graphs to extract keywords. Note, our proposed approach achieves state of the art performance under every measure considered. It is worth noting that we perform better than the results presented in [2] and [22], two recent works that make use of Deep Learning techniques. ","refs":[{"start":144,"end":145,"marker":"table","target":"#tab_1"},{"start":188,"end":191,"marker":"bibr","target":"#b1"},{"start":327,"end":331,"marker":"bibr","target":"#b21"},{"start":371,"end":375,"marker":"bibr","target":"#b15"},{"start":490,"end":493,"marker":"bibr","target":"#b4"},{"start":756,"end":759,"marker":"bibr","target":"#b1"},{"start":764,"end":768,"marker":"bibr","target":"#b21"}]}]},{"title":"Conclusion","paragraphs":[{"text":"In this work, we proposed a network that uses an Attentive Model as its core in order to perform automatic keyphrase extraction. The approach has been validated on the well-known INSPEC dataset and the experiments have been performed varying the size of the attentive model. Comparison evaluation shows that our approach outperforms competitive works on all metrics. As future works we intend testing the proposed architecture on other Keyphrase datasets and we will investigate advanced attentive architectures, such as Tree and Graph Attention models that can deal with complex text representation.","refs":[]}]}],"tables":{"tab_0":{"heading":"Table 1 .","description":"Performance obtained varying the dimension of the attentive layer.","rows":[["Embedding","Attentive Dim. Precision Recall F1-score MAP F1@5 F1@10"],["Glove-200 (Baseline) -","0.326","0.643 0.432","0.356 0.286 0.353"],["Glove-200","10","0.291","0.624 0.397","0.327 0.271 0.326"],["Glove-200","20","0.348","0.654 0.455","0.370 0.297 0.371"],["Glove-200","30","0.373","0.658 0.476","0.388 0.313 0.394"],["Glove-200","40","0.321","0.648 0.429","0.356 0.287 0.350"]]},"tab_1":{"heading":"Table 2 .","description":"Comparison results on INSPEC dataset","rows":[["Method","Precision Recall F1-score F1@5 F1@10"],["Proposed approach","0.373","0.658 0.476","0.313 0.394"],["BiLSTM [2]","0.340","0.578 0.428","-","-"],["KP Generation [22]","-","-","-","0.278 0.342"],["n-grams with tag [16]","0.252","0.517 0.339","-","-"],["NP Chunking with tag [16] 0.297","0.372 0.330","-","-"],["Pattern with tag [16]","0.217","0.399 0.281","-","-"],["TopicRank [5]","0.348","0.404 0.352","-","-"]]}},"abstract":{"title":"Abstract","paragraphs":[{"text":"Keyphrase extraction is a task of crucial importance for digital libraries. When performing automatically a task of this, the context in which a specific word is located seems to hold a substantial role. To exploit this context, in this paper we propose an architecture based on an Attentive Model: a neural network designed to focus on the most relevant parts of data. A preliminary experimental evaluation on the widely used INSPEC dataset confirms the validity of the approach and shows our approach achieves higher performance than the state of the art.","refs":[]}]}}